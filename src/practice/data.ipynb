{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e3d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import sageattention\n",
      "Successfully imported sageattention\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from omegaconf import OmegaConf\n",
    "import math, json, shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Value\n",
    "from PIL import Image, ImageDraw\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import SiglipImageProcessor, SiglipVisionModel, SiglipVisionConfig\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from musubi_tuner.cache_latents import setup_parser_common, hv_setup_parser\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer, load_user_config, generate_dataset_group_by_blueprint\n",
    "from musubi_tuner.dataset.image_video_dataset import DatasetGroup, ImageDataset, ItemInfo, save_latent_cache_framepack, BucketSelector\n",
    "from musubi_tuner.frame_pack.clip_vision import hf_clip_vision_encode\n",
    "from musubi_tuner.frame_pack.framepack_utils import load_image_encoders, load_vae, FEATURE_EXTRACTOR_CONFIG, IMAGE_ENCODER_CONFIG\n",
    "from musubi_tuner.hunyuan_model.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\n",
    "from musubi_tuner.fpack_cache_latents import encode_and_save_batch_one_frame, append_section_idx_to_latent_cache_path\n",
    "from musubi_tuner.hv_train_network import collator_class, load_prompts\n",
    "from musubi_tuner.utils.bbox_utils import get_bbox_from_mask, get_mask_from_bboxes, draw_bboxes, get_facebbox_from_bbox\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "mpath = Path('/groups/chenchen/patrick/OpenS2V-Nexus/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.dataset.image_video_dataset:load image jsonl from /groups/chenchen/patrick/OpenS2V-Nexus/datasets/OpenS2V_part1_test3_2_train_v2.jsonl\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:loaded 8887 images\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 8887 images with 1 control images per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 8887 masks with 1 control masks per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 8887 metadata with 1 bbox paths per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.config_utils:[Dataset 0]\n",
      "  is_image_dataset: True\n",
      "  resolution: (960, 544)\n",
      "  batch_size: 16\n",
      "  num_repeats: 1\n",
      "  caption_extension: \".txt\"\n",
      "  enable_bucket: False\n",
      "  bucket_no_upscale: False\n",
      "  cache_directory: \"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2_cache_v2\"\n",
      "  debug_dataset: False\n",
      "    image_directory: \"None\"\n",
      "    image_jsonl_file: \"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/OpenS2V_part1_test3_2_train_v2.jsonl\"\n",
      "    fp_latent_window_size: 9\n",
      "    fp_1f_clean_indices: [0]\n",
      "    fp_1f_target_index: 3\n",
      "    fp_1f_no_post: True, \n",
      "    control_count_per_image: 2,\n",
      "    control_resolution: (256, 256)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.argv = [\"fpack_cache_latents.py\",\n",
    "    \"--dataset_config\", \"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/OpenS2V_part1_test3_2.toml\",\n",
    "    \"--vae\", \"/home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\",\n",
    "    \"--image_encoder\", \"/home/yo564250/workspace/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--vae_chunk_size\", \"32\",\n",
    "    \"--vae_spatial_tile_sample_min_size\", \"128\", \n",
    "    \"--skip_existing\", \"--keep_cache\", \"--one_frame\", \"--one_frame_no_2x\", \"--one_frame_no_4x\"\n",
    "]\n",
    "\n",
    "\n",
    "parser = setup_parser_common()\n",
    "parser = hv_setup_parser(parser)  # VAE\n",
    "# parser = framepack_setup_parser(parser)\n",
    "parser.add_argument(\"--image_encoder\", type=str, required=True)\n",
    "parser.add_argument(\"--f1\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_2x\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_4x\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "user_config = load_user_config(args.dataset_config)\n",
    "blueprint = blueprint_generator.generate(user_config, args, architecture='fp')\n",
    "train_dataset_group = generate_dataset_group_by_blueprint(blueprint.dataset_group)\n",
    "dataset = train_dataset_group.datasets\n",
    "\n",
    "# dataset = ImageDataset(\n",
    "#     resolution=(960, 544), caption_extension='.txt', batch_size=2, num_repeats=1, \n",
    "#     enable_bucket=True, bucket_no_upscale=False, \n",
    "#     image_directory=None, \n",
    "#     image_jsonl_file=str(mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\"), \n",
    "#     control_directory=None,\n",
    "#     cache_directory=str(mpath / \"test3_2_cache_v2\"), \n",
    "#     # cache_directory=None,\n",
    "#     fp_latent_window_size=9, fp_1f_clean_indices=[0], fp_1f_target_index=9, \n",
    "#     fp_1f_no_post=True, debug_dataset=False, architecture='fp', \n",
    "#     control_resolution=(256,256)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.set_seed(0)\n",
    "dataset.prepare_for_training()\n",
    "# for _, batch in tqdm(dataset.retrieve_latent_cache_batches(4)):\n",
    "    # items = batch\n",
    "train_dataset_group = DatasetGroup([dataset])\n",
    "\n",
    "collator = collator_class(Value(\"i\", 0), Value(\"i\", 0), None)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_group,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = batch['target_latent_masks'][0,:,0].permute(1,2,0).cpu().numpy().astype(bool)[...,0]\n",
    "\n",
    "# bbox = get_bbox_from_mask(np.array(mask))\n",
    "# face_bbox = get_facebbox_from_bbox(bbox, 32, 32, 120, 68, full_width=False)\n",
    "face_bbox = batch['clean_latent_bboxes'][0,0].numpy()\n",
    "\n",
    "draw_bboxes(Image.fromarray(mask).convert(\"RGB\").resize((960,544)), [face_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "for el in tqdm(train_els):\n",
    "    name = Path(el['meta']).parent.name\n",
    "    tmp_paths = list(mpath.glob(f'test3_2_cache_v2/{name}_*_fp.safetensors'))\n",
    "    if len(tmp_paths) > 0:\n",
    "        cache = load_file(tmp_paths[0])\n",
    "        cache['clean_latent_indices_int64'] = torch.tensor([0], dtype=torch.int64)\n",
    "        save_file(cache, tmp_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = Path(\"/groups/chenchen/patrick/OpenS2V-Nexus/datasets\")\n",
    "dset_name = \"test3_2_cache_v2\"\n",
    "candidates = sorted([x.name.split(\"_fp\")[0] for x in (dset_path / dset_name).glob(\"*_fp.safetensors\")])\n",
    "print(len(candidates))\n",
    "\n",
    "list_with_bbox = []\n",
    "for name in tqdm(candidates):\n",
    "    control_kwargs = load_file(dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "\n",
    "    entity_key = [k for k in control_kwargs.keys() if \"target_latent_masks_\" in k][0]\n",
    "    clean_key = [k for k in control_kwargs.keys() if \"latents_clean_\" in k][0]\n",
    "    entity_mask = control_kwargs[entity_key][0].permute(1,2,0).cpu().numpy().astype(bool)[...,0]\n",
    "    clean_latents = control_kwargs[clean_key]\n",
    "    w, h = entity_mask.shape[1], entity_mask.shape[0]\n",
    "    clean_w, clean_h = clean_latents.shape[3], clean_latents.shape[2]\n",
    "\n",
    "    # face_bbox = [\n",
    "    #     bbox[0], bbox[1], \n",
    "    #     min((bbox[0]*entity_mask.shape[1]+clean_w)/entity_mask.shape[1], 1.0),\n",
    "    #     min((bbox[1]*entity_mask.shape[0]+clean_h)/entity_mask.shape[1], 1.0),\n",
    "    # ]\n",
    "    bbox = get_bbox_from_mask(entity_mask)\n",
    "    face_bbox = get_facebbox_from_bbox(bbox, clean_w, clean_h, w, h, full_width=False)\n",
    "    clean_latent_bboxes = torch.tensor([face_bbox]).float()\n",
    "    control_kwargs[\"clean_latent_bboxes_float32\"] = clean_latent_bboxes\n",
    "    # draw_bboxes(Image.fromarray(entity_mask).convert(\"RGB\").resize((960,544)), [face_bbox])\n",
    "    \n",
    "    # if len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 4:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0,0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 3:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 1:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"].unsqueeze(0).float()\n",
    "\n",
    "    save_file(control_kwargs, dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "    if \"clean_latent_bboxes_float32\" in control_kwargs:\n",
    "        list_with_bbox.append(name)\n",
    "print(len(list_with_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872330",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckset_selector = BucketSelector([960, 544], True, False, \"fp\")\n",
    "\n",
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_els)):\n",
    "    # train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "    name = Path(batch['meta']).parent.name\n",
    "    meta = OmegaConf.load(batch['meta'])\n",
    "    keys = [0]\n",
    "    c_H, c_W = 256, 256\n",
    "\n",
    "    image_size = Image.open(batch['image_path']).convert(\"RGB\").size\n",
    "    bucket_reso = buckset_selector.get_bucket_resolution(image_size)\n",
    "    entity_bboxes = [meta['target_body'].get(str(x), [0.0,0.0,0.0,0.0]) for x in [keys]]\n",
    "    entity_bboxes = [[x[0]/meta['width'], x[1]/meta['height'], x[2]/meta['width'], x[3]/meta['height']] for x in entity_bboxes]\n",
    "    clean_latent_bboxes = torch.tensor([[bbox[0], bbox[1], bbox[0]+(c_H / bucket_reso[1]), bbox[1]+(c_W / bucket_reso[0])] for bbox in entity_bboxes]).float()\n",
    "\n",
    "    control_cache_path = (mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "    control_cache = load_file(control_cache_path)\n",
    "    control_cache['clean_latent_bboxes_float32'] = clean_latent_bboxes\n",
    "    save_file(control_cache, mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = load_vae(\"/lustre/fs1/home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\", 32, 128, device=device)\n",
    "vae.to(device)\n",
    "\n",
    "feature_extractor = SiglipImageProcessor(**FEATURE_EXTRACTOR_CONFIG)\n",
    "\n",
    "config = SiglipVisionConfig(**IMAGE_ENCODER_CONFIG)\n",
    "image_encoder = SiglipVisionModel._from_config(config, torch_dtype=torch.float16)\n",
    "state_dict = load_file(\"/lustre/fs1/home/yo564250/workspace/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\")\n",
    "image_encoder.load_state_dict(state_dict, strict=True, assign=True)\n",
    "image_encoder.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "samples = []\n",
    "for i in [0, 10]:\n",
    "    name = Path(test_els[i]['image_path']).parent.name\n",
    "    batch_path = mpath / f\"test3_2/{name}\"\n",
    "    meta = OmegaConf.load(batch_path / \"meta.yaml\")\n",
    "\n",
    "    prompt = meta['cap'][0]\n",
    "    control_image_path = batch_path / \"source_facecrop_0.png\"\n",
    "    mask_path = batch_path / \"target_bodymask_0.png\"\n",
    "    of = \"--of target_index=9,control_index=0,no_2x,no_4x,no_post --d 1111 --f 1 --s 25 --w 1280 --h 720\"\n",
    "    sample = f\"{prompt} --i {str(control_image_path.absolute())} --ci {str(control_image_path.absolute())} --em {str(mask_path.absolute())} {of}\"\n",
    "    samples.append(sample)\n",
    "\n",
    "Path(mpath / \"test3_2_sample_prompts.txt\").write_text(\"\\n\".join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = max(1, os.cpu_count() - 1)\n",
    "\n",
    "# all_latent_cache_paths = []\n",
    "# for _, batch in tqdm(dataset.retrieve_latent_cache_batches(num_workers)):\n",
    "#     filtered_batch = []\n",
    "#     for item in batch:\n",
    "#         if item.frame_count is None:\n",
    "#             all_latent_cache_paths.append(item.latent_cache_path)\n",
    "#             all_existing = os.path.exists(item.latent_cache_path)\n",
    "#         else:\n",
    "#             latent_f = (item.frame_count - 1) // 4 + 1\n",
    "#             num_sections = max(1, math.floor((latent_f - 1) / item.fp_latent_window_size))  # min 1 section\n",
    "#             all_existing = True\n",
    "#             for sec in range(num_sections):\n",
    "#                 p = append_section_idx_to_latent_cache_path(item.latent_cache_path, sec)\n",
    "#                 all_latent_cache_paths.append(p)\n",
    "#                 all_existing = all_existing and os.path.exists(p)\n",
    "\n",
    "#         if not all_existing:  # if any section cache is missing\n",
    "#             filtered_batch.append(item)\n",
    "\n",
    "#     if len(filtered_batch) == 0:  # all sections exist\n",
    "#         continue\n",
    "\n",
    "#     encode_and_save_batch_one_frame(\n",
    "#         vae, feature_extractor, image_encoder, filtered_batch, \n",
    "#         vanilla_sampling = False,\n",
    "#         one_frame_no_2x = True,\n",
    "#         one_frame_no_4x = True,\n",
    "#     )\n",
    "    \n",
    "# # normalize paths\n",
    "# all_latent_cache_paths = [os.path.normpath(p) for p in all_latent_cache_paths]\n",
    "# all_latent_cache_paths = set(all_latent_cache_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34962f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_clean1 = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_train.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# test_clean1 = np.delete(test_clean1, errorneous)\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in test_clean1])\n",
    "# )\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#     control_imgs_sizes.extend(control_img_sizes)\n",
    "# print(np.mean([list(x) for x in control_imgs_sizes], axis=0))\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# errorneous = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     try:\n",
    "#         control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#         control_imgs_sizes.extend(control_img_sizes)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {test_clean['image_path']}: {e}\")\n",
    "#         errorneous.append(i)\n",
    "\n",
    "# for i in erroneous:\n",
    "# for test_clean in test_clean1:\n",
    "\n",
    "# train_els = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# for i, batch in enumerate(train_els):\n",
    "#     train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
