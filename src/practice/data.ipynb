{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e3d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/ykwon4/.conda/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import sageattention\n",
      "Failed to import sageattention\n",
      "model_path is /projects/bffz/ykwon4/musubi-tuner/src/practice/../musubi_tuner/ckpts/hr16/yolox-onnx/yolox_l.torchscript.pt\n",
      "model_path is /projects/bffz/ykwon4/musubi-tuner/src/practice/../musubi_tuner/ckpts/hr16/DWPose-TorchScript-BatchSize5/dw-ll_ucoco_384_bs5.torchscript.pt\n",
      "\n",
      "DWPose: Using yolox_l.torchscript.pt for bbox detection and dw-ll_ucoco_384_bs5.torchscript.pt for pose estimation\n",
      "DWPose: Caching TorchScript module yolox_l.torchscript.pt on ...\n",
      "DWPose: Caching TorchScript module dw-ll_ucoco_384_bs5.torchscript.pt on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/ykwon4/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/u/ykwon4/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from omegaconf import OmegaConf\n",
    "import math, json, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Value\n",
    "from PIL import Image, ImageDraw\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import SiglipImageProcessor, SiglipVisionModel, SiglipVisionConfig\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from musubi_tuner.cache_latents import setup_parser_common, hv_setup_parser, preprocess_contents\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer, load_user_config, generate_dataset_group_by_blueprint\n",
    "from musubi_tuner.dataset.image_video_dataset import DatasetGroup, ImageDataset, ItemInfo, save_latent_cache_framepack, BucketSelector\n",
    "from musubi_tuner.frame_pack.clip_vision import hf_clip_vision_encode\n",
    "from musubi_tuner.frame_pack.framepack_utils import load_image_encoders, load_vae, FEATURE_EXTRACTOR_CONFIG, IMAGE_ENCODER_CONFIG\n",
    "from musubi_tuner.frame_pack.hunyuan import vae_encode\n",
    "from musubi_tuner.hunyuan_model.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\n",
    "from musubi_tuner.fpack_cache_latents import encode_and_save_batch_one_frame, append_section_idx_to_latent_cache_path\n",
    "from musubi_tuner.hv_train_network import collator_class, load_prompts\n",
    "from musubi_tuner.utils.bbox_utils import get_bbox_from_mask, get_mask_from_bboxes, draw_bboxes, get_facebbox_from_bbox, get_bbox_from_meta\n",
    "from musubi_tuner.utils.preproc_utils import get_text_preproc, prepare_control_inputs_for_entity, preproc_mask, postproc_imgs\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# mpath = Path('/groups/chenchen/patrick/OpenS2V-Nexus/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d452caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.hunyuan_model.vae:Loading 3D VAE model (884-16c-hy) from: /projects/bffz/ykwon4/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.hunyuan_model.vae:VAE to dtype: torch.float16\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Set chunk_size to 32 for CausalConv3d\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Enabled spatial tiling with min size 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "vae = load_vae(\"/projects/bffz/ykwon4/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\", 32, 128, device=device)\n",
    "vae.to(device)\n",
    "\n",
    "feature_extractor = SiglipImageProcessor(**FEATURE_EXTRACTOR_CONFIG)\n",
    "\n",
    "config = SiglipVisionConfig(**IMAGE_ENCODER_CONFIG)\n",
    "image_encoder = SiglipVisionModel._from_config(config, torch_dtype=torch.float16)\n",
    "state_dict = load_file(\"/projects/bffz/ykwon4/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\")\n",
    "image_encoder.load_state_dict(state_dict, strict=True, assign=True)\n",
    "image_encoder.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fb2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"fpack_cache_latents.py\",\n",
    "    \"--dataset_config\", \"/projects/bffz/ykwon4/test.toml\",\n",
    "    \"--vae\", \"~/patrick/ComfyUI/models/vae/hunyuan_video_vae_bf16.safetensors\",\n",
    "    \"--image_encoder\", \"~/patrick/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--vae_chunk_size\", \"32\",\n",
    "    \"--vae_spatial_tile_sample_min_size\", \"128\", \n",
    "    \"--skip_existing\", \"--keep_cache\", \"--one_frame\", \"--one_frame_no_2x\", \"--one_frame_no_4x\"\n",
    "]\n",
    "\n",
    "parser = setup_parser_common()\n",
    "parser = hv_setup_parser(parser)  # VAE\n",
    "# parser = framepack_setup_parser(parser)\n",
    "parser.add_argument(\"--image_encoder\", type=str, required=True)\n",
    "parser.add_argument(\"--f1\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_2x\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_4x\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "user_config = load_user_config(args.dataset_config)\n",
    "# blueprint = blueprint_generator.generate(user_config, args, architecture='fp')\n",
    "# train_dataset_group = generate_dataset_group_by_blueprint(blueprint.dataset_group, training=True)\n",
    "# dataset = train_dataset_group.datasets[0]\n",
    "\n",
    "# dataset = ImageDataset(\n",
    "#     resolution=(960, 544), caption_extension='.txt', batch_size=2, num_repeats=1, \n",
    "#     enable_bucket=True, bucket_no_upscale=False, \n",
    "#     image_directory=None, \n",
    "#     image_jsonl_file=str(mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\"), \n",
    "#     control_directory=None,\n",
    "#     cache_directory=str(mpath / \"test3_2_cache_v2\"), \n",
    "#     # cache_directory=None,\n",
    "#     fp_latent_window_size=9, fp_1f_clean_indices=[0], fp_1f_target_index=9, \n",
    "#     fp_1f_no_post=True, debug_dataset=False, architecture='fp', \n",
    "#     control_resolution=(256,256)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9d35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1056, 864, 9, 1, True) : 1690\n",
      "(1104, 832, 9, 1, True) : 2115\n",
      "(1472, 624, 9, 1, True) : 1870\n",
      "(1280, 720, 9, 1, True) : 6954\n",
      "(1168, 784, 9, 1, True) : 2269\n",
      "(1504, 608, 9, 1, True) : 1689\n",
      "(960, 960, 9, 1, True) : 1354\n",
      "(1072, 848, 9, 1, True) : 41\n",
      "(720, 1280, 9, 1, True) : 19\n",
      "(832, 1104, 9, 1, True) : 17\n",
      "(784, 1168, 9, 1, True) : 20\n",
      "(848, 1072, 9, 1, True) : 37\n",
      "(1296, 704, 9, 1, True) : 23\n"
     ]
    }
   ],
   "source": [
    "bm = train_dataset_group.datasets[0].batch_manager\n",
    "for k,v in bm.buckets.items():\n",
    "    print(f\"{k} : {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec39d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [04:20<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "res = (1504, 608)\n",
    "key = (res[0], res[1], 9, 1, True)\n",
    "bucket = bm.buckets[key]\n",
    "\n",
    "false_list = []\n",
    "buckets = [x for x in bm.bucket_batch_indices if x[0] == key]\n",
    "for idx, (bucket_reso, batch_idx) in tqdm(enumerate(buckets), total=len(buckets)):\n",
    "    start = batch_idx * bm.batch_size\n",
    "    end = min(start + bm.batch_size, len(bucket))\n",
    "\n",
    "    batch_tensor_data = defaultdict(list)\n",
    "    for item_info in bucket[start:end]:\n",
    "        sd_latent = load_file(item_info.latent_cache_path)\n",
    "\n",
    "        for content_key in sd_latent.keys():\n",
    "            if not content_key.endswith(\"_mask\"):\n",
    "                content_key_2 = content_key.rsplit(\"_\", 1)[0]  # remove dtype\n",
    "                if any([content_key_2.startswith(x) for x in ['latents_', 'target_latent_']]):\n",
    "                    content_key_2 = content_key_2.rsplit(\"_\", 1)[0]  # remove FxHxW\n",
    "            batch_tensor_data[content_key_2].append(sd_latent[content_key])\n",
    "\n",
    "    if len(set([x.shape for x in batch_tensor_data['latents']])) > 1:\n",
    "        print(f\"{[Path(x.latent_cache_path).stem for x in bucket[start:end]]}\")\n",
    "        false_list.append(idx)\n",
    "    elif len(set([x.shape for x in batch_tensor_data['target_latent_masks']])) > 1:\n",
    "        print(f\"{[Path(x.latent_cache_path).stem for x in bucket[start:end]]}\")\n",
    "        false_list.append(idx)\n",
    "    # try:\n",
    "    #     for i, key in enumerate(list(batch_tensor_data.keys())):\n",
    "    #         batch_tensor_data[key] = torch.stack(batch_tensor_data[key])\n",
    "    #         if key.startswith(\"latents_clean\"):\n",
    "    #             batch_tensor_data[key] = batch_tensor_data[key][:,:,:bm.control_count_per_image,:,:]\n",
    "    #         if key.startswith(\"target_latent_masks\"):\n",
    "    #             batch_tensor_data[key] = batch_tensor_data[key][:,:,:bm.control_count_per_image,:,:]\n",
    "    #         if key.startswith(\"clean_latent_bboxes\"):\n",
    "    #             batch_tensor_data[key] = batch_tensor_data[key][:,:bm.control_count_per_image,:]\n",
    "    # except Exception as e:\n",
    "    #     print(f\"{e} : {[Path(x.latent_cache_path).stem for x in bucket[start:end]]}\")\n",
    "    #     false_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196566c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in false_list:\n",
    "    bucket_reso, batch_idx = buckets[idx]\n",
    "    start = batch_idx * bm.batch_size\n",
    "    end = min(start + bm.batch_size, len(bucket))\n",
    "\n",
    "    for item_info in bucket[start:end]:\n",
    "        sd_latent = load_file(item_info.latent_cache_path)\n",
    "\n",
    "        for content_key in sd_latent.keys():\n",
    "            if not content_key.endswith(\"_mask\"):\n",
    "                content_key_2 = content_key.rsplit(\"_\", 1)[0]  # remove dtype\n",
    "                if any([content_key_2.startswith(x) for x in ['latents_', 'target_latent_']]):\n",
    "                    content_key_2 = content_key_2.rsplit(\"_\", 1)[0]  # remove FxHxW\n",
    "            if content_key_2 == 'latents':\n",
    "                # print(sd_latent[content_key].shape[-2:])\n",
    "                if list(sd_latent[content_key].shape[-2:]) != [56, 144]:\n",
    "                    print(item_info.latent_cache_path)\n",
    "                    # Path(item_info.latent_cache_path).unlink()\n",
    "                    # Path(item_info.text_encoder_output_cache_path).unlink()\n",
    "        # batch_tensor_data[content_key_2].append(sd_latent[content_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fccb54c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'clean_latent_indices': [tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0],\n",
       "              tensor[1] i64 [0]],\n",
       "             'latent_indices': [tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3],\n",
       "              tensor[1] i64 [3]],\n",
       "             'clean_latent_bboxes': [tensor[1, 4] x∈[0.025, 0.992] μ=0.419 σ=0.447 [[0.107, 0.025, 0.551, 0.992]],\n",
       "              tensor[1, 4] x∈[0.050, 0.922] μ=0.469 σ=0.383 [[0.280, 0.050, 0.625, 0.922]],\n",
       "              tensor[1, 4] x∈[0.119, 0.911] μ=0.571 σ=0.358 [[0.454, 0.119, 0.798, 0.911]],\n",
       "              tensor[1, 4] x∈[0.103, 0.794] μ=0.467 σ=0.336 [[0.263, 0.103, 0.706, 0.794]],\n",
       "              tensor[1, 4] x∈[0.026, 0.998] μ=0.655 σ=0.459 [[0.600, 0.026, 0.998, 0.994]],\n",
       "              tensor[1, 4] x∈[0.027, 0.966] μ=0.498 σ=0.421 [[0.285, 0.027, 0.714, 0.966]],\n",
       "              tensor[1, 4] x∈[0.079, 0.910] μ=0.518 σ=0.386 [[0.319, 0.079, 0.764, 0.910]],\n",
       "              tensor[1, 4] x∈[0.085, 0.841] μ=0.571 σ=0.346 [[0.565, 0.085, 0.841, 0.794]]],\n",
       "             'image_embeddings': [tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-59.938, 99.938] μ=0.021 σ=1.893,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.094, 98.500] μ=0.026 σ=1.946,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.250, 96.438] μ=0.025 σ=1.850,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-61.781, 99.125] μ=0.027 σ=1.945,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.625, 98.688] μ=0.024 σ=1.927,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-61.344, 90.562] μ=0.025 σ=1.906,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.281, 97.750] μ=0.025 σ=1.875,\n",
       "              tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.562, 94.375] μ=0.024 σ=1.997],\n",
       "             'latents': [tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-5.086, 3.688] μ=0.016 σ=0.948,\n",
       "              tensor[16, 1, 58, 140] f16 n=129920 (0.2Mb) x∈[-4.711, 4.207] μ=-0.028 σ=1.100,\n",
       "              tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.660, 4.102] μ=0.030 σ=1.013,\n",
       "              tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.434, 4.480] μ=0.016 σ=1.063,\n",
       "              tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.074, 2.852] μ=0.019 σ=0.934,\n",
       "              tensor[16, 1, 58, 140] f16 n=129920 (0.2Mb) x∈[-4.816, 4.273] μ=-0.005 σ=1.114,\n",
       "              tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-5.992, 4.738] μ=0.192 σ=1.452,\n",
       "              tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.746, 3.469] μ=-0.014 σ=1.065],\n",
       "             'latents_clean': [tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.199, 4.402] μ=0.275 σ=1.514,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.691, 4.402] μ=0.159 σ=1.551,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.676, 4.402] μ=0.142 σ=1.642,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.652, 4.402] μ=0.165 σ=1.614,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.949, 4.402] μ=0.168 σ=1.559,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.570, 4.402] μ=0.132 σ=1.591,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.625, 4.402] μ=0.195 σ=1.565,\n",
       "              tensor[16, 2, 64, 64] f16 n=131072 (0.2Mb) x∈[-4.824, 4.402] μ=0.167 σ=1.578],\n",
       "             'target_latent_masks': [tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.325 σ=0.468,\n",
       "              tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.176 σ=0.379,\n",
       "              tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.187 σ=0.389,\n",
       "              tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.185 σ=0.386,\n",
       "              tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.184 σ=0.387,\n",
       "              tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.226 σ=0.417,\n",
       "              tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.237 σ=0.422,\n",
       "              tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.139 σ=0.344]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1986\n",
    "\n",
    "bucket_reso, batch_idx = bm.bucket_batch_indices[idx]\n",
    "bucket = bm.buckets[bucket_reso]\n",
    "start, end = batch_idx * bm.batch_size, min(start + bm.batch_size, len(bucket))\n",
    "\n",
    "batch_tensor_data = defaultdict(list)\n",
    "for item_info in bucket[start:end]:\n",
    "    sd_latent = load_file(item_info.latent_cache_path)\n",
    "\n",
    "    for content_key in sd_latent.keys():\n",
    "        if not content_key.endswith(\"_mask\"):\n",
    "            content_key_2 = content_key.rsplit(\"_\", 1)[0]  # remove dtype\n",
    "            if any([content_key_2.startswith(x) for x in ['latents_', 'target_latent_']]):\n",
    "                content_key_2 = content_key_2.rsplit(\"_\", 1)[0]  # remove FxHxW\n",
    "        batch_tensor_data[content_key_2].append(sd_latent[content_key])\n",
    "\n",
    "batch_tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567c3891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18102 0\n",
      "18102.0\n"
     ]
    }
   ],
   "source": [
    "image_jsonl_file = Path(user_config['datasets'][0]['image_jsonl_file'])\n",
    "cache_directory = Path(user_config['datasets'][0]['cache_directory'])\n",
    "\n",
    "image_jsonls = [json.loads(x) for x in image_jsonl_file.read_text().split(\"\\n\")]\n",
    "cache_files = defaultdict(list)\n",
    "for x in cache_directory.glob(\"*.safetensors\"):\n",
    "    tokens = x.name.split(\"_\")\n",
    "    item_key = \"_\".join(tokens[:-2])\n",
    "    cache_files[item_key].append(x)\n",
    "\n",
    "remain_jsonl = []\n",
    "throwaway_jsonl = []\n",
    "for image_jsonl in image_jsonls:\n",
    "    item_key = Path(image_jsonl['image_path']).parent.name\n",
    "    if len(cache_files.get(item_key, [])) == 2:\n",
    "        remain_jsonl.append(image_jsonl)\n",
    "    else:\n",
    "        throwaway_jsonl.append(image_jsonl)\n",
    "print(len(remain_jsonl), len(throwaway_jsonl))\n",
    "\n",
    "# image_jsonl_file_v2 = (image_jsonl_file.parent / f\"{image_jsonl_file.stem}_v2.jsonl\")\n",
    "# image_jsonl_file_v2.write_text(\"\\n\".join([json.dumps(x) for x in remain_jsonl]))\n",
    "\n",
    "# images_v2 = [Path(image_jsonl['image_path']).parent.name for image_jsonl in remain_jsonl]\n",
    "# for item_key, cache_list in cache_files.items():\n",
    "#     if item_key not in images_v2:\n",
    "#         print(cache_list)\n",
    "#         for cache in cache_list:\n",
    "#             cache.unlink()\n",
    "print(sum([len(x) for _, x in cache_files.items()]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,batch = next(iter(dataset.retrieve_latent_cache_batches(4)))\n",
    "_, _, image, contents, content_masks, target_masks, clean_latent_bboxes = preprocess_contents(batch)\n",
    "print(image)\n",
    "print(contents)\n",
    "print(target_masks)\n",
    "print(clean_latent_bboxes)\n",
    "\n",
    "i = 0\n",
    "print(Path(batch[i].item_key).parent.name)\n",
    "meta_path = Path(batch[i].item_key).parent / \"meta.yaml\"\n",
    "meta = OmegaConf.load(meta_path)\n",
    "print(meta['target_body'], meta['width'], meta['height'])\n",
    "\n",
    "bboxes = get_bbox_from_meta(meta_path, 2)\n",
    "print(bboxes)\n",
    "\n",
    "mask = target_masks[i,0].permute(1,2,0).cpu().numpy().astype(bool).max(-1)\n",
    "face_bbox = clean_latent_bboxes[i,0].numpy()\n",
    "draw_bboxes(Image.fromarray(mask).convert(\"RGB\").resize((960,544)), face_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = image.to(vae.device, dtype=vae.dtype)  # B, C, H, W\n",
    "    contents = contents.to(vae.device, dtype=vae.dtype)  # B, C, F, H, W\n",
    "    target_masks = target_masks.to(vae.device, dtype=vae.dtype)\n",
    "\n",
    "    # VAE encode: we need to encode one frame at a time because VAE encoder has stride=4 for the time dimension except for the first frame.\n",
    "    target_latent = vae_encode(image, vae).to(\"cpu\")  # B, C, 1, H/8, W/8\n",
    "    clean_latents = [vae_encode(contents[:, :, idx : idx + 1], vae).to(\"cpu\") for idx in range(contents.shape[2])]\n",
    "    clean_latents = torch.cat(clean_latents, dim=2)  # B, C, F, H/8, W/8\n",
    "\n",
    "    # apply alphas to latents\n",
    "    for b, item in enumerate(batch):\n",
    "        for i, content_mask in enumerate(content_masks[b]):\n",
    "            if content_mask is not None:\n",
    "                # apply mask to the latents\n",
    "                # print(f\"Applying content mask for item {item.item_key}, frame {i}\")\n",
    "                clean_latents[b : b + 1, :, i : i + 1] *= content_mask\n",
    "\n",
    "    # Vision encoding per‑item (once): use control content because it is the start image\n",
    "    # images = [item.control_content[0] for item in batch]  # list of [H, W, C]\n",
    "    images = [item.embed_content for item in batch]\n",
    "\n",
    "    # encode image with image encoder\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            if image.shape[-1] == 4:\n",
    "                image = image[..., :3]\n",
    "            image_encoder_output = hf_clip_vision_encode(image, feature_extractor, image_encoder)\n",
    "            image_embeddings.append(image_encoder_output.last_hidden_state)\n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)  # B, LEN, 1152\n",
    "    image_embeddings = image_embeddings.to(\"cpu\")  # Save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e318ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.set_seed(0)\n",
    "# dataset.prepare_for_training()\n",
    "# # for _, batch in tqdm(dataset.retrieve_latent_cache_batches(4)):\n",
    "#     # items = batch\n",
    "# train_dataset_group = DatasetGroup([dataset])\n",
    "\n",
    "collator = collator_class(Value(\"i\", 0), Value(\"i\", 0), None)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_group,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dea9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2274 [00:57<1:41:25,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [16, 1, 56, 144] at entry 0 and [16, 1, 58, 140] at entry 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sIjZtzBoBbM_segment_16_step1-0-49_step2-0-49_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/pJsXYQJv3es_segment_15_step1-0-65_step2-0-65_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/XiQCDv4IMNc_segment_53_step1-10-139_step2-10-129_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_4_step1-0-183_step2-0-183_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/FkCllzcYH-A_segment_12_step1-0-61_step2-0-61_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/cATjlaTS7uI_segment_100_step1-0-73_step2-0-73_step4_step5_step6_1424x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/KuI-h24wLq4_segment_0_step1-0-145_step2-0-145_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [1, 2, 56, 144] at entry 0 and [1, 2, 58, 140] at entry 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sIjZtzBoBbM_segment_16_step1-0-49_step2-0-49_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/pJsXYQJv3es_segment_15_step1-0-65_step2-0-65_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/XiQCDv4IMNc_segment_53_step1-10-139_step2-10-129_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_4_step1-0-183_step2-0-183_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/FkCllzcYH-A_segment_12_step1-0-61_step2-0-61_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/cATjlaTS7uI_segment_100_step1-0-73_step2-0-73_step4_step5_step6_1424x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/KuI-h24wLq4_segment_0_step1-0-145_step2-0-145_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sInLfhpnLh0_segment_8_step1-64-215_step2-64-151_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list indices must be integers or slices, not tuple\n",
      "2240 2248 6\n",
      "ItemInfo(item_key=KuI-h24wLq4_segment_0_step1-0-145_step2-0-145_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/KuI-h24wLq4_segment_0_step1-0-145_step2-0-145_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "{'clean_latent_indices': tensor[8, 1] i64 \u001b[38;2;127;127;127mall_zeros\u001b[0m [[0], [0], [0], [0], [0], [0], [0], [0]], 'latent_indices': tensor[8, 1] i64 x∈[3, 3] μ=3.000 σ=0. [[3], [3], [3], [3], [3], [3], [3], [3]], 'clean_latent_bboxes': tensor[8, 1, 4] n=32 x∈[-0.017, 0.996] μ=0.499 σ=0.326, 'image_embeddings': tensor[8, 729, 1152] f16 n=6718464 (13Mb) x∈[-62.250, 102.625] μ=0.023 σ=1.926, 'latents': [tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-2.871, 4.746] μ=0.023 σ=1.125, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.656, 3.752] μ=0.118 σ=1.102, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.750, 3.654] μ=-0.249 σ=1.258, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.588, 4.688] μ=-0.108 σ=1.010, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.004, 3.977] μ=0.081 σ=1.162, tensor[16, 1, 58, 140] f16 n=129920 (0.2Mb) x∈[-4.523, 3.977] μ=-0.046 σ=1.040, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-2.598, 4.730] μ=0.227 σ=1.378, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-2.857, 4.941] μ=-0.091 σ=0.997], 'latents_clean': tensor[8, 16, 1, 64, 64] f16 n=524288 (1Mb) x∈[-4.914, 4.699] μ=-0.056 σ=1.213, 'target_latent_masks': [tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.472 σ=0.499, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.233 σ=0.420, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.149 σ=0.353, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.236 σ=0.422, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.423 σ=0.492, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.167 σ=0.371, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.218 σ=0.411, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.204 σ=0.401], 'llama_vec': [tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.969, 9.156] μ=-0.000 σ=0.260, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-36.281, 9.031] μ=-0.000 σ=0.258, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.500, 7.566] μ=-0.000 σ=0.254, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.438, 8.359] μ=-0.000 σ=0.252, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-30.547, 9.570] μ=-0.000 σ=0.241, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.875, 9.641] μ=-0.000 σ=0.213, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-35.562, 11.008] μ=-0.001 σ=0.240, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-30.688, 9.195] μ=-0.001 σ=0.248], 'llama_attention_mask': [tensor[512] bool x∈[False, True] μ=0.117 σ=0.322, tensor[512] bool x∈[False, True] μ=0.113 σ=0.317, tensor[512] bool x∈[False, True] μ=0.109 σ=0.312, tensor[512] bool x∈[False, True] μ=0.113 σ=0.317, tensor[512] bool x∈[False, True] μ=0.104 σ=0.305, tensor[512] bool x∈[False, True] μ=0.076 σ=0.266, tensor[512] bool x∈[False, True] μ=0.105 σ=0.307, tensor[512] bool x∈[False, True] μ=0.109 σ=0.312], 'clip_l_pooler': [tensor[768] 3Kb x∈[-3.927, 6.396] μ=-0.104 σ=1.006, tensor[768] 3Kb x∈[-3.017, 4.003] μ=-0.110 σ=0.975, tensor[768] 3Kb x∈[-3.805, 3.118] μ=-0.111 σ=0.991, tensor[768] 3Kb x∈[-4.224, 6.188] μ=-0.106 σ=1.015, tensor[768] 3Kb x∈[-3.407, 3.646] μ=-0.107 σ=0.989, tensor[768] 3Kb x∈[-3.854, 6.735] μ=-0.105 σ=1.017, tensor[768] 3Kb x∈[-3.509, 4.754] μ=-0.106 σ=1.001, tensor[768] 3Kb x∈[-3.657, 6.121] μ=-0.106 σ=1.020]}\n",
      "[tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.472 σ=0.499, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.233 σ=0.420, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.149 σ=0.353, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.236 σ=0.422, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.423 σ=0.492, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.167 σ=0.371, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.218 σ=0.411, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.204 σ=0.401]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 42/2274 [02:13<1:51:57,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [16, 1, 56, 144] at entry 0 and [16, 1, 58, 140] at entry 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/pBe6xEPi42k_segment_66_step1-0-125_step2-0-125_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sEc9Smax2jc_segment_1_step1-10-73_step2-10-63_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/m19eAq4TU_A_segment_0_step1-158-369_step2-158-211_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/41BSrVeb-h0_segment_67_step1-0-57_step2-0-57_step4_step5_step6_1424x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/IcqxoGyV6Ig_segment_56_step1-0-79_step2-0-79_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/oHEl_KFJFh8_segment_0_step1-20-97_step2-20-77_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Pooj2tBUBEM_segment_537_step1-0-75_step2-0-75_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [1, 2, 56, 144] at entry 0 and [1, 2, 58, 140] at entry 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/pBe6xEPi42k_segment_66_step1-0-125_step2-0-125_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/sEc9Smax2jc_segment_1_step1-10-73_step2-10-63_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/m19eAq4TU_A_segment_0_step1-158-369_step2-158-211_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/41BSrVeb-h0_segment_67_step1-0-57_step2-0-57_step4_step5_step6_1424x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/IcqxoGyV6Ig_segment_56_step1-0-79_step2-0-79_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/oHEl_KFJFh8_segment_0_step1-20-97_step2-20-77_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Pooj2tBUBEM_segment_537_step1-0-75_step2-0-75_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/a8UnNvKK3Ew_segment_3_step1-0-87_step2-0-87_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list indices must be integers or slices, not tuple\n",
      "176 184 6\n",
      "ItemInfo(item_key=Pooj2tBUBEM_segment_537_step1-0-75_step2-0-75_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Pooj2tBUBEM_segment_537_step1-0-75_step2-0-75_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "{'clean_latent_indices': tensor[8, 1] i64 \u001b[38;2;127;127;127mall_zeros\u001b[0m [[0], [0], [0], [0], [0], [0], [0], [0]], 'latent_indices': tensor[8, 1] i64 x∈[3, 3] μ=3.000 σ=0. [[3], [3], [3], [3], [3], [3], [3], [3]], 'clean_latent_bboxes': tensor[8, 1, 4] n=32 x∈[0.004, 0.993] μ=0.495 σ=0.313, 'image_embeddings': tensor[8, 729, 1152] f16 n=6718464 (13Mb) x∈[-62.562, 103.688] μ=0.024 σ=1.929, 'latents': [tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.293, 4.453] μ=0.034 σ=1.043, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.898, 4.297] μ=-0.354 σ=1.565, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.898, 3.809] μ=-0.270 σ=1.174, tensor[16, 1, 58, 140] f16 n=129920 (0.2Mb) x∈[-4.727, 4.145] μ=-0.025 σ=1.056, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.414, 4.008] μ=-0.061 σ=1.091, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.008, 4.766] μ=0.157 σ=1.223, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.090, 3.916] μ=-0.028 σ=1.114, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.887, 3.482] μ=0.134 σ=0.959], 'latents_clean': tensor[8, 16, 1, 64, 64] f16 n=524288 (1Mb) x∈[-4.867, 4.230] μ=-0.171 σ=1.294, 'target_latent_masks': [tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.154 σ=0.358, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.201 σ=0.399, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.247 σ=0.429, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.222 σ=0.413, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.153 σ=0.357, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.081 σ=0.271, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.338 σ=0.471, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.261 σ=0.438], 'llama_vec': [tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-34.812, 8.258] μ=-0.000 σ=0.259, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-33.094, 10.914] μ=-0.000 σ=0.213, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.438, 7.816] μ=-0.001 σ=0.231, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-30.547, 9.398] μ=-0.001 σ=0.244, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-29.453, 8.352] μ=-0.000 σ=0.221, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-31.531, 9.141] μ=-0.001 σ=0.266, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-36.812, 10.188] μ=-0.000 σ=0.246, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-31.156, 8.531] μ=-0.001 σ=0.229], 'llama_attention_mask': [tensor[512] bool x∈[False, True] μ=0.109 σ=0.312, tensor[512] bool x∈[False, True] μ=0.082 σ=0.275, tensor[512] bool x∈[False, True] μ=0.104 σ=0.305, tensor[512] bool x∈[False, True] μ=0.102 σ=0.302, tensor[512] bool x∈[False, True] μ=0.080 σ=0.272, tensor[512] bool x∈[False, True] μ=0.127 σ=0.333, tensor[512] bool x∈[False, True] μ=0.104 σ=0.305, tensor[512] bool x∈[False, True] μ=0.096 σ=0.294], 'clip_l_pooler': [tensor[768] 3Kb x∈[-3.705, 3.204] μ=-0.108 σ=0.988, tensor[768] 3Kb x∈[-4.644, 7.325] μ=-0.105 σ=1.020, tensor[768] 3Kb x∈[-4.690, 4.100] μ=-0.114 σ=1.009, tensor[768] 3Kb x∈[-3.921, 6.564] μ=-0.106 σ=1.014, tensor[768] 3Kb x∈[-3.566, 3.140] μ=-0.112 σ=0.990, tensor[768] 3Kb x∈[-3.156, 5.357] μ=-0.103 σ=0.999, tensor[768] 3Kb x∈[-3.023, 5.700] μ=-0.107 σ=1.004, tensor[768] 3Kb x∈[-3.616, 6.813] μ=-0.107 σ=1.018]}\n",
      "[tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.154 σ=0.358, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.201 σ=0.399, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.247 σ=0.429, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.222 σ=0.413, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.153 σ=0.357, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.081 σ=0.271, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.338 σ=0.471, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.261 σ=0.438]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2274 [05:24<1:45:08,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [16, 1, 56, 144] at entry 0 and [16, 1, 58, 140] at entry 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/PJGkKLXiJVE_segment_2_step1-0-75_step2-0-75_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/JBh9ixSWQJg_segment_25_step1-2-119_step2-2-117_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/WNlsokkSoMg_segment_38_step1-0-163_step2-0-163_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/_Ua-d9OeUOg_segment_45_step1-0-53_step2-0-53_step4_step5_step6_1408x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/7Eql1iL9E4E_segment_78_step1-0-53_step2-0-53_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/FsfXheYWats_segment_21_step1-2-145_step2-36-143_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/YE4DVdcn43w_segment_5_step1-18-113_step2-18-95_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [1, 2, 56, 144] at entry 0 and [1, 2, 58, 140] at entry 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ItemInfo(item_key=Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n",
      "INFO:root:['/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/PJGkKLXiJVE_segment_2_step1-0-75_step2-0-75_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/JBh9ixSWQJg_segment_25_step1-2-119_step2-2-117_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/WNlsokkSoMg_segment_38_step1-0-163_step2-0-163_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/_Ua-d9OeUOg_segment_45_step1-0-53_step2-0-53_step4_step5_step6_1408x0576_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/7Eql1iL9E4E_segment_78_step1-0-53_step2-0-53_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/FsfXheYWats_segment_21_step1-2-145_step2-36-143_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/YE4DVdcn43w_segment_5_step1-18-113_step2-18-95_step4_step5_step6_1520x0608_fp.safetensors', '/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/Hj6mXN5u3DM_segment_13_step1-0-469_step2-0-469_step4_step5_step6_1520x0608_fp.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list indices must be integers or slices, not tuple\n",
      "536 544 6\n",
      "ItemInfo(item_key=YE4DVdcn43w_segment_5_step1-18-113_step2-18-95_step4_step5_step6, caption=, original_size=(1520, 608), bucket_size=(1504, 608, 9, 1, True), frame_count=None, latent_cache_path=/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v4_cache_selected/YE4DVdcn43w_segment_5_step1-18-113_step2-18-95_step4_step5_step6_1520x0608_fp.safetensors, content=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2274 [05:27<1:43:39,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean_latent_indices': tensor[8, 1] i64 \u001b[38;2;127;127;127mall_zeros\u001b[0m [[0], [0], [0], [0], [0], [0], [0], [0]], 'latent_indices': tensor[8, 1] i64 x∈[3, 3] μ=3.000 σ=0. [[3], [3], [3], [3], [3], [3], [3], [3]], 'clean_latent_bboxes': tensor[8, 1, 4] n=32 x∈[0.033, 0.996] μ=0.520 σ=0.339, 'image_embeddings': tensor[8, 729, 1152] f16 n=6718464 (13Mb) x∈[-62.625, 101.688] μ=0.023 σ=1.873, 'latents': [tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-2.627, 2.959] μ=-0.001 σ=0.912, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.629, 4.789] μ=-0.004 σ=1.475, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.875, 4.438] μ=-0.008 σ=0.924, tensor[16, 1, 58, 140] f16 n=129920 (0.2Mb) x∈[-2.717, 3.980] μ=0.233 σ=1.174, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.406, 3.551] μ=-0.311 σ=1.399, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-3.566, 4.332] μ=0.033 σ=1.056, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-2.895, 3.598] μ=-0.009 σ=0.885, tensor[16, 1, 56, 144] f16 n=129024 (0.2Mb) x∈[-4.434, 3.758] μ=0.083 σ=1.027], 'latents_clean': tensor[8, 16, 1, 64, 64] f16 n=524288 (1Mb) x∈[-4.898, 4.547] μ=-0.033 σ=1.108, 'target_latent_masks': [tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.174 σ=0.377, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.159 σ=0.364, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.156 σ=0.362, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.249 σ=0.430, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.243 σ=0.426, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.127 σ=0.331, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.188 σ=0.389, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.201 σ=0.399], 'llama_vec': [tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-31.344, 9.836] μ=-0.000 σ=0.237, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-29.797, 8.727] μ=-0.001 σ=0.233, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-33.969, 9.227] μ=-7.659e-05 σ=0.246, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-32.781, 9.961] μ=-0.000 σ=0.224, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-31.219, 10.000] μ=-0.001 σ=0.248, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-29.797, 10.281] μ=-0.000 σ=0.259, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-31.031, 9.117] μ=-0.000 σ=0.219, tensor[512, 4096] f16 n=2097152 (4Mb) x∈[-29.938, 9.938] μ=-0.000 σ=0.262], 'llama_attention_mask': [tensor[512] bool x∈[False, True] μ=0.100 σ=0.300, tensor[512] bool x∈[False, True] μ=0.096 σ=0.294, tensor[512] bool x∈[False, True] μ=0.104 σ=0.305, tensor[512] bool x∈[False, True] μ=0.086 σ=0.281, tensor[512] bool x∈[False, True] μ=0.107 σ=0.310, tensor[512] bool x∈[False, True] μ=0.119 σ=0.324, tensor[512] bool x∈[False, True] μ=0.086 σ=0.281, tensor[512] bool x∈[False, True] μ=0.121 σ=0.327], 'clip_l_pooler': [tensor[768] 3Kb x∈[-3.509, 3.741] μ=-0.112 σ=0.984, tensor[768] 3Kb x∈[-4.130, 2.781] μ=-0.116 σ=0.993, tensor[768] 3Kb x∈[-3.155, 3.718] μ=-0.109 σ=0.989, tensor[768] 3Kb x∈[-3.963, 5.631] μ=-0.104 σ=1.007, tensor[768] 3Kb x∈[-3.047, 4.818] μ=-0.107 σ=0.992, tensor[768] 3Kb x∈[-3.262, 3.490] μ=-0.110 σ=0.984, tensor[768] 3Kb x∈[-4.184, 4.791] μ=-0.109 σ=1.002, tensor[768] 3Kb x∈[-3.481, 3.366] μ=-0.108 σ=0.991]}\n",
      "[tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.174 σ=0.377, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.159 σ=0.364, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.156 σ=0.362, tensor[1, 2, 58, 140] f16 n=16240 (32Kb) x∈[0., 1.000] μ=0.249 σ=0.430, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.243 σ=0.426, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.127 σ=0.331, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.188 σ=0.389, tensor[1, 2, 56, 144] f16 n=16128 (32Kb) x∈[0., 1.000] μ=0.201 σ=0.399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 105/2274 [05:45<1:58:58,  3.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(batch['latents'], batch['latents_clean'])\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_dataloader):\n",
    "    # print(batch['latents'], batch['latents_clean'])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "for el in tqdm(train_els):\n",
    "    name = Path(el['meta']).parent.name\n",
    "    tmp_paths = list(mpath.glob(f'test3_2_cache_v2/{name}_*_fp.safetensors'))\n",
    "    if len(tmp_paths) > 0:\n",
    "        cache = load_file(tmp_paths[0])\n",
    "        cache['clean_latent_indices_int64'] = torch.tensor([0], dtype=torch.int64)\n",
    "        save_file(cache, tmp_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = Path(\"/groups/chenchen/patrick/OpenS2V-Nexus/datasets\")\n",
    "dset_name = \"test3_2_cache_v2\"\n",
    "candidates = sorted([x.name.split(\"_fp\")[0] for x in (dset_path / dset_name).glob(\"*_fp.safetensors\")])\n",
    "print(len(candidates))\n",
    "\n",
    "list_with_bbox = []\n",
    "for name in tqdm(candidates):\n",
    "    control_kwargs = load_file(dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "\n",
    "    entity_key = [k for k in control_kwargs.keys() if \"target_latent_masks_\" in k][0]\n",
    "    clean_key = [k for k in control_kwargs.keys() if \"latents_clean_\" in k][0]\n",
    "    entity_mask = control_kwargs[entity_key][0].permute(1,2,0).cpu().numpy().astype(bool)[...,0]\n",
    "    clean_latents = control_kwargs[clean_key]\n",
    "    w, h = entity_mask.shape[1], entity_mask.shape[0]\n",
    "    clean_w, clean_h = clean_latents.shape[3], clean_latents.shape[2]\n",
    "\n",
    "    # face_bbox = [\n",
    "    #     bbox[0], bbox[1], \n",
    "    #     min((bbox[0]*entity_mask.shape[1]+clean_w)/entity_mask.shape[1], 1.0),\n",
    "    #     min((bbox[1]*entity_mask.shape[0]+clean_h)/entity_mask.shape[1], 1.0),\n",
    "    # ]\n",
    "    bbox = get_bbox_from_mask(entity_mask)\n",
    "    face_bbox = get_facebbox_from_bbox(bbox, clean_w, clean_h, w, h, full_width=False)\n",
    "    clean_latent_bboxes = torch.tensor([face_bbox]).float()\n",
    "    control_kwargs[\"clean_latent_bboxes_float32\"] = clean_latent_bboxes\n",
    "    # draw_bboxes(Image.fromarray(entity_mask).convert(\"RGB\").resize((960,544)), [face_bbox])\n",
    "    \n",
    "    # if len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 4:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0,0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 3:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 1:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"].unsqueeze(0).float()\n",
    "\n",
    "    save_file(control_kwargs, dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "    if \"clean_latent_bboxes_float32\" in control_kwargs:\n",
    "        list_with_bbox.append(name)\n",
    "print(len(list_with_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872330",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckset_selector = BucketSelector([960, 544], True, False, \"fp\")\n",
    "\n",
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_els)):\n",
    "    # train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "    name = Path(batch['meta']).parent.name\n",
    "    meta = OmegaConf.load(batch['meta'])\n",
    "    keys = [0]\n",
    "    c_H, c_W = 256, 256\n",
    "\n",
    "    image_size = Image.open(batch['image_path']).convert(\"RGB\").size\n",
    "    bucket_reso = buckset_selector.get_bucket_resolution(image_size)\n",
    "    entity_bboxes = [meta['target_body'].get(str(x), [0.0,0.0,0.0,0.0]) for x in [keys]]\n",
    "    entity_bboxes = [[x[0]/meta['width'], x[1]/meta['height'], x[2]/meta['width'], x[3]/meta['height']] for x in entity_bboxes]\n",
    "    clean_latent_bboxes = torch.tensor([[bbox[0], bbox[1], bbox[0]+(c_H / bucket_reso[1]), bbox[1]+(c_W / bucket_reso[0])] for bbox in entity_bboxes]).float()\n",
    "\n",
    "    control_cache_path = (mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "    control_cache = load_file(control_cache_path)\n",
    "    control_cache['clean_latent_bboxes_float32'] = clean_latent_bboxes\n",
    "    save_file(control_cache, mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "samples = []\n",
    "for i in [0, 10]:\n",
    "    name = Path(test_els[i]['image_path']).parent.name\n",
    "    batch_path = mpath / f\"test3_2/{name}\"\n",
    "    meta = OmegaConf.load(batch_path / \"meta.yaml\")\n",
    "\n",
    "    prompt = meta['cap'][0]\n",
    "    control_image_path = batch_path / \"source_facecrop_0.png\"\n",
    "    mask_path = batch_path / \"target_bodymask_0.png\"\n",
    "    of = \"--of target_index=9,control_index=0,no_2x,no_4x,no_post --d 1111 --f 1 --s 25 --w 1280 --h 720\"\n",
    "    sample = f\"{prompt} --i {str(control_image_path.absolute())} --ci {str(control_image_path.absolute())} --em {str(mask_path.absolute())} {of}\"\n",
    "    samples.append(sample)\n",
    "\n",
    "Path(mpath / \"test3_2_sample_prompts.txt\").write_text(\"\\n\".join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = max(1, os.cpu_count() - 1)\n",
    "\n",
    "# all_latent_cache_paths = []\n",
    "# for _, batch in tqdm(dataset.retrieve_latent_cache_batches(num_workers)):\n",
    "#     filtered_batch = []\n",
    "#     for item in batch:\n",
    "#         if item.frame_count is None:\n",
    "#             all_latent_cache_paths.append(item.latent_cache_path)\n",
    "#             all_existing = os.path.exists(item.latent_cache_path)\n",
    "#         else:\n",
    "#             latent_f = (item.frame_count - 1) // 4 + 1\n",
    "#             num_sections = max(1, math.floor((latent_f - 1) / item.fp_latent_window_size))  # min 1 section\n",
    "#             all_existing = True\n",
    "#             for sec in range(num_sections):\n",
    "#                 p = append_section_idx_to_latent_cache_path(item.latent_cache_path, sec)\n",
    "#                 all_latent_cache_paths.append(p)\n",
    "#                 all_existing = all_existing and os.path.exists(p)\n",
    "\n",
    "#         if not all_existing:  # if any section cache is missing\n",
    "#             filtered_batch.append(item)\n",
    "\n",
    "#     if len(filtered_batch) == 0:  # all sections exist\n",
    "#         continue\n",
    "\n",
    "#     encode_and_save_batch_one_frame(\n",
    "#         vae, feature_extractor, image_encoder, filtered_batch, \n",
    "#         vanilla_sampling = False,\n",
    "#         one_frame_no_2x = True,\n",
    "#         one_frame_no_4x = True,\n",
    "#     )\n",
    "    \n",
    "# # normalize paths\n",
    "# all_latent_cache_paths = [os.path.normpath(p) for p in all_latent_cache_paths]\n",
    "# all_latent_cache_paths = set(all_latent_cache_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34962f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_clean1 = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_train.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# test_clean1 = np.delete(test_clean1, errorneous)\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in test_clean1])\n",
    "# )\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#     control_imgs_sizes.extend(control_img_sizes)\n",
    "# print(np.mean([list(x) for x in control_imgs_sizes], axis=0))\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# errorneous = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     try:\n",
    "#         control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#         control_imgs_sizes.extend(control_img_sizes)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {test_clean['image_path']}: {e}\")\n",
    "#         errorneous.append(i)\n",
    "\n",
    "# for i in erroneous:\n",
    "# for test_clean in test_clean1:\n",
    "\n",
    "# train_els = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# for i, batch in enumerate(train_els):\n",
    "#     train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
