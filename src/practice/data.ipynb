{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e3d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import sageattention\n",
      "Successfully imported sageattention\n",
      "model_path is /lustre/fs1/home/yo564250/workspace/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/ckpts/hr16/yolox-onnx/yolox_l.torchscript.pt\n",
      "model_path is /lustre/fs1/home/yo564250/workspace/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/ckpts/hr16/DWPose-TorchScript-BatchSize5/dw-ll_ucoco_384_bs5.torchscript.pt\n",
      "\n",
      "DWPose: Using yolox_l.torchscript.pt for bbox detection and dw-ll_ucoco_384_bs5.torchscript.pt for pose estimation\n",
      "DWPose: Caching TorchScript module yolox_l.torchscript.pt on ...\n",
      "DWPose: Caching TorchScript module dw-ll_ucoco_384_bs5.torchscript.pt on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yo564250/conda-envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yo564250/conda-envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from omegaconf import OmegaConf\n",
    "import math, json, shutil\n",
    "import numpy as np\n",
    "import pyiqa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Value\n",
    "from PIL import Image, ImageDraw\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import SiglipImageProcessor, SiglipVisionModel, SiglipVisionConfig\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from musubi_tuner.cache_latents import setup_parser_common, hv_setup_parser, preprocess_contents\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer, load_user_config, generate_dataset_group_by_blueprint\n",
    "from musubi_tuner.dataset.image_video_dataset import DatasetGroup, ImageDataset, ItemInfo, save_latent_cache_framepack, BucketSelector\n",
    "from musubi_tuner.frame_pack.clip_vision import hf_clip_vision_encode\n",
    "from musubi_tuner.frame_pack.framepack_utils import load_image_encoders, load_vae, FEATURE_EXTRACTOR_CONFIG, IMAGE_ENCODER_CONFIG\n",
    "from musubi_tuner.frame_pack.hunyuan import vae_encode\n",
    "from musubi_tuner.hunyuan_model.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\n",
    "from musubi_tuner.fpack_cache_latents import encode_and_save_batch_one_frame, append_section_idx_to_latent_cache_path\n",
    "from musubi_tuner.hv_train_network import collator_class, load_prompts\n",
    "from musubi_tuner.utils.bbox_utils import get_bbox_from_mask, get_mask_from_bboxes, draw_bboxes, get_facebbox_from_bbox, get_bbox_from_meta\n",
    "from musubi_tuner.utils.preproc_utils import get_text_preproc, prepare_control_inputs_for_entity, preproc_mask, postproc_imgs\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# mpath = Path('/groups/chenchen/patrick/OpenS2V-Nexus/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196dee39",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test3_part3_v3_cache_topiq_scores.csv\")\n",
    "path1 = Path(\"/lustre/fs1/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_part3_v3_cache\")\n",
    "path2 = Path(\"/lustre/fs1/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_part3_v4_cache\")\n",
    "path2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name in df.loc[((df['topiq_score'] > 0.35) & (df['topiq_face_score'] > 0.4))].file:\n",
    "    name = name.split(\"_step4\")[0]\n",
    "    files = list(path1.glob(f\"{name}*.safetensors\"))\n",
    "    for file in files:\n",
    "        shutil.copy(file, path2 / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a8183",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# path1 = Path(\"/work/hdd/bffz/ykwon4/Subjects200K/subjects200k_cache\")\n",
    "path1 = Path(\"/groups/chenchen/patrick/Anime-Shooter/test_output_chara2_cache\")\n",
    "path2 = Path(\"/groups/chenchen/patrick/Anime-Shooter/test_output_chara2_cache_subset\")\n",
    "path2.mkdir(parents=True, exist_ok=True)\n",
    "for file in sorted(list(path1.glob(\"*\")))[:2000]:\n",
    "    shutil.copy(file, path2 / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608ce10",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=3, python fpack_cache_latents.py \\\n",
    "# --dataset_config \"/home/ce407038/patrick/datasets/OpenS2V-Nexus/tmp_4.toml\" \\\n",
    "# --vae \"/home/ce407038/patrick/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\" \\\n",
    "# --image_encoder \"/home/ce407038/patrick/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\" \\\n",
    "# --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128  \\\n",
    "# --skip_existing --keep_cache --one_frame --one_frame_no_2x --one_frame_no_4x\n",
    "\n",
    "iqa_metric = pyiqa.create_metric('topiq_nr', device=device)\n",
    "iqa_metric_face = pyiqa.create_metric('topiq_nr-face', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d452caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = load_vae(\"/projects/bffz/ykwon4/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\", 32, 128, device=device)\n",
    "vae.to(device)\n",
    "\n",
    "feature_extractor = SiglipImageProcessor(**FEATURE_EXTRACTOR_CONFIG)\n",
    "\n",
    "config = SiglipVisionConfig(**IMAGE_ENCODER_CONFIG)\n",
    "image_encoder = SiglipVisionModel._from_config(config, torch_dtype=torch.float16)\n",
    "state_dict = load_file(\"/projects/bffz/ykwon4/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\")\n",
    "image_encoder.load_state_dict(state_dict, strict=True, assign=True)\n",
    "image_encoder.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b787fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible_files = list(Path(\"/work/hdd/bffz/ykwon4/OpenS2V-Nexus/test3_part3_v3_cache/\").glob(\"*_fp.safetensors\"))\n",
    "file = possible_files[10]\n",
    "\n",
    "batch = load_file(str(file))\n",
    "\n",
    "source_face_key = [k for k in batch.keys() if 'latents_clean' in k][0]\n",
    "target_image_key = [k for k in batch.keys() if 'latents_' in k and k != source_face_key][0]\n",
    "target_mask_key = [k for k in batch.keys() if 'target_latent_masks' in k][0]\n",
    "source_face = postproc_imgs(batch[source_face_key].unsqueeze(0), vae)[0]\n",
    "target_image = postproc_imgs(batch[target_image_key].unsqueeze(0), vae)[0]\n",
    "target_mask = Image.fromarray(batch[target_mask_key][:,0,:,:].permute(1,2,0)[...,0].cpu().numpy() > 0)\n",
    "\n",
    "try:\n",
    "    topiq_score = iqa_metric(Image.fromarray(target_image)).item()\n",
    "except AssertionError as e:\n",
    "    topiq_score = 0.0\n",
    "try:\n",
    "    topiq_face_score = iqa_metric_face(Image.fromarray(source_face)).item()\n",
    "except AssertionError as e:\n",
    "    topiq_face_score = 0.0\n",
    "\n",
    "print(topiq_score, topiq_face_score)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(source_face)\n",
    "ax[1].imshow(target_image)\n",
    "ax[2].imshow(target_mask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adddd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = load_file(\"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_part2_v2_cache/_U_VP1ara2E_segment_19_step1-0-79_step2-0-79_step4_step5_step6_1280x0720_fp.safetensors\")\n",
    "# batch = load_file(\"/groups/chenchen/patrick/Anime-Shooter/test_output_cache/__nYnywW7fk-segment_1432_2836-shot_1432_1498_1280x0720_fp.safetensors\")\n",
    "possible_files = list(Path(\"/work/hdd/bffz/ykwon4/Anime-Shooter/test_output_chara2_cache\").glob(\"*_fp.safetensors\"))\n",
    "\n",
    "df = []\n",
    "for i, file in tqdm(enumerate(possible_files), total=len(possible_files)):\n",
    "    batch = load_file(str(file))\n",
    "\n",
    "    source_face_key = [k for k in batch.keys() if 'latents_clean' in k][0]\n",
    "    target_image_key = [k for k in batch.keys() if 'latents_' in k and k != source_face_key][0]\n",
    "    target_mask_key = [k for k in batch.keys() if 'target_latent_masks' in k][0]\n",
    "    source_face = postproc_imgs(batch[source_face_key].unsqueeze(0), vae)[0]\n",
    "    target_image = postproc_imgs(batch[target_image_key].unsqueeze(0), vae)[0]\n",
    "    target_mask = Image.fromarray(batch[target_mask_key][:,0,:,:].permute(1,2,0)[...,0].cpu().numpy() > 0)\n",
    "\n",
    "    try:\n",
    "        topiq_score = iqa_metric(Image.fromarray(target_image)).item()\n",
    "    except AssertionError as e:\n",
    "        topiq_score = 0.0\n",
    "    # try:\n",
    "    #     topiq_face_score = iqa_metric_face(Image.fromarray(source_face)).item()\n",
    "    # except AssertionError as e:\n",
    "    #     topiq_face_score = 0.0\n",
    "    df.append({\n",
    "        \"file\": str(file.stem), \n",
    "        \"topiq_score\": topiq_score,\n",
    "        # \"topiq_face_score\": topiq_face_score\n",
    "    })\n",
    "    if i % 10 == 0:\n",
    "        pd.DataFrame(df).to_csv(\"Anime-Shooter_topiq_scores.csv\", index=False)\n",
    "        \n",
    "pd.DataFrame(df).to_csv(\"Anime-Shooter_topiq_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"fpack_cache_latents.py\",\n",
    "    \"--dataset_config\", \"/home/ce407038/patrick/datasets/OpenS2V-Nexus/tmp.toml\",\n",
    "    \"--vae\", \"~/patrick/ComfyUI/models/vae/hunyuan_video_vae_bf16.safetensors\",\n",
    "    \"--image_encoder\", \"~/patrick/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--vae_chunk_size\", \"32\",\n",
    "    \"--vae_spatial_tile_sample_min_size\", \"128\", \n",
    "    \"--skip_existing\", \"--keep_cache\", \"--one_frame\", \"--one_frame_no_2x\", \"--one_frame_no_4x\"\n",
    "]\n",
    "\n",
    "parser = setup_parser_common()\n",
    "parser = hv_setup_parser(parser)  # VAE\n",
    "# parser = framepack_setup_parser(parser)\n",
    "parser.add_argument(\"--image_encoder\", type=str, required=True)\n",
    "parser.add_argument(\"--f1\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_2x\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_4x\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "user_config = load_user_config(args.dataset_config)\n",
    "blueprint = blueprint_generator.generate(user_config, args, architecture='fp')\n",
    "train_dataset_group = generate_dataset_group_by_blueprint(blueprint.dataset_group)\n",
    "dataset = train_dataset_group.datasets[0]\n",
    "\n",
    "# dataset = ImageDataset(\n",
    "#     resolution=(960, 544), caption_extension='.txt', batch_size=2, num_repeats=1, \n",
    "#     enable_bucket=True, bucket_no_upscale=False, \n",
    "#     image_directory=None, \n",
    "#     image_jsonl_file=str(mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\"), \n",
    "#     control_directory=None,\n",
    "#     cache_directory=str(mpath / \"test3_2_cache_v2\"), \n",
    "#     # cache_directory=None,\n",
    "#     fp_latent_window_size=9, fp_1f_clean_indices=[0], fp_1f_target_index=9, \n",
    "#     fp_1f_no_post=True, debug_dataset=False, architecture='fp', \n",
    "#     control_resolution=(256,256)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,batch = next(iter(dataset.retrieve_latent_cache_batches(4)))\n",
    "_, _, image, contents, content_masks, target_masks, clean_latent_bboxes = preprocess_contents(batch)\n",
    "print(image)\n",
    "print(contents)\n",
    "print(target_masks)\n",
    "print(clean_latent_bboxes)\n",
    "\n",
    "i = 0\n",
    "print(Path(batch[i].item_key).parent.name)\n",
    "meta_path = Path(batch[i].item_key).parent / \"meta.yaml\"\n",
    "meta = OmegaConf.load(meta_path)\n",
    "print(meta['target_body'], meta['width'], meta['height'])\n",
    "\n",
    "bboxes = get_bbox_from_meta(meta_path, 2)\n",
    "print(bboxes)\n",
    "\n",
    "mask = target_masks[i,0].permute(1,2,0).cpu().numpy().astype(bool).max(-1)\n",
    "face_bbox = clean_latent_bboxes[i,0].numpy()\n",
    "draw_bboxes(Image.fromarray(mask).convert(\"RGB\").resize((960,544)), face_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = image.to(vae.device, dtype=vae.dtype)  # B, C, H, W\n",
    "    contents = contents.to(vae.device, dtype=vae.dtype)  # B, C, F, H, W\n",
    "    target_masks = target_masks.to(vae.device, dtype=vae.dtype)\n",
    "\n",
    "    # VAE encode: we need to encode one frame at a time because VAE encoder has stride=4 for the time dimension except for the first frame.\n",
    "    target_latent = vae_encode(image, vae).to(\"cpu\")  # B, C, 1, H/8, W/8\n",
    "    clean_latents = [vae_encode(contents[:, :, idx : idx + 1], vae).to(\"cpu\") for idx in range(contents.shape[2])]\n",
    "    clean_latents = torch.cat(clean_latents, dim=2)  # B, C, F, H/8, W/8\n",
    "\n",
    "    # apply alphas to latents\n",
    "    for b, item in enumerate(batch):\n",
    "        for i, content_mask in enumerate(content_masks[b]):\n",
    "            if content_mask is not None:\n",
    "                # apply mask to the latents\n",
    "                # print(f\"Applying content mask for item {item.item_key}, frame {i}\")\n",
    "                clean_latents[b : b + 1, :, i : i + 1] *= content_mask\n",
    "\n",
    "    # Vision encoding perâ€‘item (once): use control content because it is the start image\n",
    "    # images = [item.control_content[0] for item in batch]  # list of [H, W, C]\n",
    "    images = [item.embed_content for item in batch]\n",
    "\n",
    "    # encode image with image encoder\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            if image.shape[-1] == 4:\n",
    "                image = image[..., :3]\n",
    "            image_encoder_output = hf_clip_vision_encode(image, feature_extractor, image_encoder)\n",
    "            image_embeddings.append(image_encoder_output.last_hidden_state)\n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)  # B, LEN, 1152\n",
    "    image_embeddings = image_embeddings.to(\"cpu\")  # Save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_seed(0)\n",
    "dataset.prepare_for_training()\n",
    "# for _, batch in tqdm(dataset.retrieve_latent_cache_batches(4)):\n",
    "    # items = batch\n",
    "train_dataset_group = DatasetGroup([dataset])\n",
    "\n",
    "collator = collator_class(Value(\"i\", 0), Value(\"i\", 0), None)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_group,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "for el in tqdm(train_els):\n",
    "    name = Path(el['meta']).parent.name\n",
    "    tmp_paths = list(mpath.glob(f'test3_2_cache_v2/{name}_*_fp.safetensors'))\n",
    "    if len(tmp_paths) > 0:\n",
    "        cache = load_file(tmp_paths[0])\n",
    "        cache['clean_latent_indices_int64'] = torch.tensor([0], dtype=torch.int64)\n",
    "        save_file(cache, tmp_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = Path(\"/groups/chenchen/patrick/OpenS2V-Nexus/datasets\")\n",
    "dset_name = \"test3_2_cache_v2\"\n",
    "candidates = sorted([x.name.split(\"_fp\")[0] for x in (dset_path / dset_name).glob(\"*_fp.safetensors\")])\n",
    "print(len(candidates))\n",
    "\n",
    "list_with_bbox = []\n",
    "for name in tqdm(candidates):\n",
    "    control_kwargs = load_file(dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "\n",
    "    entity_key = [k for k in control_kwargs.keys() if \"target_latent_masks_\" in k][0]\n",
    "    clean_key = [k for k in control_kwargs.keys() if \"latents_clean_\" in k][0]\n",
    "    entity_mask = control_kwargs[entity_key][0].permute(1,2,0).cpu().numpy().astype(bool)[...,0]\n",
    "    clean_latents = control_kwargs[clean_key]\n",
    "    w, h = entity_mask.shape[1], entity_mask.shape[0]\n",
    "    clean_w, clean_h = clean_latents.shape[3], clean_latents.shape[2]\n",
    "\n",
    "    # face_bbox = [\n",
    "    #     bbox[0], bbox[1], \n",
    "    #     min((bbox[0]*entity_mask.shape[1]+clean_w)/entity_mask.shape[1], 1.0),\n",
    "    #     min((bbox[1]*entity_mask.shape[0]+clean_h)/entity_mask.shape[1], 1.0),\n",
    "    # ]\n",
    "    bbox = get_bbox_from_mask(entity_mask)\n",
    "    face_bbox = get_facebbox_from_bbox(bbox, clean_w, clean_h, w, h, full_width=False)\n",
    "    clean_latent_bboxes = torch.tensor([face_bbox]).float()\n",
    "    control_kwargs[\"clean_latent_bboxes_float32\"] = clean_latent_bboxes\n",
    "    # draw_bboxes(Image.fromarray(entity_mask).convert(\"RGB\").resize((960,544)), [face_bbox])\n",
    "    \n",
    "    # if len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 4:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0,0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 3:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"][0].float()\n",
    "    # elif len(control_kwargs[\"clean_latent_bboxes_float32\"].shape) == 1:\n",
    "    #     control_kwargs[\"clean_latent_bboxes_float32\"] = control_kwargs[\"clean_latent_bboxes_float32\"].unsqueeze(0).float()\n",
    "\n",
    "    save_file(control_kwargs, dset_path / f\"{dset_name}/{name}_fp.safetensors\")\n",
    "    if \"clean_latent_bboxes_float32\" in control_kwargs:\n",
    "        list_with_bbox.append(name)\n",
    "print(len(list_with_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872330",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckset_selector = BucketSelector([960, 544], True, False, \"fp\")\n",
    "\n",
    "train_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_els)):\n",
    "    # train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "    name = Path(batch['meta']).parent.name\n",
    "    meta = OmegaConf.load(batch['meta'])\n",
    "    keys = [0]\n",
    "    c_H, c_W = 256, 256\n",
    "\n",
    "    image_size = Image.open(batch['image_path']).convert(\"RGB\").size\n",
    "    bucket_reso = buckset_selector.get_bucket_resolution(image_size)\n",
    "    entity_bboxes = [meta['target_body'].get(str(x), [0.0,0.0,0.0,0.0]) for x in [keys]]\n",
    "    entity_bboxes = [[x[0]/meta['width'], x[1]/meta['height'], x[2]/meta['width'], x[3]/meta['height']] for x in entity_bboxes]\n",
    "    clean_latent_bboxes = torch.tensor([[bbox[0], bbox[1], bbox[0]+(c_H / bucket_reso[1]), bbox[1]+(c_W / bucket_reso[0])] for bbox in entity_bboxes]).float()\n",
    "\n",
    "    control_cache_path = (mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "    control_cache = load_file(control_cache_path)\n",
    "    control_cache['clean_latent_bboxes_float32'] = clean_latent_bboxes\n",
    "    save_file(control_cache, mpath / f'test3_2_cache_v2/{name}_1280x0720_fp.safetensors')\n",
    "\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_els = [json.loads(x) for x in \n",
    "    (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "]\n",
    "samples = []\n",
    "for i in [0, 10]:\n",
    "    name = Path(test_els[i]['image_path']).parent.name\n",
    "    batch_path = mpath / f\"test3_2/{name}\"\n",
    "    meta = OmegaConf.load(batch_path / \"meta.yaml\")\n",
    "\n",
    "    prompt = meta['cap'][0]\n",
    "    control_image_path = batch_path / \"source_facecrop_0.png\"\n",
    "    mask_path = batch_path / \"target_bodymask_0.png\"\n",
    "    of = \"--of target_index=9,control_index=0,no_2x,no_4x,no_post --d 1111 --f 1 --s 25 --w 1280 --h 720\"\n",
    "    sample = f\"{prompt} --i {str(control_image_path.absolute())} --ci {str(control_image_path.absolute())} --em {str(mask_path.absolute())} {of}\"\n",
    "    samples.append(sample)\n",
    "\n",
    "Path(mpath / \"test3_2_sample_prompts.txt\").write_text(\"\\n\".join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = max(1, os.cpu_count() - 1)\n",
    "\n",
    "# all_latent_cache_paths = []\n",
    "# for _, batch in tqdm(dataset.retrieve_latent_cache_batches(num_workers)):\n",
    "#     filtered_batch = []\n",
    "#     for item in batch:\n",
    "#         if item.frame_count is None:\n",
    "#             all_latent_cache_paths.append(item.latent_cache_path)\n",
    "#             all_existing = os.path.exists(item.latent_cache_path)\n",
    "#         else:\n",
    "#             latent_f = (item.frame_count - 1) // 4 + 1\n",
    "#             num_sections = max(1, math.floor((latent_f - 1) / item.fp_latent_window_size))  # min 1 section\n",
    "#             all_existing = True\n",
    "#             for sec in range(num_sections):\n",
    "#                 p = append_section_idx_to_latent_cache_path(item.latent_cache_path, sec)\n",
    "#                 all_latent_cache_paths.append(p)\n",
    "#                 all_existing = all_existing and os.path.exists(p)\n",
    "\n",
    "#         if not all_existing:  # if any section cache is missing\n",
    "#             filtered_batch.append(item)\n",
    "\n",
    "#     if len(filtered_batch) == 0:  # all sections exist\n",
    "#         continue\n",
    "\n",
    "#     encode_and_save_batch_one_frame(\n",
    "#         vae, feature_extractor, image_encoder, filtered_batch, \n",
    "#         vanilla_sampling = False,\n",
    "#         one_frame_no_2x = True,\n",
    "#         one_frame_no_4x = True,\n",
    "#     )\n",
    "    \n",
    "# # normalize paths\n",
    "# all_latent_cache_paths = [os.path.normpath(p) for p in all_latent_cache_paths]\n",
    "# all_latent_cache_paths = set(all_latent_cache_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34962f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_clean1 = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_train.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# test_clean1 = np.delete(test_clean1, errorneous)\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in test_clean1])\n",
    "# )\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#     control_imgs_sizes.extend(control_img_sizes)\n",
    "# print(np.mean([list(x) for x in control_imgs_sizes], axis=0))\n",
    "\n",
    "# control_imgs_sizes = []\n",
    "# errorneous = []\n",
    "# for i, test_clean in enumerate(tqdm(test_clean1)):\n",
    "#     try:\n",
    "#         control_img_sizes = [Image.open(v).size for k,v in test_clean.items() if 'control_path' in k]\n",
    "#         control_imgs_sizes.extend(control_img_sizes)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {test_clean['image_path']}: {e}\")\n",
    "#         errorneous.append(i)\n",
    "\n",
    "# for i in erroneous:\n",
    "# for test_clean in test_clean1:\n",
    "\n",
    "# train_els = [json.loads(x) for x in \n",
    "#     (mpath / \"OpenS2V_part1_test3_2_test.jsonl\").read_text().split('\\n')\n",
    "# ]\n",
    "# for i, batch in enumerate(train_els):\n",
    "#     train_els[i]['meta'] = str(Path(batch['image_path']).parent / \"meta.yaml\")\n",
    "\n",
    "# (mpath / \"OpenS2V_part1_test3_2_test_v2.jsonl\").write_text(\n",
    "#     \"\\n\".join([json.dumps(x) for x in train_els])\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
