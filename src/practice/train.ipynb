{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "sys.argv = [\n",
    "    \"fpack_train_network.py\",\n",
    "    \"--dit\", \"/home/yo564250/workspace/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors\",\n",
    "    \"--vae\", \"/home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\",\n",
    "    \"--text_encoder1\", \"/home/yo564250/workspace/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors\",\n",
    "    \"--text_encoder2\", \"/home/yo564250/workspace/ComfyUI/models/text_encoders/clip_l.safetensors\",\n",
    "    \"--image_encoder\", \"/home/yo564250/workspace/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--dataset_config\", \"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/OpenS2V_part1_test3_2.toml\",\n",
    "    \"--sdpa\", \"--mixed_precision\", \"bf16\", \"--one_frame\",\n",
    "    \"--optimizer_type\", \"adamw8bit\", \n",
    "    \"--learning_rate\", \"2e-4\", \n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--timestep_sampling\", \"shift\", \n",
    "    \"--weighting_scheme\", \"none\", \n",
    "    \"--discrete_flow_shift\", \"3.0\",\n",
    "    \"--max_data_loader_n_workers\", \"8\", \n",
    "    \"--persistent_data_loader_workers\",\n",
    "    \"--network_module\", \"networks.lora_framepack\", \n",
    "    \"--network_dim\", \"32\",\n",
    "    \"--max_train_epochs\", \"16\", \n",
    "    \"--save_every_n_epochs\", \"1\", \n",
    "    \"--seed\", \"42\",\n",
    "    \"--sample_prompts\", \"/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2_sample_prompts.txt\",\n",
    "    \"--sample_every_n_epochs\", \"1\", \n",
    "    \"--sample_at_first\",\n",
    "    \"--output_dir\", \"outputs/training/idmask_control_lora\", \n",
    "    \"--output_name\", \"idmask_control_lora_test1\",\n",
    "    \"--logging_dir\", \"outputs/training/idmask_control_lora/logs\", \n",
    "    \"--log_with\", \"tensorboard\",\n",
    "    \"--remove_embedding\", \"--use_attention_controlimage_masking\"\n",
    "]\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from omegaconf import OmegaConf\n",
    "from argparse import Namespace\n",
    "import math, json\n",
    "import numpy as np\n",
    "from multiprocessing import Value\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from musubi_tuner.dataset import config_utils\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer\n",
    "from musubi_tuner.networks import lora_framepack\n",
    "from musubi_tuner.hv_train_network import collator_class, setup_parser_common, read_config_from_file, load_prompts\n",
    "from musubi_tuner.fpack_train_network import framepack_setup_parser, FramePackNetworkTrainer\n",
    "\n",
    "parser = setup_parser_common()\n",
    "parser = framepack_setup_parser(parser)\n",
    "args = parser.parse_args()\n",
    "args = read_config_from_file(args, parser)\n",
    "args.vae_dtype = \"float16\"  # fixed\n",
    "args.dit_dtype = \"bfloat16\"  # fixed\n",
    "args.sample_solver = \"unipc\"  # for sample generation, fixed to unipc\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "trainer = FramePackNetworkTrainer()\n",
    "trainer.handle_model_specific_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be330c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "user_config = config_utils.load_user_config(args.dataset_config)\n",
    "blueprint = blueprint_generator.generate(user_config, args, architecture=trainer.architecture)\n",
    "train_dataset_group = config_utils.generate_dataset_group_by_blueprint(blueprint.dataset_group, training=True)\n",
    "\n",
    "collator = collator_class(Value(\"i\", 0), Value(\"i\", 0), None)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_group,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=4,\n",
    "    persistent_workers=args.persistent_data_loader_workers,\n",
    ")\n",
    "batch = next(iter(train_dataloader))\n",
    "# for batch in train_dataloader:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac306a",
   "metadata": {},
   "source": [
    "```bash\n",
    "accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 fpack_train_network.py \\\n",
    "    --dit /home/yo564250/workspace/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors \\\n",
    "    --vae /home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt \\\n",
    "    --text_encoder1 /home/yo564250/workspace/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors \\\n",
    "    --text_encoder2 /home/yo564250/workspace/ComfyUI/models/text_encoders/clip_l.safetensors \\\n",
    "    --image_encoder /home/yo564250/workspace/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors \\\n",
    "    --dataset_config /groups/chenchen/patrick/OpenS2V-Nexus/datasets/OpenS2V_part1_test3_2.toml \\\n",
    "    --sdpa --mixed_precision bf16 --one_frame \\\n",
    "    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing \\\n",
    "    --timestep_sampling shift --weighting_scheme none --discrete_flow_shift 3.0 \\\n",
    "    --max_data_loader_n_workers 8 --persistent_data_loader_workers --split_attn \\\n",
    "    --network_module networks.lora_framepack --network_dim 32 \\\n",
    "    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42 \\\n",
    "    --sample_prompts /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2_sample_prompts.txt \\\n",
    "    --sample_every_n_steps 250 --sample_at_first \\\n",
    "    --output_dir outputs/training/idmask_control_lora_wrope_v1 --output_name idmask_control_lora_wrope_v1_4 \\\n",
    "    --logging_dir outputs/training/idmask_control_lora_wrope_v1/logs --log_with tensorboard \\\n",
    "    --remove_embedding --use_attention_controlimage_masking --sample_with_latentbbox_rope\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d7beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.fpack_train_network:cache Text Encoder outputs for sample prompt: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2_sample_prompts.txt\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 1 tokenizer\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 1 from /home/yo564250/workspace/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 2 tokenizer\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 2 from /home/yo564250/workspace/ComfyUI/models/text_encoders/clip_l.safetensors\n",
      "INFO:musubi_tuner.fpack_train_network:cache Text Encoder outputs for prompt: A man seated on a green cushioned chair in a dimly lit room with a backdrop that includes climbing holds. He is dressed casually in a red shirt and jeans. In front of him on the cushion are several items, including a deck of playing cards, a small black bag, and a pair of gloves. The man appears to be engaged in a conversation or presentation, as he is gesturing with his hands and speaking. He picks up the gloves and holds them up, possibly demonstrating or explaining something about them. The lighting in the room is focused on the man, creating a spotlight effect. The overall setting suggests an informal or casual environment, possibly related to climbing or outdoor activities, given the backdrop.\n",
      "INFO:musubi_tuner.fpack_train_network:cache Text Encoder outputs for prompt: \n",
      "INFO:musubi_tuner.fpack_train_network:cache Text Encoder outputs for prompt: Three individuals seated closely together in what appears to be a casual indoor setting. The person in the center is wearing a gray hoodie with pink accents and has light-colored hair. To the left, another individual is dressed in a red shirt with a graphic design, and to the right, a person with long dark hair is wearing a light-colored top. The background includes a wall with a colorful mural or artwork, and the room has a modern, cozy ambiance with soft lighting. The individuals are engaged in conversation, with the central figure speaking and the others listening and reacting with smiles and nods. The camera remains stationary, capturing the scene from a medium shot perspective.\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading image encoder feature extractor\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading image encoder from /home/yo564250/workspace/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding image to image encoder context: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/5SmM5l0R9Sk_segment_18_step1-0-65_step2-0-65_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding image to image encoder context: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Loading VAE model from /home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\n",
      "INFO:musubi_tuner.hunyuan_model.vae:Loading 3D VAE model (884-16c-hy) from: /home/yo564250/workspace/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\n",
      "INFO:musubi_tuner.hunyuan_model.vae:VAE to dtype: torch.float16\n",
      "INFO:musubi_tuner.fpack_train_network:Loading DiT model from /home/yo564250/workspace/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Creating HunyuanVideoTransformer3DModelPacked\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Image Projection: False\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Loading DiT model from /home/yo564250/workspace/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors, device=cuda:0\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Loaded DiT model from /home/yo564250/workspace/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors, info=_IncompatibleKeys(missing_keys=[], unexpected_keys=['image_projection.down.bias', 'image_projection.down.weight', 'image_projection.up.bias', 'image_projection.up.weight'])\n",
      "INFO:musubi_tuner.networks.lora:create LoRA network from weights\n",
      "INFO:musubi_tuner.networks.lora:create LoRA for U-Net/DiT: 440 modules.\n",
      "INFO:musubi_tuner.networks.lora:weights are merged\n",
      "INFO:root:Gradient checkpointing enabled for HunyuanVideoTransformer3DModelPacked.\n"
     ]
    }
   ],
   "source": [
    "# prepare dtype\n",
    "weight_dtype = torch.bfloat16\n",
    "dit_dtype = torch.bfloat16\n",
    "dit_weight_dtype = torch.bfloat16\n",
    "\n",
    "# get embedding for sampling images\n",
    "vae_dtype = torch.float16\n",
    "sample_parameters = trainer.process_sample_prompts(args, Namespace(**{'device': device}), args.sample_prompts)\n",
    "\n",
    "# Load VAE model for sampling images: VAE is loaded to cpu to save gpu memory\n",
    "vae = trainer.load_vae(args, vae_dtype=vae_dtype, vae_path=args.vae)\n",
    "vae.requires_grad_(False)\n",
    "vae.eval()\n",
    "vae.to(device)\n",
    "\n",
    "transformer = trainer.load_transformer(\n",
    "    Namespace(**{'device': device}), args, args.dit, \"torch\", args.split_attn, device, dit_weight_dtype\n",
    ")\n",
    "transformer.eval()\n",
    "transformer.requires_grad_(False)\n",
    "\n",
    "# network = lora_framepack.create_arch_network(\n",
    "#     1.0, args.network_dim, args.network_alpha,\n",
    "#     vae, None, transformer,\n",
    "#     neuron_dropout=args.network_dropout,\n",
    "# )\n",
    "# # apply network to DiT\n",
    "# network.apply_to(None, transformer, apply_text_encoder=False, apply_unet=True)\n",
    "weights_sd = load_file(\"/home/yo564250/workspace/whisperer/related/framepackbase/musubi-tuner/outputs/training/idmask_control_lora/idmask_control_lora_test3-000008.safetensors\")\n",
    "module = lora_framepack.create_arch_network_from_weights(\n",
    "    1.0, weights_sd, unet=transformer, for_inference=True\n",
    ")\n",
    "module.merge_to(None, transformer, weights_sd, weight_dtype, \"cpu\")\n",
    "\n",
    "\n",
    "transformer.enable_gradient_checkpointing()\n",
    "# network.enable_gradient_checkpointing()  # may have no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04a75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.hv_train_network:prompt: Three individuals seated closely together in what appears to be a casual indoor setting. The person in the center is wearing a gray hoodie with pink accents and has light-colored hair. To the left, another individual is dressed in a red shirt with a graphic design, and to the right, a person with long dark hair is wearing a light-colored top. The background includes a wall with a colorful mural or artwork, and the room has a modern, cozy ambiance with soft lighting. The individuals are engaged in conversation, with the central figure speaking and the others listening and reacting with smiles and nods. The camera remains stationary, capturing the scene from a medium shot perspective.\n",
      "INFO:musubi_tuner.hv_train_network:height: 720\n",
      "INFO:musubi_tuner.hv_train_network:width: 1280\n",
      "INFO:musubi_tuner.hv_train_network:frame count: 1\n",
      "INFO:musubi_tuner.hv_train_network:sample steps: 25\n",
      "INFO:musubi_tuner.hv_train_network:guidance scale: 10.0\n",
      "INFO:musubi_tuner.hv_train_network:discrete flow shift: 14.5\n",
      "INFO:musubi_tuner.hv_train_network:seed: 1111\n",
      "INFO:musubi_tuner.hv_train_network:image path: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding image to latent space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.fpack_train_network:Encoding control image: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding entity mask: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/target_bodmask_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Set index for clean latent 1x: ['0']\n",
      "INFO:musubi_tuner.fpack_train_network:Set index for target: 9\n",
      "INFO:musubi_tuner.fpack_train_network:No clean_latents_2x\n",
      "INFO:musubi_tuner.fpack_train_network:No clean_latents_4x\n",
      "INFO:musubi_tuner.fpack_train_network:One frame inference. clean_latent: torch.Size([1, 16, 1, 34, 25]) latent_indices: tensor[1, 1] i64 [[9]], clean_latent_indices: tensor[1, 1] i64 [[0]], num_frames: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7de0551234a42c5871f50c593578d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.fpack_train_network:Waiting for 5 seconds to finish block swap\n",
      "INFO:musubi_tuner.fpack_generate_video:Decoding video...\n",
      "INFO:musubi_tuner.fpack_generate_video:Bulk decoding or one frame inference\n",
      "INFO:musubi_tuner.fpack_generate_video:Decoded. Pixel shape torch.Size([1, 3, 1, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    trainer.sample_image_inference(\n",
    "        Namespace(**{'device': device}), args, transformer, dit_dtype, vae, \n",
    "        \".\", sample_parameters[1], 0, 0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
