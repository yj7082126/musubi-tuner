{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c736ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from dataclasses import asdict\n",
    "from omegaconf import OmegaConf\n",
    "from argparse import Namespace\n",
    "import math, json, glob, shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Value\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d34edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file, save_file\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eece6734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import sageattention\n",
      "Failed to import sageattention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Xformers is installed!\n",
      "INFO:root:Flash Attn is not installed!\n",
      "INFO:root:Sage Attn is not installed!\n"
     ]
    }
   ],
   "source": [
    "from musubi_tuner.dataset import config_utils\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer\n",
    "# from musubi_tuner.networks import lora_framepack\n",
    "from musubi_tuner.hv_train_network import collator_class, setup_parser_common, read_config_from_file\n",
    "from musubi_tuner.fpack_train_network import framepack_setup_parser, FramePackNetworkTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e343cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"fpack_train_network.py\",\n",
    "    \"--dit\", \"/projects/bffz/ykwon4/ComfyUI/models/diffusion_models/FramePackI2V_HY_bf16.safetensors\",\n",
    "    \"--vae\", \"/projects/bffz/ykwon4/ComfyUI/models/vae/hunyuan-video-t2v-720p-vae.pt\",\n",
    "    \"--text_encoder1\", \"/projects/bffz/ykwon4/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors\",\n",
    "    \"--text_encoder2\", \"/projects/bffz/ykwon4/ComfyUI/models/text_encoders/clip_l.safetensors\",\n",
    "    \"--image_encoder\", \"/projects/bffz/ykwon4/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--dataset_config\", \"/projects/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v2.toml\",\n",
    "    \"--sdpa\", \"--mixed_precision\", \"bf16\", \"--one_frame\",\n",
    "    \"--optimizer_type\", \"adamw8bit\", \n",
    "    \"--learning_rate\", \"2e-4\", \n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--timestep_sampling\", \"shift\", \n",
    "    \"--weighting_scheme\", \"none\", \n",
    "    \"--discrete_flow_shift\", \"3.0\",\n",
    "    \"--max_data_loader_n_workers\", \"8\", \n",
    "    \"--persistent_data_loader_workers\",\n",
    "    \"--network_module\", \"networks.lora_framepack\", \n",
    "    \"--network_dim\", \"32\",\n",
    "    \"--max_train_epochs\", \"16\", \n",
    "    \"--save_every_n_epochs\", \"1\", \n",
    "    \"--seed\", \"42\",\n",
    "    \"--sample_prompts\", \"/projects/bffz/ykwon4/OpenS2V-Nexus/test3_part2_sample_prompts.txt\",\n",
    "    \"--sample_every_n_epochs\", \"1\", \n",
    "    \"--sample_at_first\",\n",
    "    \"--output_dir\", \"outputs/training/idmask_control_lora\", \n",
    "    \"--output_name\", \"idmask_control_lora_test1\",\n",
    "    \"--logging_dir\", \"outputs/training/idmask_control_lora/logs\", \n",
    "    \"--log_with\", \"tensorboard\",\n",
    "    \"--remove_embedding\", \"--use_attention_controlimage_masking\"\n",
    "]\n",
    "\n",
    "parser = setup_parser_common()\n",
    "parser = framepack_setup_parser(parser)\n",
    "args = parser.parse_args()\n",
    "args = read_config_from_file(args, parser)\n",
    "args.vae_dtype = \"float16\"  # fixed\n",
    "args.dit_dtype = \"bfloat16\"  # fixed\n",
    "args.sample_solver = \"unipc\"  # for sample generation, fixed to unipc\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "trainer = FramePackNetworkTrainer()\n",
    "trainer.handle_model_specific_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d67a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.dataset.image_video_dataset:load image jsonl from /projects/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v2.jsonl\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:loaded 25813 images\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 25813 images with 1 control images per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 25813 masks with 1 control masks per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:found 25813 metadata with 1 bbox paths per image in JSONL data\n",
      "INFO:musubi_tuner.dataset.config_utils:[Dataset 0]\n",
      "  is_image_dataset: True\n",
      "  resolution: (960, 544)\n",
      "  batch_size: 16\n",
      "  num_repeats: 1\n",
      "  caption_extension: \".txt\"\n",
      "  enable_bucket: False\n",
      "  bucket_no_upscale: False\n",
      "  cache_directory: \"/projects/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v2_cache\"\n",
      "  debug_dataset: False\n",
      "  item_name_type: \"basename\"  \n",
      "    image_directory: \"None\"\n",
      "    image_jsonl_file: \"/projects/bffz/ykwon4/OpenS2V-Nexus/test3_part2_v2.jsonl\"\n",
      "    fp_latent_window_size: 9\n",
      "    fp_1f_clean_indices: [0]\n",
      "    fp_1f_target_index: 3\n",
      "    fp_1f_no_post: True, \n",
      "    control_count_per_image: 1,\n",
      "    control_resolution: (256, 256)\n",
      "\n",
      "\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:bucket: (960, 544, 9, 1, True), count: 25813\n",
      "INFO:musubi_tuner.dataset.image_video_dataset:total batches: 1614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean_latent_indices': tensor[16, 1] i64 \u001b[38;2;127;127;127mall_zeros\u001b[0m,\n",
       " 'latent_indices': tensor[16, 1] i64 x∈[3, 3] μ=3.000 σ=0.,\n",
       " 'clean_latent_bboxes': tensor[16, 1, 4] n=64 x∈[0.006, 0.997] μ=0.473 σ=0.309,\n",
       " 'image_embeddings': tensor[16, 729, 1152] f16 n=13436928 (26Mb) x∈[-61.750, 108.312] μ=0.023 σ=1.882,\n",
       " 'latents': tensor[16, 16, 1, 90, 160] f16 n=3686400 (7.0Mb) x∈[-5.094, 5.004] μ=0.008 σ=1.153,\n",
       " 'latents_clean': tensor[16, 16, 1, 64, 64] f16 n=1048576 (2Mb) x∈[-4.746, 4.652] μ=-0.019 σ=1.084,\n",
       " 'target_latent_masks': tensor[16, 1, 1, 90, 160] f16 n=230400 (0.4Mb) x∈[0., 1.000] μ=0.350 σ=0.473,\n",
       " 'llama_vec': tensor[16, 512, 4096] f16 n=33554432 (64Mb) x∈[-36.531, 10.781] μ=-0.001 σ=0.394,\n",
       " 'llama_attention_mask': tensor[16, 512] bool n=8192 (8Kb) x∈[False, True] μ=0.267 σ=0.442,\n",
       " 'clip_l_pooler': tensor[16, 768] n=12288 (48Kb) x∈[-5.064, 7.806] μ=-0.107 σ=0.996}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "user_config = config_utils.load_user_config(args.dataset_config)\n",
    "blueprint = blueprint_generator.generate(user_config, args, architecture=trainer.architecture)\n",
    "train_dataset_group = config_utils.generate_dataset_group_by_blueprint(blueprint.dataset_group, training=True)\n",
    "collator = collator_class(Value(\"i\", 0), Value(\"i\", 0), None)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_group, batch_size=1, shuffle=True, collate_fn=collator, num_workers=8, persistent_workers=True    \n",
    ")\n",
    "# train_dataset_group.datasets[0].batch_manager[271]\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47474be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1614/1614 [33:53<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtype\n",
    "weight_dtype = torch.bfloat16\n",
    "dit_dtype = torch.bfloat16\n",
    "dit_weight_dtype = torch.bfloat16\n",
    "# get embedding for sampling images\n",
    "vae_dtype = torch.float16\n",
    "sample_parameters = trainer.process_sample_prompts(args, Namespace(**{'device': device}), args.sample_prompts)\n",
    "\n",
    "# Load VAE model for sampling images: VAE is loaded to cpu to save gpu memory\n",
    "vae = trainer.load_vae(args, vae_dtype=vae_dtype, vae_path=args.vae)\n",
    "vae.requires_grad_(False)\n",
    "vae.eval()\n",
    "vae.to(device)\n",
    "\n",
    "transformer = trainer.load_transformer(\n",
    "    Namespace(**{'device': device}), args, args.dit, \"torch\", args.split_attn, device, dit_weight_dtype\n",
    ")\n",
    "transformer.eval()\n",
    "transformer.requires_grad_(False)\n",
    "\n",
    "# # apply network to DiT\n",
    "# network.apply_to(None, transformer, apply_text_encoder=False, apply_unet=True)\n",
    "weights_sd = load_file(\"/home/yo564250/workspace/whisperer/related/framepackbase/musubi-tuner/outputs/training/idmask_control_lora/idmask_control_lora_test3-000008.safetensors\")\n",
    "module = lora_framepack.create_arch_network_from_weights(\n",
    "    1.0, weights_sd, unet=transformer, for_inference=True\n",
    ")\n",
    "module.merge_to(None, transformer, weights_sd, weight_dtype, \"cpu\")\n",
    "\n",
    "transformer.enable_gradient_checkpointing()\n",
    "# network.enable_gradient_checkpointing()  # may have no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    latents = batch[\"latents\"]\n",
    "    noise = torch.randn_like(latents)\n",
    "\n",
    "    # get_noisy_model_input_and_timesteps\n",
    "    logits_norm = torch.randn(8, device=device)\n",
    "    t = logits_norm.sigmoid()\n",
    "    t = (t * 3.0) / (1 + (3.0 - 1) * t)\n",
    "    timesteps = t * 1000.0\n",
    "    t = t.view(-1, 1, 1, 1, 1)\n",
    "    noisy_model_input = (1 - t) * latents + t * noise\n",
    "    timesteps += 1\n",
    "    # compute_loss_weighting_for_sd3\n",
    "    weighting = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b696ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 1, 68, 120])\n",
      "torch.Size([8, 3072, 1, 34, 60])\n",
      "torch.Size([8, 2040, 3072])\n",
      "torch.Size([8, 256, 1, 34, 60])\n",
      "torch.Size([8, 2040, 256])\n",
      "torch.Size([8, 16, 2, 32, 32])\n",
      "torch.Size([8, 256, 3072])\n",
      "torch.Size([8, 256, 256])\n",
      "torch.Size([8, 256, 3072])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     clean_latent_rope_freqs\u001b[38;5;241m.\u001b[39mappend(clean_latent_rope_freq)\n\u001b[1;32m     69\u001b[0m processed_clean_latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(processed_clean_latents, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m clean_latent_rope_freqs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_latent_rope_freqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_clean_latents\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_latent_rope_freqs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latents = batch['latents'].to(device, dtype=dit_dtype) #B C T H W\n",
    "    latent_indices = batch['latent_indices']\n",
    "    clean_latents = batch['latents_clean'].to(device, dtype=dit_dtype) #B C n h w\n",
    "    clean_latent_indices = batch['clean_latent_indices'] #B 1\n",
    "    clean_latent_bboxes = batch['clean_latent_bboxes'] #B n 4\n",
    "\n",
    "    print(latents.shape)\n",
    "    hidden_states = transformer.x_embedder.proj(latents)\n",
    "    B, C, T, H, W = hidden_states.shape\n",
    "    print(hidden_states.shape)\n",
    "    hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "    print(hidden_states.shape)\n",
    "\n",
    "    rope_freqs = transformer.rope(\n",
    "        frame_indices=latent_indices, height=H, width=W, \n",
    "        device=hidden_states.device)\n",
    "    print(rope_freqs.shape)\n",
    "    rope_freqs = rope_freqs.flatten(2).transpose(1, 2)\n",
    "    print(rope_freqs.shape)\n",
    "\n",
    "    print(clean_latents.shape)\n",
    "    N = clean_latents.shape[2]\n",
    "    clean_latents = [clean_latents[:,:,[i]] for i in range(N)]\n",
    "    clean_latent_indices = [clean_latent_indices[:,[i]] for i in range(clean_latent_indices.shape[1])]\n",
    "    if len(clean_latent_indices) != len(clean_latents):\n",
    "        clean_latent_indices = [clean_latent_indices[0]] * N\n",
    "    clean_latent_bboxes = [clean_latent_bboxes[:,i] for i in range(clean_latent_bboxes.shape[1])]\n",
    "    \n",
    "    processed_clean_latents, clean_latent_rope_freqs = [], []\n",
    "    for i, clean_latent in enumerate(clean_latents):\n",
    "        clean_latent = clean_latent.to(hidden_states)\n",
    "        clean_latent = transformer.clean_x_embedder.proj(clean_latent)\n",
    "        clean_latent_index = clean_latent_indices[i]\n",
    "        clean_latent_bbox = clean_latent_bboxes[i]\n",
    "\n",
    "        if clean_latent.shape[0] != B:\n",
    "            clean_latent = repeat_to_batch_size(clean_latent, B)\n",
    "        if clean_latent_index.shape[0] != B:\n",
    "            clean_latent_index = repeat_to_batch_size(clean_latent_index, B)\n",
    "        _, _, _, clean_H, clean_W = clean_latent.shape\n",
    "        if clean_latent_bbox.shape[0] != B:\n",
    "            clean_latent_bbox = repeat_to_batch_size(clean_latent_bbox, B)\n",
    "\n",
    "        clean_latent_rope_freq = []\n",
    "        for b in range(B):\n",
    "            cb_rel = clean_latent_bbox[b]\n",
    "            if cb_rel.mean() > 0.0:\n",
    "                cb_0, cb_1, cb_2, cb_3 = int(cb_rel[0]*W), int(cb_rel[1]*H), int(cb_rel[2]*W), int(cb_rel[3]*H)\n",
    "                cb_rope_freq = transformer.rope(\n",
    "                    frame_indices=clean_latent_index[[b]], \n",
    "                    height=cb_3, width=cb_2,\n",
    "                    start_height=cb_1, start_width=cb_0,\n",
    "                    step_H=(cb_3 - cb_1) / clean_H,\n",
    "                    step_W=(cb_2 - cb_0) / clean_W,\n",
    "                    device=clean_latent.device)\n",
    "\n",
    "                clean_latent_rope_freq.append(cb_rope_freq)\n",
    "        clean_latent_rope_freq = torch.cat(clean_latent_rope_freq, dim=0)\n",
    "\n",
    "        clean_latent = clean_latent.flatten(2).transpose(1, 2)\n",
    "        clean_latent_rope_freq = clean_latent_rope_freq.flatten(2).transpose(1, 2)\n",
    "        print(clean_latent.shape)\n",
    "        print(clean_latent_rope_freq.shape)\n",
    "\n",
    "        processed_clean_latents.append(clean_latent)\n",
    "        clean_latent_rope_freqs.append(clean_latent_rope_freq)\n",
    "\n",
    "    processed_clean_latents = torch.cat(processed_clean_latents, dim=1)\n",
    "    clean_latent_rope_freqs = torch.cat(clean_latent_rope_freqs, dim=1)\n",
    "    print(processed_clean_latents.shape)\n",
    "    print(clean_latent_rope_freqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d00e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_latent_bboxes_float32': tensor[1, 4] x∈[0.029, 0.608] μ=0.370 σ=0.252 [[0.342, 0.029, 0.608, 0.500]],\n",
       " 'clean_latent_indices_int64': tensor[1] i64 [0],\n",
       " 'image_embeddings_float16': tensor[729, 1152] f16 n=839808 (1.6Mb) x∈[-60.719, 90.688] μ=0.026 σ=1.979,\n",
       " 'latent_indices_int64': tensor[1] i64 [9],\n",
       " 'latents_1x68x120_float16': tensor[16, 1, 68, 120] f16 n=130560 (0.2Mb) x∈[-4.133, 4.762] μ=0.327 σ=1.521,\n",
       " 'latents_clean_1x68x120_float16': tensor[16, 1, 32, 32] f16 n=16384 (32Kb) x∈[-4.199, 3.943] μ=0.018 σ=1.051,\n",
       " 'target_latent_masks_1x68x120_float16': tensor[1, 1, 68, 120] f16 n=8160 (16Kb) x∈[0., 1.000] μ=0.307 σ=0.455}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_file('/groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2_cache_v2/_0ccfGEpIwY_segment_16_step1-0-85_step2-0-85_step4_step5_step6_1280x0720_fp.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_hidden_states(\n",
    "    self,\n",
    "    latents,\n",
    "    latent_indices=None,\n",
    "    clean_latents=None,\n",
    "    clean_latent_indices=None,\n",
    "    clean_latent_bboxes=None, # [B, N, 4]\n",
    "):\n",
    "    hidden_states = self.x_embedder.proj(latents)\n",
    "    B, C, T, H, W = hidden_states.shape\n",
    "\n",
    "    if latent_indices is None:\n",
    "        latent_indices = repeat_to_batch_size(torch.arange(0, T).unsqueeze(0), B)\n",
    "    if latent_indices.shape[0] != B:\n",
    "        latent_indices = repeat_to_batch_size(latent_indices, B)\n",
    "\n",
    "    hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "\n",
    "    rope_freqs = self.rope(frame_indices=latent_indices, height=H, width=W, device=hidden_states.device)\n",
    "    rope_freqs = rope_freqs.flatten(2).transpose(1, 2)\n",
    "\n",
    "    if clean_latents is not None and clean_latent_indices is not None:\n",
    "        processed_clean_latents, clean_latent_rope_freqs = [], []\n",
    "        if type(clean_latents) != list:\n",
    "            clean_latents = [clean_latents[:,:,[i]] for i in range(clean_latents.shape[2])]\n",
    "            clean_latent_indices = [clean_latent_indices[:,[i]] for i in range(clean_latent_indices.shape[1])]\n",
    "        if len(clean_latent_indices) != len(clean_latents):\n",
    "            clean_latent_indices = [clean_latent_indices[0]] * len(clean_latents)\n",
    "        if clean_latent_bboxes is None:\n",
    "            clean_latent_bboxes = [torch.tensor([[0.0, 0.0, 1.0, 1.0]], device=hidden_states.device).repeat(B,1)] * N\n",
    "        else:\n",
    "            clean_latent_bboxes = [clean_latent_bboxes[:,i] for i in range(clean_latent_bboxes.shape[1])]\n",
    "\n",
    "        for i, clean_latent in enumerate(clean_latents):\n",
    "            clean_latent = clean_latent.to(hidden_states)\n",
    "            clean_latent = self.clean_x_embedder.proj(clean_latent)\n",
    "            clean_latent_index = clean_latent_indices[i]\n",
    "            clean_latent_bbox = clean_latent_bboxes[:,i]\n",
    "\n",
    "            if clean_latent.shape[0] != B:\n",
    "                clean_latent = repeat_to_batch_size(clean_latent, B)\n",
    "            if clean_latent_index.shape[0] != B:\n",
    "                clean_latent_index = repeat_to_batch_size(clean_latent_index, B)\n",
    "            _, _, _, clean_H, clean_W = clean_latent.shape\n",
    "\n",
    "            if clean_latent_bbox.shape[0] != B:\n",
    "                clean_latent_bbox = repeat_to_batch_size(clean_latent_bbox, B)\n",
    "\n",
    "            clean_latent_rope_freq = []\n",
    "            for b in range(B):\n",
    "                cb = [\n",
    "                    int(clean_latent_bbox[b,0]*W), int(clean_latent_bbox[b,1]*H), \n",
    "                    int(clean_latent_bbox[b,2]*W), int(clean_latent_bbox[b,3]*H)\n",
    "                ]\n",
    "                cb_rope_freq = self.rope(\n",
    "                    frame_indices=clean_latent_index[[b]], \n",
    "                    height=cb[3], width=cb[2],\n",
    "                    start_height=cb[1], start_width=cb[0],\n",
    "                    step_H=(cb[3] - cb[1]) / clean_H,\n",
    "                    step_W=(cb[2] - cb[0]) / clean_W,\n",
    "                    device=clean_latent.device)\n",
    "\n",
    "                clean_latent_rope_freq.append(cb_rope_freq)\n",
    "            clean_latent_rope_freq = torch.cat(clean_latent_rope_freq, dim=0)\n",
    "\n",
    "            clean_latent = clean_latent.flatten(2).transpose(1, 2)\n",
    "            clean_latent_rope_freq = clean_latent_rope_freq.flatten(2).transpose(1, 2)\n",
    "            \n",
    "            processed_clean_latents.append(clean_latent)\n",
    "            clean_latent_rope_freqs.append(clean_latent_rope_freq)\n",
    "\n",
    "        processed_clean_latents = torch.cat(processed_clean_latents, dim=1)\n",
    "        clean_latent_rope_freqs = torch.cat(clean_latent_rope_freqs, dim=1)\n",
    "\n",
    "        # logger.info(f\"Clean Latent Rope Freq Shape: {clean_latent_rope_freq.shape}\")\n",
    "        # logger.info(f\"Rope Freq Shape: {rope_freqs.shape}\")\n",
    "        hidden_states = torch.cat([processed_clean_latents, hidden_states], dim=1)\n",
    "        rope_freqs = torch.cat([clean_latent_rope_freqs, rope_freqs], dim=1)\n",
    "\n",
    "    return hidden_states, rope_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04a75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.hv_train_network:prompt: Three individuals seated closely together in what appears to be a casual indoor setting. The person in the center is wearing a gray hoodie with pink accents and has light-colored hair. To the left, another individual is dressed in a red shirt with a graphic design, and to the right, a person with long dark hair is wearing a light-colored top. The background includes a wall with a colorful mural or artwork, and the room has a modern, cozy ambiance with soft lighting. The individuals are engaged in conversation, with the central figure speaking and the others listening and reacting with smiles and nods. The camera remains stationary, capturing the scene from a medium shot perspective.\n",
      "INFO:musubi_tuner.hv_train_network:height: 720\n",
      "INFO:musubi_tuner.hv_train_network:width: 1280\n",
      "INFO:musubi_tuner.hv_train_network:frame count: 1\n",
      "INFO:musubi_tuner.hv_train_network:sample steps: 25\n",
      "INFO:musubi_tuner.hv_train_network:guidance scale: 10.0\n",
      "INFO:musubi_tuner.hv_train_network:discrete flow shift: 14.5\n",
      "INFO:musubi_tuner.hv_train_network:seed: 1111\n",
      "INFO:musubi_tuner.hv_train_network:image path: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding image to latent space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.fpack_train_network:Encoding control image: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/source_facecrop_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Encoding entity mask: /groups/chenchen/patrick/OpenS2V-Nexus/datasets/test3_2/RH9DTExtz1s_segment_55_step1-0-73_step2-0-73_step4_step5_step6/target_bodmask_0.png\n",
      "INFO:musubi_tuner.fpack_train_network:Set index for clean latent 1x: ['0']\n",
      "INFO:musubi_tuner.fpack_train_network:Set index for target: 9\n",
      "INFO:musubi_tuner.fpack_train_network:No clean_latents_2x\n",
      "INFO:musubi_tuner.fpack_train_network:No clean_latents_4x\n",
      "INFO:musubi_tuner.fpack_train_network:One frame inference. clean_latent: torch.Size([1, 16, 1, 34, 25]) latent_indices: tensor[1, 1] i64 [[9]], clean_latent_indices: tensor[1, 1] i64 [[0]], num_frames: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7de0551234a42c5871f50c593578d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.fpack_train_network:Waiting for 5 seconds to finish block swap\n",
      "INFO:musubi_tuner.fpack_generate_video:Decoding video...\n",
      "INFO:musubi_tuner.fpack_generate_video:Bulk decoding or one frame inference\n",
      "INFO:musubi_tuner.fpack_generate_video:Decoded. Pixel shape torch.Size([1, 3, 1, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    trainer.sample_image_inference(\n",
    "        Namespace(**{'device': device}), args, transformer, dit_dtype, vae, \n",
    "        \".\", sample_parameters[1], 0, 0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
