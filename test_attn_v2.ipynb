{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1867f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 11:46:10.338501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753152370.357595   89949 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753152370.363583   89949 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-22 11:46:10.442717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:numexpr.utils:Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xformers is installed!\n",
      "Flash Attn is installed!\n",
      "Sage Attn is installed!\n",
      "Trying to import sageattention\n",
      "Successfully imported sageattention\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"src/\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,\"\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import math, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lovely_tensors as lt\n",
    "from einops import rearrange\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "\n",
    "from musubi_tuner.dataset.image_video_dataset import resize_image_to_bucket\n",
    "from musubi_tuner.networks import lora_framepack\n",
    "from musubi_tuner.frame_pack.clip_vision import hf_clip_vision_encode\n",
    "from musubi_tuner.frame_pack.framepack_utils import load_vae, load_text_encoder1, load_text_encoder2, load_image_encoders\n",
    "from musubi_tuner.frame_pack.hunyuan import encode_prompt_conds, vae_encode, vae_decode\n",
    "from musubi_tuner.frame_pack.hunyuan_video_packed import load_packed_model, attn_cache\n",
    "from musubi_tuner.frame_pack.k_diffusion_hunyuan import sample_hunyuan\n",
    "from musubi_tuner.frame_pack.utils import crop_or_pad_yield_mask\n",
    "from musubi_tuner.wan_generate_video import merge_lora_weights\n",
    "\n",
    "sys.path.append(\"/data/whisperer/utils\")\n",
    "from img_utils import convert_from_qwen2vl_format\n",
    "from vistory_utils import StoryDataset\n",
    "\n",
    "device = torch.device('cuda')\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b7da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Creating HunyuanVideoTransformer3DModelPacked\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Loading DiT model from /data/stale/patrickkwon/video/stable-diffusion-webui/models/Hunyuan/FramePackI2V_HY_bf16.safetensors, device=cuda\n",
      "INFO:musubi_tuner.frame_pack.hunyuan_video_packed:Loaded DiT model from /data/stale/patrickkwon/video/stable-diffusion-webui/models/Hunyuan/FramePackI2V_HY_bf16.safetensors, info=<All keys matched successfully>\n",
      "INFO:musubi_tuner.hunyuan_model.vae:Loading 3D VAE model (884-16c-hy) from: /data/stale/patrickkwon/video/stable-diffusion-webui/models/VAE/hunyuan-video-t2v-720p-vae.pt\n",
      "INFO:musubi_tuner.hunyuan_model.vae:VAE to dtype: torch.float16\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Set chunk_size to 32 for CausalConv3d\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Enabled spatial tiling with min size 128\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 1 tokenizer\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 1 from /shared/video/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 2 tokenizer\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading text encoder 2 from /shared/video/ComfyUI/models/text_encoders/clip_l.safetensors\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading image encoder feature extractor\n",
      "INFO:musubi_tuner.frame_pack.framepack_utils:Loading image encoder from /shared/video/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\n"
     ]
    }
   ],
   "source": [
    "dit_path = \"/data/stale/patrickkwon/video/stable-diffusion-webui/models/Hunyuan/FramePackI2V_HY_bf16.safetensors\"\n",
    "vae_path = \"/data/stale/patrickkwon/video/stable-diffusion-webui/models/VAE/hunyuan-video-t2v-720p-vae.pt\"\n",
    "text_encoder1_path = \"/shared/video/ComfyUI/models/text_encoders/llava_llama3_fp16.safetensors\"\n",
    "text_encoder2_path = \"/shared/video/ComfyUI/models/text_encoders/clip_l.safetensors\"\n",
    "image_encoder_path = \"/shared/video/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\"\n",
    "\n",
    "model = load_packed_model(device, dit_path, '', device)\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "vae = load_vae(vae_path, 32, 128, device)\n",
    "\n",
    "tokenizer1, text_encoder1 = load_text_encoder1(SimpleNamespace(text_encoder1=text_encoder1_path), False, device)\n",
    "tokenizer2, text_encoder2 = load_text_encoder2(SimpleNamespace(text_encoder2=text_encoder2_path))\n",
    "feature_extractor, image_encoder = load_image_encoders(SimpleNamespace(image_encoder=image_encoder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bad092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_preproc(prompt, text_encoder1, text_encoder2, tokenizer1, tokenizer2, device=torch.device('cuda')):\n",
    "    with torch.autocast(device_type=device.type, dtype=text_encoder1.dtype), torch.no_grad():\n",
    "        llama_vec, clip_l_pooler, llama_strtokens = encode_prompt_conds(\n",
    "            prompt, text_encoder1, text_encoder2, tokenizer1, tokenizer2, custom_system_prompt=None, return_tokendict=True\n",
    "        )\n",
    "        llama_vec = llama_vec.to(device, dtype=torch.bfloat16)\n",
    "        clip_l_pooler = clip_l_pooler.to(device, dtype=torch.bfloat16)\n",
    "        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n",
    "\n",
    "    llama_vec_n = torch.zeros_like(llama_vec).to(device, dtype=torch.bfloat16)\n",
    "    clip_l_pooler_n  = torch.zeros_like(clip_l_pooler).to(device, dtype=torch.bfloat16)\n",
    "    llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)\n",
    "    return {\n",
    "        \"prompt_embeds\" : llama_vec,\n",
    "        \"prompt_embeds_mask\" : llama_attention_mask,\n",
    "        \"prompt_poolers\" : clip_l_pooler,\n",
    "        \"negative_prompt_embeds\" : llama_vec_n,\n",
    "        \"negative_prompt_embeds_mask\" : llama_attention_mask_n,\n",
    "        \"negative_prompt_poolers\" : clip_l_pooler_n,\n",
    "    }, llama_strtokens\n",
    "\n",
    "def getres(orig_width, orig_height, target_area=480*480, div_factor=16):\n",
    "    if target_area is not None:\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "        new_height = math.sqrt(target_area / aspect_ratio)\n",
    "        new_width = target_area / new_height\n",
    "    else:\n",
    "        new_width, new_height = orig_width, orig_height\n",
    "    new_width = int(round(new_width / div_factor) * div_factor)\n",
    "    new_height = int(round(new_height / div_factor) * div_factor)\n",
    "\n",
    "    return new_width, new_height\n",
    "\n",
    "def preproc_image(image_path, width=None, height=None):\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image_pil)\n",
    "    if width is not None and height is not None:\n",
    "        image_np = resize_image_to_bucket(image_np, (width, height))\n",
    "    image_tensor = (torch.from_numpy(image_np).float() / 127.5 - 1.0).permute(2,0,1)[None, :, None]\n",
    "    return image_tensor, image_np\n",
    "\n",
    "def preproc_mask(mask_path, width, height):\n",
    "    if mask_path == '':\n",
    "        image_pil = Image.new(\"L\", (width // 8, height // 8), 255)\n",
    "    else:\n",
    "        image_pil = Image.open(mask_path).convert(\"L\")\n",
    "    image_np = np.array(image_pil)\n",
    "    if width is not None and height is not None:\n",
    "        image_np = resize_image_to_bucket(image_np,  (width // 8, height // 8))\n",
    "    image_tensor = (torch.from_numpy(image_np).float() / 255.0)[None, None, None, :, :]\n",
    "    return image_tensor, image_np\n",
    "\n",
    "def prepare_image_inputs(image_path,feature_extractor, image_encoder, width=None, height=None, \n",
    "                         target_index=1, device=torch.device('cuda'), dtype=torch.bfloat16):\n",
    "    img_tensor, img_np = preproc_image(image_path, width, height)\n",
    "    with torch.no_grad():\n",
    "        image_encoder_output = hf_clip_vision_encode(img_np, feature_extractor, image_encoder)\n",
    "    image_encoder_last_hidden_state = image_encoder_output.last_hidden_state.to(device, dtype=dtype)\n",
    "\n",
    "    latent_indices = torch.tensor([target_index], dtype=torch.int64)  # 1x1 latent index for target image\n",
    "    return {\n",
    "        \"image_embeddings\" : image_encoder_last_hidden_state,\n",
    "        \"latent_indices\" : latent_indices,\n",
    "    } , img_np\n",
    "\n",
    "def prepare_control_inputs(control_image_paths, control_image_mask_paths, width=None, height=None,\n",
    "                           control_indices=[0,10]):\n",
    "    control_latents, control_nps = [], []\n",
    "    for i, (control_image_path, control_mask_path) in enumerate(zip(control_image_paths, control_image_mask_paths)):\n",
    "        c_img_tensor, c_img_np = preproc_image(control_image_path, width, height)\n",
    "        c_img_latent = vae_encode(c_img_tensor, vae).cpu()\n",
    "        c_mask_image, c_mask_np = preproc_mask(control_mask_path, width, height)\n",
    "        c_img_latent = c_img_latent * c_mask_image\n",
    "        control_latents.append(c_img_latent)\n",
    "        control_nps.append(np.concatenate([c_img_np, resize_image_to_bucket(c_mask_np, (width, height))[..., None]], -1))\n",
    "    clean_latents = torch.cat(control_latents, dim=2)  # (1, 16, num_control_images, H//8, W//8)\n",
    "    clean_latent_indices = torch.tensor([control_indices], dtype=torch.int64)\n",
    "\n",
    "    return {\n",
    "        \"clean_latents\" : clean_latents, \n",
    "        \"clean_latent_indices\" : clean_latent_indices,\n",
    "        \"clean_latents_2x\" : None, \n",
    "        \"clean_latent_2x_indices\" : None,\n",
    "        \"clean_latents_4x\" : None, \n",
    "        \"clean_latent_4x_indices\" : None,\n",
    "    } , control_nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa67dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The girl in a school blazer in a classroom.'\n",
    "height, width = 832, 480\n",
    "\n",
    "image_path = \"docs/kisekaeichi_start.png\"\n",
    "control_image_paths = [\"docs/kisekaeichi_start.png\", \"docs/kisekaeichi_ref.png\"]\n",
    "control_image_mask_paths = [\"docs/kisekaeichi_start_mask.png\", \"docs/kisekaeichi_ref_mask.png\"]\n",
    "target_index = [1]\n",
    "control_indices = [0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7c9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d6f430ba1b46e389f116de3563a780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Activate Attention Caching for Exp.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'extra_attention_mask' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     19\u001b[0m attn_cache\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m---> 20\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msample_hunyuan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munipc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreal_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer_blocks.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer_blocks.8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer_blocks.14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_latent_bbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtext_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontrol_kwargs\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m history_pixels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     33\u001b[0m     vae_decode(results[:, :, i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, :, :], vae)\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(results\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     34\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m result_img \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mclamp(((history_pixels[\u001b[38;5;241m0\u001b[39m,:,\u001b[38;5;241m0\u001b[39m,:,:]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/k_diffusion_hunyuan.py:124\u001b[0m, in \u001b[0;36msample_hunyuan\u001b[0;34m(transformer, sampler, initial_latent, concat_latent, strength, width, height, frames, real_guidance_scale, distilled_guidance_scale, guidance_rescale, shift, num_inference_steps, batch_size, generator, prompt_embeds, prompt_embeds_mask, prompt_poolers, negative_prompt_embeds, negative_prompt_embeds_mask, negative_prompt_poolers, dtype, device, negative_kwargs, callback, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m sampler_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    103\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m     cfg_scale\u001b[38;5;241m=\u001b[39mreal_guidance_scale,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     ),\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampler \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munipc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 124\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msample_unipc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampler \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/uni_pc_fm.py:142\u001b[0m, in \u001b[0;36msample_unipc\u001b[0;34m(model, noise, sigmas, extra_args, callback, disable, variant)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_unipc\u001b[39m(model, noise, sigmas, extra_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbh1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m variant \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbh2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlowMatchUniPC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_pbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/uni_pc_fm.py:119\u001b[0m, in \u001b[0;36mFlowMatchUniPC.sample\u001b[0;34m(self, x, sigmas, callback, disable_pbar)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         model_prev_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_t\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    120\u001b[0m         t_prev_list \u001b[38;5;241m=\u001b[39m [vec_t]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m order:\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/uni_pc_fm.py:23\u001b[0m, in \u001b[0;36mFlowMatchUniPC.model_fn\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodel_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/wrapper.py:37\u001b[0m, in \u001b[0;36mfm_wrapper.<locals>.k_model\u001b[0;34m(x, sigma, **extra_args)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, concat_latent\u001b[38;5;241m.\u001b[39mto(x)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m pred_positive \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpositive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m     40\u001b[0m     pred_negative \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(pred_positive)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/whisperer/related/framepackbase/musubi-tuner/src/musubi_tuner/frame_pack/hunyuan_video_packed.py:1959\u001b[0m, in \u001b[0;36mHunyuanVideoTransformer3DModelPacked.forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1957\u001b[0m     text_len \u001b[38;5;241m=\u001b[39m encoder_attention_mask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1958\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m encoder_hidden_states[:, :text_len]\n\u001b[0;32m-> 1959\u001b[0m     encoder_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mextra_attention_mask\u001b[49m[:, :text_len]\n\u001b[1;32m   1960\u001b[0m     seqlen_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'extra_attention_mask' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "text_kwargs, llama_strtokens = get_text_preproc(prompt, text_encoder1, text_encoder2, tokenizer1, tokenizer2, device=device)\n",
    "\n",
    "## Image Preprocessing\n",
    "image_kwargs, img_np = prepare_image_inputs(\n",
    "    image_path, feature_extractor, image_encoder, width=width, height=height, \n",
    "    target_index=target_index, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "control_kwargs, control_nps = prepare_control_inputs(\n",
    "    control_image_paths, control_image_mask_paths, width=width, height=height, \n",
    "    control_indices=control_indices)\n",
    "\n",
    "num_inference_steps=25\n",
    "seed = 42\n",
    "generator = torch.Generator(device=\"cpu\")\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "attn_cache.clear()\n",
    "results = sample_hunyuan(\n",
    "    transformer=model, sampler='unipc', width=width, height=height, frames=1,\n",
    "    real_guidance_scale=1.0, distilled_guidance_scale=10.0, guidance_rescale=0.0, shift=None,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    generator=generator, device=device, dtype=torch.bfloat16,\n",
    "    cache_results=True, \n",
    "    cache_layers = ['transformer_blocks.2', 'transformer_blocks.8', 'transformer_blocks.14'], \n",
    "    clean_latent_bbox=None,\n",
    "    **text_kwargs, \n",
    "    **image_kwargs,\n",
    "    **control_kwargs\n",
    ")\n",
    "history_pixels = torch.cat([\n",
    "    vae_decode(results[:, :, i:i + 1, :, :], vae).cpu() for i in range(results.shape[2])\n",
    "], dim=2)\n",
    "result_img = (torch.clamp(((history_pixels[0,:,0,:,:]+1.0)/2.0).permute(1,2,0).cpu(), 0.0, 1.0) * 255.).numpy().astype(np.uint8)\n",
    "result_img = np.concatenate([result_img, np.ones(result_img.shape[:2] + (1, ))*255], axis=-1).astype(np.uint8)\n",
    "\n",
    "Image.fromarray(np.concatenate(control_nps + [result_img], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e867c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2531, 26, 34])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=272x208>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = 'Empress' \n",
    "\n",
    "def get_text_inds_from_dict(input_str, llama_strtokens):\n",
    "    start_inds = [i for i, x in llama_strtokens.items() if input_str.startswith(x)]\n",
    "    end_inds = [i for i, x in llama_strtokens.items() if input_str.endswith(x)]\n",
    "\n",
    "    if len(start_inds) == 0 or len(end_inds) == 0:\n",
    "        print(\"Error\")\n",
    "        return []\n",
    "    else:\n",
    "        if len(start_inds) > 1 or len(end_inds) > 1:\n",
    "            return list(range(start_inds[0], end_inds[-1]+1))\n",
    "        else:\n",
    "            return list(range(start_inds[0], end_inds[0]+1))\n",
    "        \n",
    "def get_attn_map(attn_cache, attn_inds, block_id=f'transformer_blocks.2', \n",
    "                 token_type = 'text',\n",
    "                 height=960, width=960, token_C=2,\n",
    "                 embed_size = 729,\n",
    "                 t_0=0, t_1=25):\n",
    "    timesteps = sorted(list(attn_cache[list(attn_cache.keys())[0]].keys()), reverse=False)\n",
    "    token_H, token_W = height // 16, width // 16\n",
    "    hidden_size = token_H * token_W * token_C\n",
    "    \n",
    "    attention_probs = sum(attn_cache[block_id][timesteps[t]] for t in range(t_0, t_1))\n",
    "    attention_map = attention_probs[:,:,:hidden_size,:]\n",
    "    attention_map = rearrange(attention_map, 'B A (C H W) D -> B A C H W D', H=token_H, W=token_W)\n",
    "    attention_map = attention_map[:,:,-1,:,:,:].sum(1).squeeze(1).permute(0,3,1,2) #B, D, H, W\n",
    "    if token_type == 'text':\n",
    "        attn_inds = [hidden_size+embed_size+x for x in attn_inds]\n",
    "    elif token_type == 'image':\n",
    "        attn_inds = list(range((token_H*token_W*attn_inds[0]), (token_H*token_W*(attn_inds[0]+1))))\n",
    "    else:\n",
    "        attn_inds = attn_inds\n",
    "    print(attention_map.shape)\n",
    "    attention_data = attention_map[0,attn_inds,:,:].mean(axis=0).to(dtype=torch.float32).cpu().numpy()\n",
    "\n",
    "    cmap = plt.cm.viridis\n",
    "    norm = plt.Normalize(vmin=attention_data.min(), vmax=attention_data.max()) # Normalize data to [0,1] range\n",
    "    colored_data = cmap(norm(attention_data))[:, :, :3] * 255 # Get RGB values and scale to 0-255\n",
    "    colored_data_uint8 = colored_data.astype(np.uint8)\n",
    "    attention_image = Image.fromarray(colored_data_uint8)\n",
    "    return attention_image\n",
    "\n",
    "attn_inds = get_text_inds_from_dict(input_str, llama_strtokens)\n",
    "attn_images = get_attn_map(attn_cache, attn_inds, block_id=f'transformer_blocks.14', token_type = 'text',\n",
    "                           height=height, width=width, token_C=len(control_indices + target_index))\n",
    "attn_images.resize((width//2, height//2), Image.Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vistory_dataset = StoryDataset(Path(\"/data/whisperer/datasets/storyviz/ViStoryBench/ViStoryBench\"))\n",
    "\n",
    "# case = Path(\"/data/whisperer/outputs/20250717_154000/[PANEL-1]\")\n",
    "# case_config = json.loads((case.parent / \"config.json\").read_text())\n",
    "# layout = json.loads((case.parent / \"pose_layout.json\").read_text())[case.name]\n",
    "# width, height = layout['bbox'][2] - layout['bbox'][0], layout['bbox'][3] - layout['bbox'][1]\n",
    "# width, height = getres(width, height, target_area=960*960, div_factor=16)\n",
    "\n",
    "# story = vistory_dataset.load_story(case_config['story_name'])\n",
    "# story['shots'] = {f\"[PANEL-{i+1}]\" : v for i,v in enumerate([x for x in story['shots'] if x['index'] in case_config['pages']]) }\n",
    "# story_shot = story['shots'][case.name]\n",
    "\n",
    "# prompt = story_shot['camera'] + \". \" + story_shot['script'] + \". \" + story_shot['scene']\n",
    "# entity_prompts = story_shot['character_name']\n",
    "# image_path = story['characters'][entity_prompts[0]]['images'][0]\n",
    "# control_image_paths = [image_path]\n",
    "\n",
    "# c_width, c_height = Image.open(control_image_paths[0]).size\n",
    "# c_width, c_height = getres(c_width, c_height, target_area=None, div_factor=16)\n",
    "# print(width, height, c_width, c_height)\n",
    "\n",
    "# control_image_mask_paths = [\"\", \"\"]\n",
    "# target_index = [1]\n",
    "# control_indices = [10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
