{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 16:35:49.676419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754033749.798634  152801 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754033749.866710  152801 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-01 16:35:50.044851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"src/\")\n",
    "sys.argv = [\"fpack_cache_latents.py\",\n",
    "    \"--dataset_config\", \"/data/whisperer/datasets/storyviz/OpenS2V-Nexus/datasets/test2/train.toml\",\n",
    "    \"--vae\", \"/data/stale/patrickkwon/video/stable-diffusion-webui/models/VAE/hunyuan-video-t2v-720p-vae.pt\",\n",
    "    \"--image_encoder\", \"/shared/video/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\",\n",
    "    \"--vae_chunk_size\", \"32\",\n",
    "    \"--vae_spatial_tile_sample_min_size\", \"128\", \n",
    "    \"--skip_existing\", \"--keep_cache\", \"--one_frame\", \"--one_frame_no_2x\", \"--one_frame_no_4x\"\n",
    "]\n",
    "from typing import List, Optional\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import SiglipImageProcessor, SiglipVisionModel, SiglipVisionConfig\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from musubi_tuner.dataset import config_utils\n",
    "from musubi_tuner.dataset.config_utils import BlueprintGenerator, ConfigSanitizer\n",
    "from musubi_tuner.dataset.image_video_dataset import ImageDataset, ItemInfo, save_latent_cache_framepack, ARCHITECTURE_FRAMEPACK\n",
    "from musubi_tuner.frame_pack import hunyuan\n",
    "from musubi_tuner.frame_pack.framepack_utils import load_image_encoders, load_vae, FEATURE_EXTRACTOR_CONFIG, IMAGE_ENCODER_CONFIG\n",
    "from musubi_tuner.hunyuan_model.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\n",
    "from musubi_tuner.frame_pack.clip_vision import hf_clip_vision_encode\n",
    "import musubi_tuner.cache_latents as cache_latents\n",
    "from musubi_tuner.cache_latents import preprocess_contents\n",
    "\n",
    "parser = cache_latents.setup_parser_common()\n",
    "parser = cache_latents.hv_setup_parser(parser)  # VAE\n",
    "# parser = framepack_setup_parser(parser)\n",
    "parser.add_argument(\"--image_encoder\", type=str, required=True)\n",
    "parser.add_argument(\"--f1\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_2x\", action=\"store_true\")\n",
    "parser.add_argument(\"--one_frame_no_4x\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04a75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "vae = load_vae(\"/data/stale/patrickkwon/video/stable-diffusion-webui/models/VAE/hunyuan-video-t2v-720p-vae.pt\", 32, 128, device=device)\n",
    "vae.to(device)\n",
    "\n",
    "feature_extractor = SiglipImageProcessor(**FEATURE_EXTRACTOR_CONFIG)\n",
    "\n",
    "config = SiglipVisionConfig(**IMAGE_ENCODER_CONFIG)\n",
    "image_encoder = SiglipVisionModel._from_config(config, torch_dtype=torch.float16)\n",
    "state_dict = load_file(\"/shared/video/ComfyUI/models/clip_vision/sigclip_vision_patch14_384.safetensors\")\n",
    "image_encoder.load_state_dict(state_dict, strict=True, assign=True)\n",
    "image_encoder.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blueprint_generator = BlueprintGenerator(ConfigSanitizer())\n",
    "# user_config = config_utils.load_user_config(args.dataset_config)\n",
    "# blueprint = blueprint_generator.generate(user_config, args, architecture=ARCHITECTURE_FRAMEPACK)\n",
    "# train_dataset_group = config_utils.generate_dataset_group_by_blueprint(blueprint.dataset_group)\n",
    "# datasets = train_dataset_group.datasets\n",
    "\n",
    "dataset = ImageDataset(\n",
    "    resolution=(960, 544), caption_extension='.txt', batch_size=1, num_repeats=1, enable_bucket=True, bucket_no_upscale=False, \n",
    "\n",
    "    image_directory=None, \n",
    "    image_jsonl_file='/data/whisperer/datasets/storyviz/OpenS2V-Nexus/datasets/test2/train-clean1.jsonl', \n",
    "    control_directory=None,\n",
    "    cache_directory='/data/whisperer/datasets/storyviz/OpenS2V-Nexus/datasets/test2/cache_clean', \n",
    "\n",
    "    fp_latent_window_size=9, fp_1f_clean_indices=[0], fp_1f_target_index=9, \n",
    "    fp_1f_no_post=True, debug_dataset=False, architecture='fp', \n",
    ")\n",
    "dataset.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a66ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_save_batch_one_frame(\n",
    "    vae: AutoencoderKLCausal3D,\n",
    "    feature_extractor: SiglipImageProcessor,\n",
    "    image_encoder: SiglipVisionModel,\n",
    "    batch: List[ItemInfo],\n",
    "    vanilla_sampling: bool = False,\n",
    "    one_frame_no_2x: bool = False,\n",
    "    one_frame_no_4x: bool = False,\n",
    "):\n",
    "    # item.content: target image (H, W, C)\n",
    "    # item.control_content: list of images (H, W, C)\n",
    "    _, _, contents, content_masks = preprocess_contents(batch)\n",
    "    contents = contents.to(vae.device, dtype=vae.dtype)  # B, C, F, H, W\n",
    "\n",
    "    # VAE encode: we need to encode one frame at a time because VAE encoder has stride=4 for the time dimension except for the first frame.\n",
    "    latents = [hunyuan.vae_encode(contents[:, :, idx : idx + 1], vae).to(\"cpu\") for idx in range(contents.shape[2])]\n",
    "    latents = torch.cat(latents, dim=2)  # B, C, F, H/8, W/8\n",
    "\n",
    "    # apply alphas to latents\n",
    "    for b, item in enumerate(batch):\n",
    "        for i, content_mask in enumerate(content_masks[b]):\n",
    "            if content_mask is not None:\n",
    "                # apply mask to the latents\n",
    "                # print(f\"Applying content mask for item {item.item_key}, frame {i}\")\n",
    "                latents[b : b + 1, :, i : i + 1] *= content_mask\n",
    "\n",
    "    # Vision encoding perâ€‘item (once): use control content because it is the start image\n",
    "    # images = [item.control_content[0] for item in batch]  # list of [H, W, C]\n",
    "    images = [item.embed_content for item in batch]\n",
    "\n",
    "    # encode image with image encoder\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            if image.shape[-1] == 4:\n",
    "                image = image[..., :3]\n",
    "            image_encoder_output = hf_clip_vision_encode(image, feature_extractor, image_encoder)\n",
    "            image_embeddings.append(image_encoder_output.last_hidden_state)\n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)  # B, LEN, 1152\n",
    "    image_embeddings = image_embeddings.to(\"cpu\")  # Save memory\n",
    "\n",
    "    # save cache for each item in the batch\n",
    "    for b, item in enumerate(batch):\n",
    "        # indices generation (same as inference): each item may have different clean_latent_indices, so we generate them per item\n",
    "        clean_latent_indices = item.fp_1f_clean_indices  # list of indices for clean latents\n",
    "        if clean_latent_indices is None or len(clean_latent_indices) == 0:\n",
    "            clean_latent_indices = [0]\n",
    "\n",
    "        if not item.fp_1f_no_post:\n",
    "            clean_latent_indices = clean_latent_indices + [1 + item.fp_latent_window_size]\n",
    "        clean_latent_indices = torch.Tensor(clean_latent_indices).long()  #  N\n",
    "\n",
    "        latent_index = torch.Tensor([item.fp_1f_target_index]).long()  #  1\n",
    "\n",
    "        # zero values is not needed to cache even if one_frame_no_2x or 4x is False\n",
    "        clean_latents_2x = None\n",
    "        clean_latents_4x = None\n",
    "\n",
    "        if one_frame_no_2x:\n",
    "            clean_latent_2x_indices = None\n",
    "        else:\n",
    "            index = 1 + item.fp_latent_window_size + 1\n",
    "            clean_latent_2x_indices = torch.arange(index, index + 2)  #  2\n",
    "\n",
    "        if one_frame_no_4x:\n",
    "            clean_latent_4x_indices = None\n",
    "        else:\n",
    "            index = 1 + item.fp_latent_window_size + 1 + 2\n",
    "            clean_latent_4x_indices = torch.arange(index, index + 16)  #  16\n",
    "\n",
    "        # clean latents preparation (emulating inference)\n",
    "        clean_latents = latents[b, :, :-1]  # C, F, H, W\n",
    "        if not item.fp_1f_no_post:\n",
    "            # If zero post is enabled, we need to add a zero frame at the end\n",
    "            clean_latents = F.pad(clean_latents, (0, 0, 0, 0, 0, 1), value=0.0)  # C, F+1, H, W\n",
    "\n",
    "        # Target latents for this section (ground truth)\n",
    "        target_latents = latents[b, :, -1:]  # C, 1, H, W\n",
    "\n",
    "        print(f\"Saving cache for item {item.item_key} at {item.latent_cache_path}. no_post: {item.fp_1f_no_post}\")\n",
    "        print(f\"  Clean latent indices: {clean_latent_indices}, latent index: {latent_index}\")\n",
    "        print(f\"  Clean latents: {clean_latents.shape}, target latents: {target_latents.shape}\")\n",
    "        print(f\"  Clean latents 2x indices: {clean_latent_2x_indices}, clean latents 4x indices: {clean_latent_4x_indices}\")\n",
    "        print(\n",
    "            f\"  Clean latents 2x: {clean_latents_2x.shape if clean_latents_2x is not None else 'None'}, \"\n",
    "            f\"Clean latents 4x: {clean_latents_4x.shape if clean_latents_4x is not None else 'None'}\"\n",
    "        )\n",
    "        print(f\"  Image embeddings: {image_embeddings[b].shape}\")\n",
    "\n",
    "        # save cache (file path is inside item.latent_cache_path pattern), remove batch dim\n",
    "        save_latent_cache_framepack(\n",
    "            item_info=item,\n",
    "            latent=target_latents,  # Ground truth for this section\n",
    "            latent_indices=latent_index,  # Indices for the ground truth section\n",
    "            clean_latents=clean_latents,  # Start frame + history placeholder\n",
    "            clean_latent_indices=clean_latent_indices,  # Indices for start frame + history placeholder\n",
    "            clean_latents_2x=clean_latents_2x,  # History placeholder\n",
    "            clean_latent_2x_indices=clean_latent_2x_indices,  # Indices for history placeholder\n",
    "            clean_latents_4x=clean_latents_4x,  # History placeholder\n",
    "            clean_latent_4x_indices=clean_latent_4x_indices,  # Indices for history placeholder\n",
    "            image_embeddings=image_embeddings[b],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = max(1, os.cpu_count() - 1)\n",
    "\n",
    "all_latent_cache_paths = []\n",
    "for _, batch in tqdm(dataset.retrieve_latent_cache_batches(num_workers)):\n",
    "    filtered_batch = []\n",
    "    for item in batch:\n",
    "        if item.frame_count is None:\n",
    "            all_latent_cache_paths.append(item.latent_cache_path)\n",
    "            all_existing = os.path.exists(item.latent_cache_path)\n",
    "        else:\n",
    "            latent_f = (item.frame_count - 1) // 4 + 1\n",
    "            num_sections = max(1, math.floor((latent_f - 1) / item.fp_latent_window_size))  # min 1 section\n",
    "            all_existing = True\n",
    "            for sec in range(num_sections):\n",
    "                p = append_section_idx_to_latent_cache_path(item.latent_cache_path, sec)\n",
    "                all_latent_cache_paths.append(p)\n",
    "                all_existing = all_existing and os.path.exists(p)\n",
    "\n",
    "        if not all_existing:  # if any section cache is missing\n",
    "            filtered_batch.append(item)\n",
    "\n",
    "    if len(filtered_batch) == 0:  # all sections exist\n",
    "        continue\n",
    "\n",
    "    encode_and_save_batch_one_frame(\n",
    "        vae, feature_extractor, image_encoder, filtered_batch, False, True, True\n",
    "    )\n",
    "    \n",
    "# normalize paths\n",
    "all_latent_cache_paths = [os.path.normpath(p) for p in all_latent_cache_paths]\n",
    "all_latent_cache_paths = set(all_latent_cache_paths)\n",
    "\n",
    "# remove old cache files not in the dataset\n",
    "all_cache_files = dataset.get_all_latent_cache_files()\n",
    "for cache_file in all_cache_files:\n",
    "    if os.path.normpath(cache_file) not in all_latent_cache_paths:\n",
    "        if not args.keep_cache:\n",
    "            os.remove(cache_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
