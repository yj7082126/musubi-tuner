> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Wan 2.1

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Wan2.1](https://github.com/Wan-Video/Wan2.1). The features are as follows.

- fp8 support and memory reduction by block swap: Inference of a 720x1280x81frames videos with 24GB VRAM, training with 720x1280 images with 24GB VRAM
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- Supports xformers and Sage attention

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>
[Wan2.1](https://github.com/Wan-Video/Wan2.1) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–ï¼š720x1280x81framesã®å‹•ç”»ã‚’24GB VRAMã§æ¨è«–å¯èƒ½ã€720x1280ã®ç”»åƒã§ã®å­¦ç¿’ãŒ24GB VRAMã§å¯èƒ½
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- xformersãŠã‚ˆã³Sage attentionå¯¾å¿œ

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚
</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the T5 `models_t5_umt5-xxl-enc-bf16.pth` and CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` from the following page: https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/tree/main

Download the VAE from the above page `Wan2.1_VAE.pth` or download `split_files/vae/wan_2.1_vae.safetensors` from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

Download the DiT weights from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

Wan2.1 Fun Control model weights can be downloaded from [here](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control). Navigate to each weight page and download. The Fun Control model seems to support not only T2V but also I2V tasks.

Please select the appropriate weights according to T2V, I2V, resolution, model size, etc. 

`fp16` and `bf16` models can be used, and `fp8_e4m3fn` models can be used if `--fp8` (or `--fp8_base`) is specified without specifying `--fp8_scaled`. **Please note that `fp8_scaled` models are not supported even with `--fp8_scaled`.**

(Thanks to Comfy-Org for providing the repackaged weights.)

### Model support matrix / ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹

* columns: training dtype (è¡Œï¼šå­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿å‹)
* rows: model dtype (åˆ—ï¼šãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹)

| model \ training |bf16|fp16|--fp8_base|--fp8base & --fp8_scaled|
|--|--|--|--|--|
|bf16|âœ“|--|âœ“|âœ“|
|fp16|--|âœ“|âœ“|âœ“|
|fp8_e4m3fn|--|--|âœ“|--|
|fp8_scaled|--|--|--|--|

<details>
<summary>æ—¥æœ¬èª</summary>
T5 `models_t5_umt5-xxl-enc-bf16.pth` ãŠã‚ˆã³CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/tree/main

VAEã¯ä¸Šã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `Wan2.1_VAE.pth` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `split_files/vae/wan_2.1_vae.safetensors` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

DiTã®é‡ã¿ã‚’æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

Wan2.1 Fun Controlãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ã€[ã“ã¡ã‚‰](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)ã‹ã‚‰ã€ãã‚Œãã‚Œã®é‡ã¿ã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚Fun Controlãƒ¢ãƒ‡ãƒ«ã¯T2Vã ã‘ã§ãªãI2Vã‚¿ã‚¹ã‚¯ã«ã‚‚å¯¾å¿œã—ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚

T2Vã‚„I2Vã€è§£åƒåº¦ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãªã©ã«ã‚ˆã‚Šé©åˆ‡ãªé‡ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

`fp16` ãŠã‚ˆã³ `bf16` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ã¾ãŸã€`--fp8` ï¼ˆã¾ãŸã¯`--fp8_base`ï¼‰ã‚’æŒ‡å®šã—`--fp8_scaled`ã‚’æŒ‡å®šã‚’ã—ãªã„ã¨ãã«ã¯ `fp8_e4m3fn` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚**`fp8_scaled` ãƒ¢ãƒ‡ãƒ«ã¯ã„ãšã‚Œã®å ´åˆã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã®ã§ã”æ³¨æ„ãã ã•ã„ã€‚**

ï¼ˆrepackagedç‰ˆã®é‡ã¿ã‚’æä¾›ã—ã¦ãã ã•ã£ã¦ã„ã‚‹Comfy-Orgã«æ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚ï¼‰
</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

### Latent Pre-caching

Latent pre-caching is almost the same as in HunyuanVideo. Create the cache using the following command:

```bash
python src/musubi_tuner/wan_cache_latents.py --dataset_config path/to/toml --vae path/to/wan_2.1_vae.safetensors
```

If you train I2V models, add `--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` to specify the CLIP model. If not specified, the training will raise an error.

If you're running low on VRAM, specify `--vae_cache_cpu` to use the CPU for the VAE internal cache, which will reduce VRAM usage somewhat.

The control video settings are required for training the Fun-Control model. Please refer to [Dataset Settings](/src/musubi_tuner/dataset/dataset_config.md#sample-for-video-dataset-with-control-images) for details.

<details>
<summary>æ—¥æœ¬èª</summary>
latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¯HunyuanVideoã¨ã»ã¼åŒã˜ã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

I2Vãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€`--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚æŒ‡å®šã—ãªã„ã¨å­¦ç¿’æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

VRAMãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€`--vae_cache_cpu` ã‚’æŒ‡å®šã™ã‚‹ã¨VAEã®å†…éƒ¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«CPUã‚’ä½¿ã†ã“ã¨ã§ã€ä½¿ç”¨VRAMã‚’å¤šå°‘å‰Šæ¸›ã§ãã¾ã™ã€‚

Fun-Controlãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€åˆ¶å¾¡ç”¨å‹•ç”»ã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](/src/musubi_tuner/dataset/dataset_config.md#sample-for-video-dataset-with-control-images)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
</details>

### Text Encoder Output Pre-caching

Text encoder output pre-caching is also almost the same as in HunyuanVideo. Create the cache using the following command:

```bash
python src/musubi_tuner/wan_cache_text_encoder_outputs.py --dataset_config path/to/toml  --t5 path/to/models_t5_umt5-xxl-enc-bf16.pth --batch_size 16 
```

Adjust `--batch_size` according to your available VRAM.

For systems with limited VRAM (less than ~16GB), use `--fp8_t5` to run the T5 in fp8 mode.

<details>
<summary>æ—¥æœ¬èª</summary>
ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚‚HunyuanVideoã¨ã»ã¼åŒã˜ã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

VRAMãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç´„16GBæœªæº€ï¼‰ã®å ´åˆã¯ã€T5ã‚’fp8ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã« `--fp8_t5` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
</details>

## Training / å­¦ç¿’

### Training

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/wan_train_network.py 
    --task t2v-1.3B 
    --dit path/to/wan2.1_xxx_bf16.safetensors 
    --dataset_config path/to/toml --sdpa --mixed_precision bf16 --fp8_base 
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing 
    --max_data_loader_n_workers 2 --persistent_data_loader_workers 
    --network_module networks.lora_wan --network_dim 32 
    --timestep_sampling shift --discrete_flow_shift 3.0 
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42
    --output_dir path/to/output_dir --output_name name-of-lora
```
The above is an example. The appropriate values for `timestep_sampling` and `discrete_flow_shift` need to be determined by experimentation.

For additional options, use `python src/musubi_tuner/wan_train_network.py --help` (note that many options are unverified).

`--task` is one of `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` (for Wan2.1 official models), `t2v-1.3B-FC`, `t2v-14B-FC`, and `i2v-14B-FC` (for Wan2.1 Fun Control model). Specify the DiT weights for the task with `--dit`.

Don't forget to specify `--network_module networks.lora_wan`.

Other options are mostly the same as `hv_train_network.py`.

Use `convert_lora.py` for converting the LoRA weights after training, as in HunyuanVideo.

<details>
<summary>æ—¥æœ¬èª</summary>
`timestep_sampling`ã‚„`discrete_flow_shift`ã¯ä¸€ä¾‹ã§ã™ã€‚ã©ã®ã‚ˆã†ãªå€¤ãŒé©åˆ‡ã‹ã¯å®Ÿé¨“ãŒå¿…è¦ã§ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ `python src/musubi_tuner/wan_train_network.py --help` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆå¤šãã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æœªæ¤œè¨¼ã§ã™ï¼‰ã€‚

`--task` ã«ã¯ `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` ï¼ˆã“ã‚Œã‚‰ã¯Wan2.1å…¬å¼ãƒ¢ãƒ‡ãƒ«ï¼‰ã€`t2v-1.3B-FC`, `t2v-14B-FC`, `i2v-14B-FC`ï¼ˆWan2.1-Fun Controlãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--dit`ã«ã€taskã«å¿œã˜ãŸDiTã®é‡ã¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

 `--network_module` ã« `networks.lora_wan` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã»ã¼`hv_train_network.py`ã¨åŒæ§˜ã§ã™ã€‚

å­¦ç¿’å¾Œã®LoRAã®é‡ã¿ã®å¤‰æ›ã¯ã€HunyuanVideoã¨åŒæ§˜ã«`convert_lora.py`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
</details>

### Command line options for training with sampling / ã‚µãƒ³ãƒ—ãƒ«ç”»åƒç”Ÿæˆã«é–¢é€£ã™ã‚‹å­¦ç¿’æ™‚ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³

Example of command line options for training with sampling / è¨˜è¿°ä¾‹:  

```bash
--vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth 
--sample_prompts /path/to/prompt_file.txt 
--sample_every_n_epochs 1 --sample_every_n_steps 1000 -- sample_at_first
```
Each option is the same as when generating images or as HunyuanVideo. Please refer to [here](/docs/sampling_during_training.md) for details.

If you train I2V models, add `--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` to specify the CLIP model. 

You can specify the initial image, the negative prompt and the control video (for Wan2.1-Fun-Control) in the prompt file. Please refer to [here](/docs/sampling_during_training.md#prompt-file--ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«).

<details>
<summary>æ—¥æœ¬èª</summary>
å„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¨è«–æ™‚ã€ãŠã‚ˆã³HunyuanVideoã®å ´åˆã¨åŒæ§˜ã§ã™ã€‚[ã“ã¡ã‚‰](/docs/sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

I2Vãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€`--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã€åˆæœŸç”»åƒã‚„ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€åˆ¶å¾¡å‹•ç”»ï¼ˆWan2.1-Fun-Controlç”¨ï¼‰ç­‰ã‚’æŒ‡å®šã§ãã¾ã™ã€‚[ã“ã¡ã‚‰](/docs/sampling_during_training.md#prompt-file--ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
</details>


## Inference / æ¨è«–

### Inference Options Comparison / æ¨è«–ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ¯”è¼ƒ

#### Speed Comparison (Faster â†’ Slower) / é€Ÿåº¦æ¯”è¼ƒï¼ˆé€Ÿã„â†’é…ã„ï¼‰
*Note: Results may vary depending on GPU type*

fp8_fast > bf16/fp16 (no block swap) > fp8 > fp8_scaled > bf16/fp16 (block swap)

#### Quality Comparison (Higher â†’ Lower) / å“è³ªæ¯”è¼ƒï¼ˆé«˜â†’ä½ï¼‰

bf16/fp16 > fp8_scaled > fp8 >> fp8_fast

### T2V Inference / T2Væ¨è«–

The following is an example of T2V inference (input as a single line):

```bash
python src/musubi_tuner/wan_generate_video.py --fp8 --task t2v-1.3B --video_size  832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_t2v_1.3B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth 
--attn_mode torch
```

`--task` is one of `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` (these are Wan2.1 official models), `t2v-1.3B-FC`, `t2v-14B-FC` and `i2v-14B-FC` (for Wan2.1-Fun Control model).

`--attn_mode` is `torch`, `sdpa` (same as `torch`), `xformers`, `sageattn`,`flash2`, `flash` (same as `flash2`) or `flash3`. `torch` is the default. Other options require the corresponding library to be installed. `flash3` (Flash attention 3) is not tested.

Specifying `--fp8` runs DiT in fp8 mode. fp8 can significantly reduce memory consumption but may impact output quality.

`--fp8_scaled` can be specified in addition to `--fp8` to run the model in fp8 weights optimization. This increases memory consumption and speed slightly but improves output quality. See [here](advanced_config.md#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–) for details.

`--fp8_fast` option is also available for faster inference on RTX 40x0 GPUs. This option requires `--fp8_scaled` option. **This option seems to degrade the output quality.**

`--fp8_t5` can be used to specify the T5 model in fp8 format. This option reduces memory usage for the T5 model.  

`--negative_prompt` can be used to specify a negative prompt. If omitted, the default negative prompt is used.

`--flow_shift` can be used to specify the flow shift (default 3.0 for I2V with 480p, 5.0 for others).

`--guidance_scale` can be used to specify the guidance scale for classifier free guidance (default 5.0).

`--blocks_to_swap` is the number of blocks to swap during inference. The default value is None (no block swap). The maximum value is 39 for 14B model and 29 for 1.3B model.

`--vae_cache_cpu` enables VAE cache in main memory. This reduces VRAM usage slightly but processing is slower.

`--compile` enables torch.compile. See [here](/README.md#inference) for details.

`--trim_tail_frames` can be used to trim the tail frames when saving. The default is 0.

`--cfg_skip_mode` specifies the mode for skipping CFG in different steps. The default is `none` (all steps).`--cfg_apply_ratio` specifies the ratio of steps where CFG is applied. See below for details.

`--include_patterns` and `--exclude_patterns` can be used to specify which LoRA modules to apply or exclude during training. If not specified, all modules are applied by default. These options accept regular expressions. 

`--include_patterns` specifies the modules to be applied, and `--exclude_patterns` specifies the modules to be excluded. The regular expression is matched against the LoRA key name, and include takes precedence.

The key name to be searched is in sd-scripts format (`lora_unet_<module_name with dot replaced by _>`). For example, `lora_unet_blocks_9_cross_attn_k`.

For example, if you specify `--exclude_patterns "blocks_[23]\d_"`, it will exclude modules containing `blocks_20` to `blocks_39`. If you specify `--include_patterns "cross_attn" --exclude_patterns "blocks_(0|1|2|3|4)_"`, it will apply LoRA to modules containing `cross_attn` and not containing `blocks_0` to `blocks_4`.

If you specify multiple LoRA weights, please specify them with multiple arguments. For example: `--include_patterns "cross_attn" ".*" --exclude_patterns "dummy_do_not_exclude" "blocks_(0|1|2|3|4)"`. `".*"` is a regex that matches everything. `dummy_do_not_exclude` is a dummy regex that does not match anything.

`--cpu_noise` generates initial noise on the CPU. This may result in the same results as ComfyUI with the same seed (depending on other settings).

If you are using the Fun Control model, specify the control video with `--control_path`. You can specify a video file or a folder containing multiple image files. The number of frames in the video file (or the number of images) should be at least the number specified in `--video_length` (plus 1 frame if you specify `--end_image_path`).

Please try to match the aspect ratio of the control video with the aspect ratio specified in `--video_size` (there may be some deviation from the initial image of I2V due to the use of bucketing processing).

Other options are same as `hv_generate_video.py` (some options are not supported, please check the help).

<details>
<summary>æ—¥æœ¬èª</summary>
`--task` ã«ã¯ `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` ï¼ˆã“ã‚Œã‚‰ã¯Wan2.1å…¬å¼ãƒ¢ãƒ‡ãƒ«ï¼‰ã€`t2v-1.3B-FC`, `t2v-14B-FC`, `i2v-14B-FC`ï¼ˆWan2.1-Fun Controlãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚

`--attn_mode` ã«ã¯ `torch`, `sdpa`ï¼ˆ`torch`ã¨åŒã˜ï¼‰ã€`xformers`, `sageattn`, `flash2`, `flash`ï¼ˆ`flash2`ã¨åŒã˜ï¼‰, `flash3` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `torch` ã§ã™ã€‚ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€å¯¾å¿œã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚`flash3`ï¼ˆFlash attention 3ï¼‰ã¯æœªãƒ†ã‚¹ãƒˆã§ã™ã€‚

`--fp8` ã‚’æŒ‡å®šã™ã‚‹ã¨DiTãƒ¢ãƒ‡ãƒ«ã‚’fp8å½¢å¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚fp8ã¯ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ãŒã€å‡ºåŠ›å“è³ªã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
    
`--fp8_scaled` ã‚’ `--fp8` ã¨ä½µç”¨ã™ã‚‹ã¨ã€fp8ã¸ã®é‡ã¿é‡å­åŒ–ã‚’è¡Œã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã¨é€Ÿåº¦ã¯ã‚ãšã‹ã«æ‚ªåŒ–ã—ã¾ã™ãŒã€å‡ºåŠ›å“è³ªãŒå‘ä¸Šã—ã¾ã™ã€‚è©³ã—ãã¯[ã“ã¡ã‚‰](advanced_config.md#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--fp8_fast` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯RTX 40x0 GPUã§ã®é«˜é€Ÿæ¨è«–ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--fp8_scaled` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ã€‚**å‡ºåŠ›å“è³ªãŒåŠ£åŒ–ã™ã‚‹ã‚ˆã†ã§ã™ã€‚**

`--fp8_t5` ã‚’æŒ‡å®šã™ã‚‹ã¨T5ãƒ¢ãƒ‡ãƒ«ã‚’fp8å½¢å¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚T5ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

`--negative_prompt` ã§ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã§ãã¾ã™ã€‚çœç•¥ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--flow_shift` ã§flow shiftã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆ480pã®I2Vã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3.0ã€ãã‚Œä»¥å¤–ã¯5.0ï¼‰ã€‚

`--guidance_scale` ã§classifier free guianceã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5.0ï¼‰ã€‚

`--blocks_to_swap` ã¯æ¨è«–æ™‚ã®block swapã®æ•°ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯Noneï¼ˆblock swapãªã—ï¼‰ã§ã™ã€‚æœ€å¤§å€¤ã¯14Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ39ã€1.3Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ29ã§ã™ã€‚

`--vae_cache_cpu` ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€VAEã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã«ä¿æŒã—ã¾ã™ã€‚VRAMä½¿ç”¨é‡ãŒå¤šå°‘æ¸›ã‚Šã¾ã™ãŒã€å‡¦ç†ã¯é…ããªã‚Šã¾ã™ã€‚

`--compile`ã§torch.compileã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](/README.md#inference)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--trim_tail_frames` ã§ä¿å­˜æ™‚ã«æœ«å°¾ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã§ã™ã€‚

`--cfg_skip_mode` ã¯ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `none`ï¼ˆå…¨ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€‚`--cfg_apply_ratio` ã¯CFGãŒé©ç”¨ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’æŒ‡å®šã—ã¾ã™ã€‚è©³ç´°ã¯å¾Œè¿°ã—ã¾ã™ã€‚

LoRAã®ã©ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹ã‹ã‚’ã€`--include_patterns`ã¨`--exclude_patterns`ã§æŒ‡å®šã§ãã¾ã™ï¼ˆæœªæŒ‡å®šæ™‚ãƒ»ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é©ç”¨ã•ã‚Œã¾ã™
ï¼‰ã€‚ã“ã‚Œã‚‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¯ã€æ­£è¦è¡¨ç¾ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--include_patterns`ã¯é©ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€`--exclude_patterns`ã¯é©ç”¨ã—ãªã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚æ­£è¦è¡¨ç¾ãŒLoRAã®ã‚­ãƒ¼åã«å«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹ã§åˆ¤æ–­ã•ã‚Œã€includeãŒå„ªå…ˆã•ã‚Œã¾ã™ã€‚

æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ã‚­ãƒ¼åã¯ sd-scripts å½¢å¼ï¼ˆ`lora_unet_<ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã®ãƒ‰ãƒƒãƒˆã‚’_ã«ç½®æ›ã—ãŸã‚‚ã®>`ï¼‰ã§ã™ã€‚ä¾‹ï¼š`lora_unet_blocks_9_cross_attn_k`

ãŸã¨ãˆã° `--exclude_patterns "blocks_[23]\d_"`ã®ã¿ã‚’æŒ‡å®šã™ã‚‹ã¨ã€`blocks_20`ã‹ã‚‰`blocks_39`ã‚’å«ã‚€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒé™¤å¤–ã•ã‚Œã¾ã™ã€‚`--include_patterns "cross_attn" --exclude_patterns "blocks_(0|1|2|3|4)_"`ã®ã‚ˆã†ã«includeã¨excludeã‚’æŒ‡å®šã™ã‚‹ã¨ã€`cross_attn`ã‚’å«ã‚€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã€ã‹ã¤`blocks_0`ã‹ã‚‰`blocks_4`ã‚’å«ã¾ãªã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«LoRAãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚

è¤‡æ•°ã®LoRAã®é‡ã¿ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ã€è¤‡æ•°å€‹ã®å¼•æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼š`--include_patterns "cross_attn" ".*" --exclude_patterns "dummy_do_not_exclude" "blocks_(0|1|2|3|4)"` `".*"`ã¯å…¨ã¦ã«ãƒãƒƒãƒã™ã‚‹æ­£è¦è¡¨ç¾ã§ã™ã€‚`dummy_do_not_exclude`ã¯ä½•ã«ã‚‚ãƒãƒƒãƒã—ãªã„ãƒ€ãƒŸãƒ¼ã®æ­£è¦è¡¨ç¾ã§ã™ã€‚

`--cpu_noise`ã‚’æŒ‡å®šã™ã‚‹ã¨åˆæœŸãƒã‚¤ã‚ºã‚’CPUã§ç”Ÿæˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚ŠåŒä¸€seedæ™‚ã®çµæœãŒComfyUIã¨åŒã˜ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆä»–ã®è¨­å®šã«ã‚‚ã‚ˆã‚Šã¾ã™ï¼‰ã€‚

Fun Controlãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`--control_path`ã§åˆ¶å¾¡ç”¨ã®æ˜ åƒã‚’æŒ‡å®šã—ã¾ã™ã€‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã€ã¾ãŸã¯è¤‡æ•°æšã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚“ã ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã§ãã¾ã™ã€‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆã¾ãŸã¯ç”»åƒã®æšæ•°ï¼‰ã¯ã€`--video_length`ã§æŒ‡å®šã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ æ•°ä»¥ä¸Šã«ã—ã¦ãã ã•ã„ï¼ˆå¾Œè¿°ã®`--end_image_path`ã‚’æŒ‡å®šã—ãŸå ´åˆã¯ã€ã•ã‚‰ã«+1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã€‚

åˆ¶å¾¡ç”¨ã®æ˜ åƒã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¯ã€`--video_size`ã§æŒ‡å®šã—ãŸã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¨ã§ãã‚‹ã‹ãã‚Šåˆã‚ã›ã¦ãã ã•ã„ï¼ˆbucketingã®å‡¦ç†ã‚’æµç”¨ã—ã¦ã„ã‚‹ãŸã‚I2Vã®åˆæœŸç”»åƒã¨ã‚ºãƒ¬ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `hv_generate_video.py` ã¨åŒã˜ã§ã™ï¼ˆä¸€éƒ¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ˜ãƒ«ãƒ—ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰ã€‚
</details>

#### CFG Skip Mode / CFGã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰

 These options allow you to balance generation speed against prompt accuracy. More skipped steps results in faster generation with potential quality degradation.

Setting `--cfg_apply_ratio` to 0.5 speeds up the denoising loop by up to 25%.

`--cfg_skip_mode` specified one of the following modes:

- `early`: Skips CFG in early steps for faster generation, applying guidance mainly in later refinement steps
- `late`: Skips CFG in later steps, applying guidance during initial structure formation
- `middle`: Skips CFG in middle steps, applying guidance in both early and later steps
- `early_late`: Skips CFG in both early and late steps, applying only in middle steps
- `alternate`: Applies CFG in alternate steps based on the specified ratio
- `none`: Applies CFG at all steps (default)

`--cfg_apply_ratio` specifies a value from 0.0 to 1.0 controlling the proportion of steps where CFG is applied. For example, setting 0.5 means CFG will be applied in only 50% of the steps.

If num_steps is 10, the following table shows the steps where CFG is applied based on the `--cfg_skip_mode` option (A means CFG is applied, S means it is skipped, `--cfg_apply_ratio` is 0.6):

| skip mode | CFG apply pattern |
|---|---|
| early | SSSSAAAAAA |
| late | AAAAAASSSS |
| middle | AAASSSSAAA |
| early_late | SSAAAAAASS |
| alternate | SASASAASAS |

The appropriate settings are unknown, but you may want to try `late` or `early_late` mode with a ratio of around 0.3 to 0.5.
<details>
<summary>æ—¥æœ¬èª</summary>
ã“ã‚Œã‚‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ç”Ÿæˆé€Ÿåº¦ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãŒå¤šã„ã»ã©ã€ç”Ÿæˆé€Ÿåº¦ãŒé€Ÿããªã‚Šã¾ã™ãŒã€å“è³ªãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ratioã«0.5ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã®ãƒ«ãƒ¼ãƒ—ãŒæœ€å¤§25%ç¨‹åº¦ã€é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

`--cfg_skip_mode` ã¯æ¬¡ã®ãƒ¢ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ï¼š

- `early`ï¼šåˆæœŸã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ä¸»ã«çµ‚ç›¤ã®ç²¾ç´°åŒ–ã®ã‚¹ãƒ†ãƒƒãƒ—ã§é©ç”¨ã—ã¾ã™
- `late`ï¼šçµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€åˆæœŸã®æ§‹é€ ãŒæ±ºã¾ã‚‹æ®µéšã§é©ç”¨ã—ã¾ã™
- `middle`ï¼šä¸­é–“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€åˆæœŸã¨çµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸¡æ–¹ã§é©ç”¨ã—ã¾ã™
- `early_late`ï¼šåˆæœŸã¨çµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸¡æ–¹ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ä¸­é–“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿é©ç”¨ã—ã¾ã™
- `alternate`ï¼šæŒ‡å®šã•ã‚ŒãŸå‰²åˆã«åŸºã¥ã„ã¦CFGã‚’é©ç”¨ã—ã¾ã™

`--cfg_apply_ratio` ã¯ã€CFGãŒé©ç”¨ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’0.0ã‹ã‚‰1.0ã®å€¤ã§æŒ‡å®šã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€0.5ã«è¨­å®šã™ã‚‹ã¨ã€CFGã¯ã‚¹ãƒ†ãƒƒãƒ—ã®50%ã®ã¿ã§é©ç”¨ã•ã‚Œã¾ã™ã€‚

å…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¸Šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

é©åˆ‡ãªè¨­å®šã¯ä¸æ˜ã§ã™ãŒã€ãƒ¢ãƒ¼ãƒ‰ã¯`late`ã¾ãŸã¯`early_late`ã€ratioã¯0.3~0.5ç¨‹åº¦ã‹ã‚‰è©¦ã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
</details>

#### Skip Layer Guidance

Skip Layer Guidance is a feature that uses the output of a model with some blocks skipped as the unconditional output of classifier free guidance. It was originally proposed in [SD 3.5](https://github.com/comfyanonymous/ComfyUI/pull/5404) and first applied in Wan2GP in [this PR](https://github.com/deepbeepmeep/Wan2GP/pull/61). It may improve the quality of generated videos.

The implementation of SD 3.5 is [here](https://github.com/Stability-AI/sd3.5/blob/main/sd3_impls.py), and the implementation of Wan2GP (the PR mentioned above) has some different specifications. This inference script allows you to choose between the two methods.

*The SD3.5 method applies slg output in addition to cond and uncond (slows down the speed). The Wan2GP method uses only cond and slg output.*

The following arguments are available:

- `--slg_mode`: Specifies the SLG mode. `original` for SD 3.5 method, `uncond` for Wan2GP method. Default is None (no SLG).
- `--slg_layers`: Specifies the indices of the blocks (layers) to skip in SLG, separated by commas. Example: `--slg_layers 4,5,6`. Default is empty (no skip). If this option is not specified, `--slg_mode` is ignored.
- `--slg_scale`: Specifies the scale of SLG when `original`. Default is 3.0.
- `--slg_start`: Specifies the start step of SLG application in inference steps from 0.0 to 1.0. Default is 0.0 (applied from the beginning).
- `--slg_end`: Specifies the end step of SLG application in inference steps from 0.0 to 1.0. Default is 0.3 (applied up to 30% from the beginning).

Appropriate settings are unknown, but you may want to try `original` mode with a scale of around 3.0 and a start ratio of 0.0 and an end ratio of 0.5, with layers 4, 5, and 6 skipped.

<details>
<summary>æ—¥æœ¬èª</summary>
Skip Layer Guidanceã¯ã€ä¸€éƒ¨ã®blockã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’classifier free guidanceã®unconditionalå‡ºåŠ›ã«ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚å…ƒã€…ã¯[SD 3.5](https://github.com/comfyanonymous/ComfyUI/pull/5404)ã§ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã§ã€Wan2.1ã«ã¯[Wan2GPã®ã“ã¡ã‚‰ã®PR](https://github.com/deepbeepmeep/Wan2GP/pull/61)ã§åˆã‚ã¦é©ç”¨ã•ã‚Œã¾ã—ãŸã€‚ç”Ÿæˆå‹•ç”»ã®å“è³ªãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

SD 3.5ã®å®Ÿè£…ã¯[ã“ã¡ã‚‰](https://github.com/Stability-AI/sd3.5/blob/main/sd3_impls.py)ã§ã€Wan2GPã®å®Ÿè£…ï¼ˆå‰è¿°ã®PRï¼‰ã¯ä¸€éƒ¨ä»•æ§˜ãŒç•°ãªã‚Šã¾ã™ã€‚ã“ã®æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ä¸¡è€…ã®æ–¹å¼ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚

â€»SD3.5æ–¹å¼ã¯condã¨uncondã«åŠ ãˆã¦slg outputã‚’é©ç”¨ã—ã¾ã™ï¼ˆé€Ÿåº¦ãŒä½ä¸‹ã—ã¾ã™ï¼‰ã€‚Wan2GPæ–¹å¼ã¯condã¨slg outputã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ä»¥ä¸‹ã®å¼•æ•°ãŒã‚ã‚Šã¾ã™ã€‚

- `--slg_mode`ï¼šSLGã®ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚`original`ã§SD 3.5ã®æ–¹å¼ã€`uncond`ã§Wan2GPã®æ–¹å¼ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã§ã€SLGã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚
- `--slg_layers`ï¼šSLGã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹block (layer)ã®ã‚¤ãƒ³ãƒ‡ã‚¯ã‚¹ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŒ‡å®šã—ã¾ã™ã€‚ä¾‹ï¼š`--slg_layers 4,5,6`ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç©ºï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ï¼‰ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ãªã„ã¨`--slg_mode`ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
- `--slg_scale`ï¼š`original`ã®ã¨ãã®SLGã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3.0ã§ã™ã€‚
- `--slg_start`ï¼šæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®SLGé©ç”¨é–‹å§‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’0.0ã‹ã‚‰1.0ã®å‰²åˆã§æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.0ã§ã™ï¼ˆæœ€åˆã‹ã‚‰é©ç”¨ï¼‰ã€‚
- `--slg_end`ï¼šæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®SLGé©ç”¨çµ‚äº†ã‚¹ãƒ†ãƒƒãƒ—ã‚’0.0ã‹ã‚‰1.0ã®å‰²åˆã§æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.3ã§ã™ï¼ˆæœ€åˆã‹ã‚‰30%ã¾ã§é©ç”¨ï¼‰ã€‚

é©åˆ‡ãªè¨­å®šã¯ä¸æ˜ã§ã™ãŒã€`original`ãƒ¢ãƒ¼ãƒ‰ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’3.0ç¨‹åº¦ã€é–‹å§‹å‰²åˆã‚’0.0ã€çµ‚äº†å‰²åˆã‚’0.5ç¨‹åº¦ã«è¨­å®šã—ã€4, 5, 6ã®layerã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¨­å®šã‹ã‚‰å§‹ã‚ã‚‹ã¨è‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
</details>

### I2V Inference / I2Væ¨è«–

The following is an example of I2V inference (input as a single line):

```bash
python src/musubi_tuner/wan_generate_video.py --fp8 --task i2v-14B --video_size 832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_i2v_480p_14B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth --clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth 
--attn_mode torch --image_path path/to/image.jpg
```

Add `--clip` to specify the CLIP model. `--image_path` is the path to the image to be used as the initial frame.

`--end_image_path` can be used to specify the end image. This option is experimental. When this option is specified, the saved video will be slightly longer than the specified number of frames and will have noise, so it is recommended to specify `--trim_tail_frames 3` to trim the tail frames.

You can also use the Fun Control model for I2V inference. Specify the control video with `--control_path`. 

Other options are same as T2V inference.

<details>
<summary>æ—¥æœ¬èª</summary>
`--clip` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--image_path` ã¯åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹ã§ã™ã€‚

`--end_image_path` ã§çµ‚äº†ç”»åƒã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ä¿å­˜ã•ã‚Œã‚‹å‹•ç”»ãŒæŒ‡å®šãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚ˆã‚Šã‚‚ã‚„ã‚„å¤šããªã‚Šã€ã‹ã¤ãƒã‚¤ã‚ºãŒä¹—ã‚‹ãŸã‚ã€`--trim_tail_frames 3` ãªã©ã‚’æŒ‡å®šã—ã¦æœ«å°¾ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

I2Væ¨è«–ã§ã‚‚Fun Controlãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã§ãã¾ã™ã€‚`--control_path` ã§åˆ¶å¾¡ç”¨ã®æ˜ åƒã‚’æŒ‡å®šã—ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯T2Væ¨è«–ã¨åŒã˜ã§ã™ã€‚
</details>

### New Batch and Interactive Modes / æ–°ã—ã„ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰

In addition to single video generation, Wan 2.1 now supports batch generation from file and interactive prompt input:

#### Batch Mode from File / ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰

Generate multiple videos from prompts stored in a text file:

```bash
python src/musubi_tuner/wan_generate_video.py --from_file prompts.txt --task t2v-14B 
--dit path/to/model.safetensors --vae path/to/vae.safetensors 
--t5 path/to/t5_model.pth --save_path output_directory
```

The prompts file format:
- One prompt per line
- Empty lines and lines starting with # are ignored (comments)
- Each line can include prompt-specific parameters using command-line style format:

```
A beautiful sunset over mountains --w 832 --h 480 --f 81 --d 42 --s 20
A busy city street at night --w 480 --h 832 --g 7.5 --n low quality, blurry
```

Supported inline parameters (if ommitted, default values from the command line are used):
- `--w`: Width
- `--h`: Height
- `--f`: Frame count
- `--d`: Seed
- `--s`: Inference steps
- `--g` or `--l`: Guidance scale
- `--fs`: Flow shift
- `--i`: Image path (for I2V)
- `--cn`: Control path (for Fun Control)
- `--n`: Negative prompt

In batch mode, models are loaded once and reused for all prompts, significantly improving overall generation time compared to multiple single runs.

#### Interactive Mode / ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰

Interactive command-line interface for entering prompts:

```bash
python src/musubi_tuner/wan_generate_video.py --interactive --task t2v-14B 
--dit path/to/model.safetensors --vae path/to/vae.safetensors 
--t5 path/to/t5_model.pth --save_path output_directory
```

In interactive mode:
- Enter prompts directly at the command line
- Use the same inline parameter format as batch mode
- Use Ctrl+D (or Ctrl+Z on Windows) to exit
- Models remain loaded between generations for efficiency

<details>
<summary>æ—¥æœ¬èª</summary>
å˜ä¸€å‹•ç”»ã®ç”Ÿæˆã«åŠ ãˆã¦ã€Wan 2.1ã¯ç¾åœ¨ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒãƒƒãƒç”Ÿæˆã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

#### ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰

ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰è¤‡æ•°ã®å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ï¼š

```bash
python src/musubi_tuner/wan_generate_video.py --from_file prompts.txt --task t2v-14B 
--dit path/to/model.safetensors --vae path/to/vae.safetensors 
--t5 path/to/t5_model.pth --save_path output_directory
```

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ï¼š
- 1è¡Œã«1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- ç©ºè¡Œã‚„#ã§å§‹ã¾ã‚‹è¡Œã¯ç„¡è¦–ã•ã‚Œã¾ã™ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰
- å„è¡Œã«ã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å½¢å¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆçœç•¥ã—ãŸå ´åˆã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒä½¿ç”¨ã•ã‚Œã¾ã™ï¼‰
- `--w`: å¹…
- `--h`: é«˜ã•
- `--f`: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
- `--d`: ã‚·ãƒ¼ãƒ‰
- `--s`: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—
- `--g` ã¾ãŸã¯ `--l`: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«
- `--fs`: ãƒ•ãƒ­ãƒ¼ã‚·ãƒ•ãƒˆ
- `--i`: ç”»åƒãƒ‘ã‚¹ï¼ˆI2Vç”¨ï¼‰
- `--cn`: ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ã‚¹ï¼ˆFun Controlç”¨ï¼‰
- `--n`: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å†åˆ©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€è¤‡æ•°å›ã®å˜ä¸€å®Ÿè¡Œã¨æ¯”è¼ƒã—ã¦å…¨ä½“çš„ãªç”Ÿæˆæ™‚é–“ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã™ã€‚

#### ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼š

```bash
python src/musubi_tuner/wan_generate_video.py --interactive --task t2v-14B 
--dit path/to/model.safetensors --vae path/to/vae.safetensors 
--t5 path/to/t5_model.pth --save_path output_directory
```

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ï¼š
- ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›
- ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã¨åŒã˜ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã‚’ä½¿ç”¨
- çµ‚äº†ã™ã‚‹ã«ã¯ Ctrl+D (Windowsã§ã¯ Ctrl+Z) ã‚’ä½¿ç”¨
- åŠ¹ç‡ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿæˆé–“ã§èª­ã¿è¾¼ã¾ã‚ŒãŸã¾ã¾ã«ãªã‚Šã¾ã™
</details>

