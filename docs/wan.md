> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Inference with WAN2.1 / Wan2.1ã®æ¨è«–

## Overview / æ¦‚è¦

This is an unofficial inference script for [Wan2.1](https://github.com/Wan-Video/Wan2.1). The features are as follows.

- fp8 support and memory reduction by block swap: Inference of a 720x1280x81frames video is possible with 24GB VRAM
    
- Flash attention can be executed without installation (using PyTorch's scaled dot product attention)
- Supports xformers and Sage attention

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>
[Wan2.1](https://github.com/Wan-Video/Wan2.1) ã®éå…¬å¼æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–ï¼š720x1280x81framesã®å‹•ç”»ã‚’24GB VRAMã§æ¨è«–å¯èƒ½
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- xformersãŠã‚ˆã³Sage attentionå¯¾å¿œ

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚
</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the T5 `models_t5_umt5-xxl-enc-bf16.pth` and CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` from the following page: https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/tree/main

Download the VAE from the above page `Wan2.1_VAE.pth` or download `split_files/vae/wan_2.1_vae.safetensors` from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

Download the DiT weights from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

Please select the appropriate weights according to T2V, I2V, resolution, model size, etc. fp8 models can be used if `--fp8` is specified.

(Thanks to Comfy-Org for providing the repackaged weights.)
<details>
<summary>æ—¥æœ¬èª</summary>
T5 `models_t5_umt5-xxl-enc-bf16.pth` ãŠã‚ˆã³CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Wan-AI/Wan2.1-T2V-14B/tree/main

VAEã¯ä¸Šã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `Wan2.1_VAE.pth` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `split_files/vae/wan_2.1_vae.safetensors` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

DiTã®é‡ã¿ã‚’æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

T2Vã‚„I2Vã€è§£åƒåº¦ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãªã©ã«ã‚ˆã‚Šé©åˆ‡ãªé‡ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚`--fp8`æŒ‡å®šæ™‚ã¯fp8ãƒ¢ãƒ‡ãƒ«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚

ï¼ˆrepackagedç‰ˆã®é‡ã¿ã‚’æä¾›ã—ã¦ãã ã•ã£ã¦ã„ã‚‹Comfy-Orgã«æ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚ï¼‰
</details>

## Inference / æ¨è«–

### T2V Inference / T2Væ¨è«–

The following is an example of T2V inference (input as a single line):

```bash
python wan_generate_video.py --fp8 --task t2v-1.3B --video_size  832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_t2v_1.3B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth 
--attn_mode torch
```

`--task` is one of `t2v-1.3B`, `t2v-14B`, `i2v-14B` and `t2i-14B`.

`--attn_mode` is `torch`, `sdpa` (same as `torch`), `xformers`, `sageattn`,`flash2`, `flash` (same as `flash2`) or `flash3`. `torch` is the default. Other options require the corresponding library to be installed. `flash3` (Flash attention 3) is not tested.

`--fp8_t5` can be used to specify the T5 model in fp8 format. This option reduces memory usage for the T5 model.  

`--negative_prompt` can be used to specify a negative prompt. If omitted, the default negative prompt is used.

` --flow_shift` can be used to specify the flow shift (default 3.0 for I2V with 480p, 5.0 for others).

`--guidance_scale` can be used to specify the guidance scale for classifier free guiance (default 5.0).

`--blocks_to_swap` is the number of blocks to swap during inference. The default value is None (no block swap). The maximum value is 39 for 14B model and 29 for 1.3B model.

`--vae_cache_cpu` enables VAE cache in main memory. This reduces VRAM usage slightly but processing is slower.

Other options are same as `hv_generate_video.py` (some options are not supported, please check the help).

<details>
<summary>æ—¥æœ¬èª</summary>
`--task` ã«ã¯ `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚

`--attn_mode` ã«ã¯ `torch`, `sdpa`ï¼ˆ`torch`ã¨åŒã˜ï¼‰ã€`xformers`, `sageattn`, `flash2`, `flash`ï¼ˆ`flash2`ã¨åŒã˜ï¼‰, `flash3` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `torch` ã§ã™ã€‚ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€å¯¾å¿œã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚`flash3`ï¼ˆFlash attention 3ï¼‰ã¯æœªãƒ†ã‚¹ãƒˆã§ã™ã€‚

`--fp8_t5` ã‚’æŒ‡å®šã™ã‚‹ã¨T5ãƒ¢ãƒ‡ãƒ«ã‚’fp8å½¢å¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚T5ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

`--negative_prompt` ã§ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã§ãã¾ã™ã€‚çœç•¥ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--flow_shift` ã§flow shiftã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆ480pã®I2Vã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3.0ã€ãã‚Œä»¥å¤–ã¯5.0ï¼‰ã€‚

`--guidance_scale` ã§classifier free guianceã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5.0ï¼‰ã€‚

`--blocks_to_swap` ã¯æ¨è«–æ™‚ã®block swapã®æ•°ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯Noneï¼ˆblock swapãªã—ï¼‰ã§ã™ã€‚æœ€å¤§å€¤ã¯14Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ39ã€1.3Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ29ã§ã™ã€‚

`--vae_cache_cpu` ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€VAEã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã«ä¿æŒã—ã¾ã™ã€‚VRAMä½¿ç”¨é‡ãŒå¤šå°‘æ¸›ã‚Šã¾ã™ãŒã€å‡¦ç†ã¯é…ããªã‚Šã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `hv_generate_video.py` ã¨åŒã˜ã§ã™ï¼ˆä¸€éƒ¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ˜ãƒ«ãƒ—ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰ã€‚
</details>

### I2V Inference / I2Væ¨è«–

The following is an example of I2V inference (input as a single line):

```bash
python wan_generate_video.py --fp8 --task i2v-14B --video_size 832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_i2v_480p_14B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth --clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth 
--attn_mode torch --image_path path/to/image.jpg
```

Add `--clip` to specify the CLIP model. `--image_path` is the path to the image to be used as the initial frame.

Other options are same as T2V inference.

<details>
<summary>æ—¥æœ¬èª</summary>
`--clip` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--image_path` ã¯åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹ã§ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯T2Væ¨è«–ã¨åŒã˜ã§ã™ã€‚
</details>
