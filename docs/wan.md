> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Wan 2.1

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Wan2.1](https://github.com/Wan-Video/Wan2.1). The features are as follows.

- fp8 support and memory reduction by block swap: Inference of a 720x1280x81frames videos with 24GB VRAM, training with 720x1280 images with 24GB VRAM
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- Supports xformers and Sage attention

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>
[Wan2.1](https://github.com/Wan-Video/Wan2.1) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–ï¼š720x1280x81framesã®å‹•ç”»ã‚’24GB VRAMã§æ¨è«–å¯èƒ½ã€720x1280ã®ç”»åƒã§ã®å­¦ç¿’ãŒ24GB VRAMã§å¯èƒ½
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- xformersãŠã‚ˆã³Sage attentionå¯¾å¿œ

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚
</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the T5 `models_t5_umt5-xxl-enc-bf16.pth` and CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` from the following page: https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/tree/main

Download the VAE from the above page `Wan2.1_VAE.pth` or download `split_files/vae/wan_2.1_vae.safetensors` from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

Download the DiT weights from the following page: https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

Please select the appropriate weights according to T2V, I2V, resolution, model size, etc. 

`fp16` and `bf16` models can be used, and `fp8_e4m3fn` models can be used if `--fp8` (or `--fp8_base`) is specified without specifying `--fp8_scaled`. **Please note that `fp8_scaled` models are not supported even with `--fp8_scaled`.**

(Thanks to Comfy-Org for providing the repackaged weights.)

### Model support matrix / ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹

* columns: training dtype (è¡Œï¼šå­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿å‹)
* rows: model dtype (åˆ—ï¼šãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹)

| model \ training |bf16|fp16|--fp8_base|--fp8base & --fp8_scaled|
|--|--|--|--|--|
|bf16|âœ“|--|âœ“|âœ“|
|fp16|--|âœ“|âœ“|âœ“|
|fp8_e4m3fn|--|--|âœ“|--|
|fp8_scaled|--|--|--|--|

<details>
<summary>æ—¥æœ¬èª</summary>
T5 `models_t5_umt5-xxl-enc-bf16.pth` ãŠã‚ˆã³CLIP `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/tree/main

VAEã¯ä¸Šã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `Wan2.1_VAE.pth` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ `split_files/vae/wan_2.1_vae.safetensors` ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/vae

DiTã®é‡ã¿ã‚’æ¬¡ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼šhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models

T2Vã‚„I2Vã€è§£åƒåº¦ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãªã©ã«ã‚ˆã‚Šé©åˆ‡ãªé‡ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

`fp16` ãŠã‚ˆã³ `bf16` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ã¾ãŸã€`--fp8` ï¼ˆã¾ãŸã¯`--fp8_base`ï¼‰ã‚’æŒ‡å®šã—`--fp8_scaled`ã‚’æŒ‡å®šã‚’ã—ãªã„ã¨ãã«ã¯ `fp8_e4m3fn` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚**`fp8_scaled` ãƒ¢ãƒ‡ãƒ«ã¯ã„ãšã‚Œã®å ´åˆã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã®ã§ã”æ³¨æ„ãã ã•ã„ã€‚**

ï¼ˆrepackagedç‰ˆã®é‡ã¿ã‚’æä¾›ã—ã¦ãã ã•ã£ã¦ã„ã‚‹Comfy-Orgã«æ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚ï¼‰
</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

### Latent Pre-caching

Latent pre-caching is almost the same as in HunyuanVideo. Create the cache using the following command:

```bash
python wan_cache_latents.py --dataset_config path/to/toml --vae path/to/wan_2.1_vae.safetensors
```

If you train I2V models, add `--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` to specify the CLIP model. If not specified, the training will raise an error.

If you're running low on VRAM, specify `--vae_cache_cpu` to use the CPU for the VAE internal cache, which will reduce VRAM usage somewhat.

<details>
<summary>æ—¥æœ¬èª</summary>
latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¯HunyuanVideoã¨ã»ã¼åŒã˜ã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

I2Vãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€`--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚æŒ‡å®šã—ãªã„ã¨å­¦ç¿’æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

VRAMãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€`--vae_cache_cpu` ã‚’æŒ‡å®šã™ã‚‹ã¨VAEã®å†…éƒ¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«CPUã‚’ä½¿ã†ã“ã¨ã§ã€ä½¿ç”¨VRAMã‚’å¤šå°‘å‰Šæ¸›ã§ãã¾ã™ã€‚
</details>

### Text Encoder Output Pre-caching

Text encoder output pre-caching is also almost the same as in HunyuanVideo. Create the cache using the following command:

```bash
python wan_cache_text_encoder_outputs.py --dataset_config path/to/toml  --t5 path/to/models_t5_umt5-xxl-enc-bf16.pth --batch_size 16 
```

Adjust `--batch_size` according to your available VRAM.

For systems with limited VRAM (less than ~16GB), use `--fp8_t5` to run the T5 in fp8 mode.

<details>
<summary>æ—¥æœ¬èª</summary>
ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚‚HunyuanVideoã¨ã»ã¼åŒã˜ã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

VRAMãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç´„16GBæœªæº€ï¼‰ã®å ´åˆã¯ã€T5ã‚’fp8ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã« `--fp8_t5` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
</details>

## Training / å­¦ç¿’

### Training

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 wan_train_network.py 
    --task t2v-1.3B 
    --dit path/to/wan2.1_xxx_bf16.safetensors 
    --dataset_config path/to/toml --sdpa --mixed_precision bf16 --fp8_base 
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing 
    --max_data_loader_n_workers 2 --persistent_data_loader_workers 
    --network_module networks.lora_wan --network_dim 32 
    --timestep_sampling shift --discrete_flow_shift 3.0 
    --max_train_epochs 16 --save_every_n_epochs 1 --seed 42
    --output_dir path/to/output_dir --output_name name-of-lora
```
The above is an example. The appropriate values for `timestep_sampling` and `discrete_flow_shift` need to be determined by experimentation.

For additional options, use `python wan_train_network.py --help` (note that many options are unverified).

`--task` is one of `t2v-1.3B`, `t2v-14B`, `i2v-14B` and `t2i-14B`. Specify the DiT weights for the task with `--dit`.

Don't forget to specify `--network_module networks.lora_wan`.

Other options are mostly the same as `hv_train_network.py`.

Use `convert_lora.py` for converting the LoRA weights after training, as in HunyuanVideo.

<details>
<summary>æ—¥æœ¬èª</summary>
`timestep_sampling`ã‚„`discrete_flow_shift`ã¯ä¸€ä¾‹ã§ã™ã€‚ã©ã®ã‚ˆã†ãªå€¤ãŒé©åˆ‡ã‹ã¯å®Ÿé¨“ãŒå¿…è¦ã§ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ `python wan_train_network.py --help` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆå¤šãã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æœªæ¤œè¨¼ã§ã™ï¼‰ã€‚

`--task` ã«ã¯ `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--dit`ã«ã€taskã«å¿œã˜ãŸDiTã®é‡ã¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

 `--network_module` ã« `networks.lora_wan` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã»ã¼`hv_train_network.py`ã¨åŒæ§˜ã§ã™ã€‚

å­¦ç¿’å¾Œã®LoRAã®é‡ã¿ã®å¤‰æ›ã¯ã€HunyuanVideoã¨åŒæ§˜ã«`convert_lora.py`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
</details>

### Command line options for training with sampling / ã‚µãƒ³ãƒ—ãƒ«ç”»åƒç”Ÿæˆã«é–¢é€£ã™ã‚‹å­¦ç¿’æ™‚ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³

Example of command line options for training with sampling / è¨˜è¿°ä¾‹:  

```bash
--vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth 
--sample_prompts /path/to/prompt_file.txt 
--sample_every_n_epochs 1 --sample_every_n_steps 1000 -- sample_at_first
```
Each option is the same as when generating images or as HunyuanVideo. Please refer to [here](/docs/sampling_during_training.md) for details.

If you train I2V models, add `--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` to specify the CLIP model. 

You can specify the initial image and negative prompts in the prompt file. Please refer to [here](/docs/sampling_during_training.md#prompt-file--ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«).

<details>
<summary>æ—¥æœ¬èª</summary>
å„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ¨è«–æ™‚ã€ãŠã‚ˆã³HunyuanVideoã®å ´åˆã¨åŒæ§˜ã§ã™ã€‚[ã“ã¡ã‚‰](/docs/sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

I2Vãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€`--clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã€åˆæœŸç”»åƒã‚„ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç­‰ã‚’æŒ‡å®šã§ãã¾ã™ã€‚[ã“ã¡ã‚‰](/docs/sampling_during_training.md#prompt-file--ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
</details>


## Inference / æ¨è«–

### Inference Options Comparison / æ¨è«–ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ¯”è¼ƒ

#### Speed Comparison (Faster â†’ Slower) / é€Ÿåº¦æ¯”è¼ƒï¼ˆé€Ÿã„â†’é…ã„ï¼‰
*Note: Results may vary depending on GPU type*

fp8_fast > bf16/fp16 (no block swap) > fp8 > fp8_scaled > bf16/fp16 (block swap)

#### Quality Comparison (Higher â†’ Lower) / å“è³ªæ¯”è¼ƒï¼ˆé«˜â†’ä½ï¼‰

bf16/fp16 > fp8_scaled > fp8 >> fp8_fast

### T2V Inference / T2Væ¨è«–

The following is an example of T2V inference (input as a single line):

```bash
python wan_generate_video.py --fp8 --task t2v-1.3B --video_size  832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_t2v_1.3B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth 
--attn_mode torch
```

`--task` is one of `t2v-1.3B`, `t2v-14B`, `i2v-14B` and `t2i-14B`.

`--attn_mode` is `torch`, `sdpa` (same as `torch`), `xformers`, `sageattn`,`flash2`, `flash` (same as `flash2`) or `flash3`. `torch` is the default. Other options require the corresponding library to be installed. `flash3` (Flash attention 3) is not tested.

Specifying `--fp8` runs DiT in fp8 mode. fp8 can significantly reduce memory consumption but may impact output quality.

`--fp8_scaled` can be specified in addition to `--fp8` to run the model in fp8 weights optimization. This increases memory consumption and speed slightly but improves output quality. See [here](advanced_config.md#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–) for details.

`--fp8_fast` option is also available for faster inference on RTX 40x0 GPUs. This option requires `--fp8_scaled` option. **This option seems to degrade the output quality.**

`--fp8_t5` can be used to specify the T5 model in fp8 format. This option reduces memory usage for the T5 model.  

`--negative_prompt` can be used to specify a negative prompt. If omitted, the default negative prompt is used.

`--flow_shift` can be used to specify the flow shift (default 3.0 for I2V with 480p, 5.0 for others).

`--guidance_scale` can be used to specify the guidance scale for classifier free guidance (default 5.0).

`--blocks_to_swap` is the number of blocks to swap during inference. The default value is None (no block swap). The maximum value is 39 for 14B model and 29 for 1.3B model.

`--vae_cache_cpu` enables VAE cache in main memory. This reduces VRAM usage slightly but processing is slower.

`--compile` enables torch.compile. See [here](/README.md#inference) for details.

`--trim_tail_frames` can be used to trim the tail frames when saving. The default is 0.

`--cfg_skip_mode` specifies the mode for skipping CFG in different steps. The default is `none` (all steps).`--cfg_apply_ratio` specifies the ratio of steps where CFG is applied. See below for details.

Other options are same as `hv_generate_video.py` (some options are not supported, please check the help).

<details>
<summary>æ—¥æœ¬èª</summary>
`--task` ã«ã¯ `t2v-1.3B`, `t2v-14B`, `i2v-14B`, `t2i-14B` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚

`--attn_mode` ã«ã¯ `torch`, `sdpa`ï¼ˆ`torch`ã¨åŒã˜ï¼‰ã€`xformers`, `sageattn`, `flash2`, `flash`ï¼ˆ`flash2`ã¨åŒã˜ï¼‰, `flash3` ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `torch` ã§ã™ã€‚ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€å¯¾å¿œã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚`flash3`ï¼ˆFlash attention 3ï¼‰ã¯æœªãƒ†ã‚¹ãƒˆã§ã™ã€‚

`--fp8` ã‚’æŒ‡å®šã™ã‚‹ã¨DiTãƒ¢ãƒ‡ãƒ«ã‚’fp8å½¢å¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚fp8ã¯ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ãŒã€å‡ºåŠ›å“è³ªã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
    
`--fp8_scaled` ã‚’ `--fp8` ã¨ä½µç”¨ã™ã‚‹ã¨ã€fp8ã¸ã®é‡ã¿é‡å­åŒ–ã‚’è¡Œã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã¨é€Ÿåº¦ã¯ã‚ãšã‹ã«æ‚ªåŒ–ã—ã¾ã™ãŒã€å‡ºåŠ›å“è³ªãŒå‘ä¸Šã—ã¾ã™ã€‚è©³ã—ãã¯[ã“ã¡ã‚‰](advanced_config.md#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--fp8_fast` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯RTX 40x0 GPUã§ã®é«˜é€Ÿæ¨è«–ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--fp8_scaled` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ã€‚**å‡ºåŠ›å“è³ªãŒåŠ£åŒ–ã™ã‚‹ã‚ˆã†ã§ã™ã€‚**

`--fp8_t5` ã‚’æŒ‡å®šã™ã‚‹ã¨T5ãƒ¢ãƒ‡ãƒ«ã‚’fp8å½¢å¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚T5ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

`--negative_prompt` ã§ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã§ãã¾ã™ã€‚çœç•¥ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`--flow_shift` ã§flow shiftã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆ480pã®I2Vã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3.0ã€ãã‚Œä»¥å¤–ã¯5.0ï¼‰ã€‚

`--guidance_scale` ã§classifier free guianceã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5.0ï¼‰ã€‚

`--blocks_to_swap` ã¯æ¨è«–æ™‚ã®block swapã®æ•°ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯Noneï¼ˆblock swapãªã—ï¼‰ã§ã™ã€‚æœ€å¤§å€¤ã¯14Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ39ã€1.3Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆ29ã§ã™ã€‚

`--vae_cache_cpu` ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€VAEã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã«ä¿æŒã—ã¾ã™ã€‚VRAMä½¿ç”¨é‡ãŒå¤šå°‘æ¸›ã‚Šã¾ã™ãŒã€å‡¦ç†ã¯é…ããªã‚Šã¾ã™ã€‚

`--compile`ã§torch.compileã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](/README.md#inference)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--trim_tail_frames` ã§ä¿å­˜æ™‚ã«æœ«å°¾ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã§ã™ã€‚

`--cfg_skip_mode` ã¯ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `none`ï¼ˆå…¨ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€‚`--cfg_apply_ratio` ã¯CFGãŒé©ç”¨ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’æŒ‡å®šã—ã¾ã™ã€‚è©³ç´°ã¯å¾Œè¿°ã—ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `hv_generate_video.py` ã¨åŒã˜ã§ã™ï¼ˆä¸€éƒ¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ˜ãƒ«ãƒ—ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰ã€‚
</details>

#### CFG Skip Mode / CFGã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰

 These options allow you to balance generation speed against prompt accuracy. More skipped steps results in faster generation with potential quality degradation.

Setting `--cfg_apply_ratio` to 0.5 speeds up the denoising loop by up to 25%.

`--cfg_skip_mode` specified one of the following modes:

- `early`: Skips CFG in early steps for faster generation, applying guidance mainly in later refinement steps
- `late`: Skips CFG in later steps, applying guidance during initial structure formation
- `middle`: Skips CFG in middle steps, applying guidance in both early and later steps
- `early_late`: Skips CFG in both early and late steps, applying only in middle steps
- `alternate`: Applies CFG in alternate steps based on the specified ratio
- `none`: Applies CFG at all steps (default)

`--cfg_apply_ratio` specifies a value from 0.0 to 1.0 controlling the proportion of steps where CFG is applied. For example, setting 0.5 means CFG will be applied in only 50% of the steps.

If num_steps is 10, the following table shows the steps where CFG is applied based on the `--cfg_skip_mode` option (A means CFG is applied, S means it is skipped, `--cfg_apply_ratio` is 0.6):

| skip mode | CFG apply pattern |
|---|---|
| early | SSSSAAAAAA |
| late | AAAAAASSSS |
| middle | AAASSSSAAA |
| early_late | SSAAAAAASS |
| alternate | SASASAASAS |

The appropriate settings are unknown, but you may want to try `late` or `early_late` mode with a ratio of around 0.3 to 0.5.
<details>
<summary>æ—¥æœ¬èª</summary>
ã“ã‚Œã‚‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ç”Ÿæˆé€Ÿåº¦ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãŒå¤šã„ã»ã©ã€ç”Ÿæˆé€Ÿåº¦ãŒé€Ÿããªã‚Šã¾ã™ãŒã€å“è³ªãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ratioã«0.5ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã®ãƒ«ãƒ¼ãƒ—ãŒæœ€å¤§25%ç¨‹åº¦ã€é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

`--cfg_skip_mode` ã¯æ¬¡ã®ãƒ¢ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¾ã™ï¼š

- `early`ï¼šåˆæœŸã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ä¸»ã«çµ‚ç›¤ã®ç²¾ç´°åŒ–ã®ã‚¹ãƒ†ãƒƒãƒ—ã§é©ç”¨ã—ã¾ã™
- `late`ï¼šçµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€åˆæœŸã®æ§‹é€ ãŒæ±ºã¾ã‚‹æ®µéšã§é©ç”¨ã—ã¾ã™
- `middle`ï¼šä¸­é–“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€åˆæœŸã¨çµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸¡æ–¹ã§é©ç”¨ã—ã¾ã™
- `early_late`ï¼šåˆæœŸã¨çµ‚ç›¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸¡æ–¹ã§CFGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ä¸­é–“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿é©ç”¨ã—ã¾ã™
- `alternate`ï¼šæŒ‡å®šã•ã‚ŒãŸå‰²åˆã«åŸºã¥ã„ã¦CFGã‚’é©ç”¨ã—ã¾ã™

`--cfg_apply_ratio` ã¯ã€CFGãŒé©ç”¨ã•ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’0.0ã‹ã‚‰1.0ã®å€¤ã§æŒ‡å®šã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€0.5ã«è¨­å®šã™ã‚‹ã¨ã€CFGã¯ã‚¹ãƒ†ãƒƒãƒ—ã®50%ã®ã¿ã§é©ç”¨ã•ã‚Œã¾ã™ã€‚

å…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¸Šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

é©åˆ‡ãªè¨­å®šã¯ä¸æ˜ã§ã™ãŒã€ãƒ¢ãƒ¼ãƒ‰ã¯`late`ã¾ãŸã¯`early_late`ã€ratioã¯0.3~0.5ç¨‹åº¦ã‹ã‚‰è©¦ã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
</details>

### I2V Inference / I2Væ¨è«–

The following is an example of I2V inference (input as a single line):

```bash
python wan_generate_video.py --fp8 --task i2v-14B --video_size 832 480 --video_length 81 --infer_steps 20 
--prompt "prompt for the video" --save_path path/to/save.mp4 --output_type both 
--dit path/to/wan2.1_i2v_480p_14B_bf16_etc.safetensors --vae path/to/wan_2.1_vae.safetensors 
--t5 path/to/models_t5_umt5-xxl-enc-bf16.pth --clip path/to/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth 
--attn_mode torch --image_path path/to/image.jpg
```

Add `--clip` to specify the CLIP model. `--image_path` is the path to the image to be used as the initial frame.

`--end_image_path` can be used to specify the end image. This option is experimental. When this option is specified, the saved video will be slightly longer than the specified number of frames and will have noise, so it is recommended to specify `--trim_tail_frames 3` to trim the tail frames.

Other options are same as T2V inference.

<details>
<summary>æ—¥æœ¬èª</summary>
`--clip` ã‚’è¿½åŠ ã—ã¦CLIPãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚`--image_path` ã¯åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹ã§ã™ã€‚

`--end_image_path` ã§çµ‚äº†ç”»åƒã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ä¿å­˜ã•ã‚Œã‚‹å‹•ç”»ãŒæŒ‡å®šãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚ˆã‚Šã‚‚ã‚„ã‚„å¤šããªã‚Šã€ã‹ã¤ãƒã‚¤ã‚ºãŒä¹—ã‚‹ãŸã‚ã€`--trim_tail_frames 3` ãªã©ã‚’æŒ‡å®šã—ã¦æœ«å°¾ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚


ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯T2Væ¨è«–ã¨åŒã˜ã§ã™ã€‚
</details>
