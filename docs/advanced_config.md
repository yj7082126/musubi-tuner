> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Advanced configuration / é«˜åº¦ãªè¨­å®š

## Table of contents / ç›®æ¬¡

- [How to specify `network_args`](#how-to-specify-network_args--network_argsã®æŒ‡å®šæ–¹æ³•)
- [LoRA+](#lora)
- [Select the target modules of LoRA](#select-the-target-modules-of-lora--loraã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹)
- [Save and view logs in TensorBoard format](#save-and-view-logs-in-tensorboard-format--tensorboardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [Save and view logs in wandb](#save-and-view-logs-in-wandb--wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [FP8 weight optimization for models](#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–)
- [PyTorch Dynamo optimization for model training](#pytorch-dynamo-optimization-for-model-training--ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹pytorch-dynamoã®æœ€é©åŒ–)
- [LoRA Post-Hoc EMA merging](#lora-post-hoc-ema-merging--loraã®post-hoc-emaãƒãƒ¼ã‚¸)
- [MagCache](#magcache)

## How to specify `network_args` / `network_args`ã®æŒ‡å®šæ–¹æ³•

The `--network_args` option is an option for specifying detailed arguments to LoRA. Specify the arguments in the form of `key=value` in `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>
`--network_args`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€LoRAã¸ã®è©³ç´°ãªå¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚`--network_args`ã«ã¯ã€`key=value`ã®å½¢å¼ã§å¼•æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

If you specify it on the command line, write as follows. / ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 
    --network_args "key1=value1" "key2=value2" ...
```

If you specify it in the configuration file, write as follows. / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```toml
network_args = ["key1=value1", "key2=value2", ...]
```

If you specify `"verbose=True"`, detailed information of LoRA will be displayed. / `"verbose=True"`ã‚’æŒ‡å®šã™ã‚‹ã¨LoRAã®è©³ç´°ãªæƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```bash
--network_args "verbose=True" "key1=value1" "key2=value2" ...
```

## LoRA+

LoRA+ is a method to improve the training speed by increasing the learning rate of the UP side (LoRA-B) of LoRA. Specify the multiplier for the learning rate. The original paper recommends 16, but adjust as needed. It seems to be good to start from around 4. For details, please refer to the [related PR of sd-scripts](https://github.com/kohya-ss/sd-scripts/pull/1233).

Specify `loraplus_lr_ratio` with `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA+ã¯ã€LoRAã®UPå´ï¼ˆLoRA-Bï¼‰ã®å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã§å­¦ç¿’é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚å­¦ç¿’ç‡ã«å¯¾ã™ã‚‹å€ç‡ã‚’æŒ‡å®šã—ã¾ã™ã€‚å…ƒè«–æ–‡ã§ã¯16ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚4ç¨‹åº¦ã‹ã‚‰å§‹ã‚ã‚‹ã¨ã‚ˆã„ã‚ˆã†ã§ã™ã€‚è©³ç´°ã¯[sd-scriptsã®é–¢é€£PR]https://github.com/kohya-ss/sd-scripts/pull/1233)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--network_args`ã§`loraplus_lr_ratio`ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 --network_args "loraplus_lr_ratio=4" ...
```

## Select the target modules of LoRA / LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹

*This feature is highly experimental and the specification may change. / ã“ã®æ©Ÿèƒ½ã¯ç‰¹ã«å®Ÿé¨“çš„ãªã‚‚ã®ã§ã€ä»•æ§˜ã¯å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚*

By specifying `exclude_patterns` and `include_patterns` with `--network_args`, you can select the target modules of LoRA.

`exclude_patterns` excludes modules that match the specified pattern. `include_patterns` targets only modules that match the specified pattern.

Specify the values as a list. For example, `"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`.

The pattern is a regular expression for the module name. The module name is in the form of `double_blocks.0.img_mod.linear` or `single_blocks.39.modulation.linear`. The regular expression is not a partial match but a complete match.

The patterns are applied in the order of `exclude_patterns`â†’`include_patterns`. By default, the Linear layers of `img_mod`, `txt_mod`, and `modulation` of double blocks and single blocks are excluded.

(`.*(img_mod|txt_mod|modulation).*` is specified.)

<details>
<summary>æ—¥æœ¬èª</summary>

`--network_args`ã§`exclude_patterns`ã¨`include_patterns`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

`exclude_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é™¤å¤–ã—ã¾ã™ã€‚`include_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

å€¤ã¯ã€ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¾ã™ã€‚`"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã«å¯¾ã™ã‚‹æ­£è¦è¡¨ç¾ã§ã™ã€‚ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã¯ã€ãŸã¨ãˆã°`double_blocks.0.img_mod.linear`ã‚„`single_blocks.39.modulation.linear`ã®ã‚ˆã†ãªå½¢å¼ã§ã™ã€‚æ­£è¦è¡¨ç¾ã¯éƒ¨åˆ†ä¸€è‡´ã§ã¯ãªãå®Œå…¨ä¸€è‡´ã§ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€`exclude_patterns`â†’`include_patterns`ã®é †ã§é©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€double blocksã¨single blocksã®Linearå±¤ã®ã†ã¡ã€`img_mod`ã€`txt_mod`ã€`modulation`ãŒé™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚

ï¼ˆ`.*(img_mod|txt_mod|modulation).*`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ï¼‰
</details>

### Example / è¨˜è¿°ä¾‹

Only the modules of double blocks / double blocksã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*single_blocks.*']"
```

Only the modules of single blocks from the 10th / single blocksã®10ç•ªç›®ä»¥é™ã®Linearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*']" "include_patterns=[r'.*single_blocks\.\d{2}\.linear.*']"
```

## Save and view logs in TensorBoard format / TensorBoardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

Specify the folder to save the logs with the `--logging_dir` option. Logs in TensorBoard format will be saved.

For example, if you specify `--logging_dir=logs`, a `logs` folder will be created in the working folder, and logs will be saved in the date folder inside it.

Also, if you specify the `--log_prefix` option, the specified string will be added before the date. For example, use `--logging_dir=logs --log_prefix=lora_setting1_` for identification.

To view logs in TensorBoard, open another command prompt and activate the virtual environment. Then enter the following in the working folder.

```powershell
tensorboard --logdir=logs
```

(tensorboard installation is required.)

Then open a browser and access http://localhost:6006/ to display it.

<details>
<summary>æ—¥æœ¬èª</summary>
`--logging_dir`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ãƒ­ã‚°ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚TensorBoardå½¢å¼ã®ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ãŸã¨ãˆã°`--logging_dir=logs`ã¨æŒ‡å®šã™ã‚‹ã¨ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã«logsãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã€ãã®ä¸­ã®æ—¥æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ã¾ãŸ`--log_prefix`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨ã€æ—¥æ™‚ã®å‰ã«æŒ‡å®šã—ãŸæ–‡å­—åˆ—ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚`--logging_dir=logs --log_prefix=lora_setting1_`ãªã©ã¨ã—ã¦è­˜åˆ¥ç”¨ã«ãŠä½¿ã„ãã ã•ã„ã€‚

TensorBoardã§ãƒ­ã‚°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€åˆ¥ã®ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ãã€ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹ã«ã—ã¦ã‹ã‚‰ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å…¥åŠ›ã—ã¾ã™ã€‚

```powershell
tensorboard --logdir=logs
```

ï¼ˆtensorboardã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ï¼‰

ãã®å¾Œãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€http://localhost:6006/ ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
</details>

## Save and view logs in wandb / wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

`--log_with wandb` option is available to save logs in wandb format. `tensorboard` or `all` is also available. The default is `tensorboard`.

Specify the project name with `--log_tracker_name` when using wandb.

<details>
<summary>æ—¥æœ¬èª</summary>
`--log_with wandb`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨wandbå½¢å¼ã§ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚`tensorboard`ã‚„`all`ã‚‚æŒ‡å®šå¯èƒ½ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`tensorboard`ã§ã™ã€‚

wandbã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`--log_tracker_name`ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
</details>

## FP8 weight optimization for models / ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®FP8ã¸ã®æœ€é©åŒ–

The `--fp8_scaled` option is available to quantize the weights of the model to FP8 (E4M3) format with appropriate scaling. This reduces the VRAM usage while maintaining precision. Important weights are kept in FP16/BF16/FP32 format.

The model weights must be in fp16 or bf16. Weights that have been pre-converted to float8_e4m3 cannot be used.

Wan2.1 inference and training are supported.

Specify the `--fp8_scaled` option in addition to the `--fp8` option during inference.

Specify the `--fp8_scaled` option in addition to the `--fp8_base` option during training.

Acknowledgments: This feature is based on the [implementation](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py) of [HunyuanVideo](https://github.com/Tencent/HunyuanVideo). The selection of high-precision modules is based on the [implementation](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py) of [diffusion-pipe](https://github.com/tdrussell/diffusion-pipe). I would like to thank these repositories.

<details>
<summary>æ—¥æœ¬èª</summary>
é‡ã¿ã‚’å˜ç´”ã«FP8ã¸castã™ã‚‹ã®ã§ã¯ãªãã€é©åˆ‡ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§FP8å½¢å¼ã«é‡å­åŒ–ã™ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚ã¾ãŸã€é‡è¦ãªé‡ã¿ã¯FP16/BF16/FP32å½¢å¼ã§ä¿æŒã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ã€fp16ã¾ãŸã¯bf16ãŒå¿…è¦ã§ã™ã€‚ã‚ã‚‰ã‹ã˜ã‚float8_e4m3ã«å¤‰æ›ã•ã‚ŒãŸé‡ã¿ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚

Wan2.1ã®æ¨è«–ã€å­¦ç¿’ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

æ¨è«–æ™‚ã¯`--fp8`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŠ ãˆã¦ `--fp8_scaled`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

å­¦ç¿’æ™‚ã¯`--fp8_base`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŠ ãˆã¦ `--fp8_scaled`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

è¬è¾ï¼šã“ã®æ©Ÿèƒ½ã¯ã€[HunyuanVideo](https://github.com/Tencent/HunyuanVideo)ã®[å®Ÿè£…](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py)ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã¾ãŸã€é«˜ç²¾åº¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é¸æŠã«ãŠã„ã¦ã¯[diffusion-pipe](https://github.com/tdrussell/diffusion-pipe)ã®[å®Ÿè£…](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py)ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒªã«æ„Ÿè¬ã—ã¾ã™ã€‚

</details>

### Key features and implementation details / ä¸»ãªç‰¹å¾´ã¨å®Ÿè£…ã®è©³ç´°

- Implements FP8 (E4M3) weight quantization for Linear layers
- Reduces VRAM requirements by using 8-bit weights for storage (slightly increased compared to existing `--fp8` `--fp8_base` options)
- Quantizes weights to FP8 format with appropriate scaling instead of simple cast to FP8
- Maintains computational precision by dequantizing to original precision (FP16/BF16/FP32) during forward pass
- Preserves important weights in FP16/BF16/FP32 format

The implementation:

1. Quantizes weights to FP8 format with appropriate scaling
2. Replaces weights by FP8 quantized weights and stores scale factors in model state dict
3. Applies monkey patching to Linear layers for transparent dequantization during computation

<details>
<summary>æ—¥æœ¬èª</summary>

- Linearå±¤ã®FP8ï¼ˆE4M3ï¼‰é‡ã¿é‡å­åŒ–ã‚’å®Ÿè£…
- 8ãƒ“ãƒƒãƒˆã®é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ï¼ˆæ—¢å­˜ã®`--fp8` `--fp8_base` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«æ¯”ã¹ã¦å¾®å¢—ï¼‰
- å˜ç´”ãªFP8ã¸ã®castã§ã¯ãªãã€é©åˆ‡ãªå€¤ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦é‡ã¿ã‚’FP8å½¢å¼ã«é‡å­åŒ–
- forwardæ™‚ã«å…ƒã®ç²¾åº¦ï¼ˆFP16/BF16/FP32ï¼‰ã«é€†é‡å­åŒ–ã—ã¦è¨ˆç®—ç²¾åº¦ã‚’ç¶­æŒ
- ç²¾åº¦ãŒé‡è¦ãªé‡ã¿ã¯FP16/BF16/FP32ã®ã¾ã¾ä¿æŒ

å®Ÿè£…:

1. ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹é©åˆ‡ãªå€ç‡ã§é‡ã¿ã‚’FP8å½¢å¼ã«é‡å­åŒ–
2. é‡ã¿ã‚’FP8é‡å­åŒ–é‡ã¿ã«ç½®ãæ›ãˆã€å€ç‡ã‚’ãƒ¢ãƒ‡ãƒ«ã®state dictã«ä¿å­˜
3. Linearå±¤ã«monkey patchingã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã›ãšã«é€†é‡å­åŒ–
 </details>

 ## PyTorch Dynamo optimization for model training / ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã‘ã‚‹PyTorch Dynamoã®æœ€é©åŒ–

The PyTorch Dynamo options are now available to optimize the training process. PyTorch Dynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster by using TorchInductor, a deep learning compiler. This integration allows for potential speedups in training while maintaining model accuracy.

[PR #215](https://github.com/kohya-ss/musubi-tuner/pull/215) added this feature.

Specify the `--dynamo_backend` option to enable Dynamo optimization with one of the available backends from the `DynamoBackend` enum.

Additional options allow for fine-tuning the Dynamo behavior:
- `--dynamo_mode`: Controls the optimization strategy
- `--dynamo_fullgraph`: Enables fullgraph mode for potentially better optimization
- `--dynamo_dynamic`: Enables dynamic shape handling

The `--dynamo_dynamic` option has been reported to have many problems based on the validation in PR #215.

### Available options:

```
--dynamo_backend {NO, INDUCTOR, NVFUSER, CUDAGRAPHS, CUDAGRAPHS_FALLBACK, etc.}
    Specifies the Dynamo backend to use (default is NO, which disables Dynamo)

--dynamo_mode {default, reduce-overhead, max-autotune}
    Specifies the optimization mode (default is 'default')
    - 'default': Standard optimization
    - 'reduce-overhead': Focuses on reducing compilation overhead
    - 'max-autotune': Performs extensive autotuning for potentially better performance

--dynamo_fullgraph
    Flag to enable fullgraph mode, which attempts to capture and optimize the entire model graph

--dynamo_dynamic
    Flag to enable dynamic shape handling for models with variable input shapes
```

### Usage example:

```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode default
```

For more aggressive optimization:
```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode max-autotune --dynamo_fullgraph
```

Note: The best combination of options may depend on your specific model and hardware. Experimentation may be necessary to find the optimal configuration.

<details>
<summary>æ—¥æœ¬èª</summary>
PyTorch Dynamoã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚PyTorch Dynamoã¯ã€TorchInductorï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å¤‰æ›´ã‚’åŠ ãˆã‚‹ã“ã¨ãªãPyTorchãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã®Pythonãƒ¬ãƒ™ãƒ«ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã§ã™ã€‚ã“ã®çµ±åˆã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ç¶­æŒã—ãªãŒã‚‰å­¦ç¿’ã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™ã€‚

[PR #215](https://github.com/kohya-ss/musubi-tuner/pull/215) ã§è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚

`--dynamo_backend`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ã€`DynamoBackend`åˆ—æŒ™å‹ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ä¸€ã¤ã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€Dynamoæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚

è¿½åŠ ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€Dynamoã®å‹•ä½œã‚’å¾®èª¿æ•´ã§ãã¾ã™ï¼š
- `--dynamo_mode`ï¼šæœ€é©åŒ–æˆ¦ç•¥ã‚’åˆ¶å¾¡ã—ã¾ã™
- `--dynamo_fullgraph`ï¼šã‚ˆã‚Šè‰¯ã„æœ€é©åŒ–ã®å¯èƒ½æ€§ã®ãŸã‚ã«ãƒ•ãƒ«ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã—ã¾ã™
- `--dynamo_dynamic`ï¼šå‹•çš„å½¢çŠ¶å‡¦ç†ã‚’æœ‰åŠ¹ã«ã—ã¾ã™

PR #215ã§ã®æ¤œè¨¼ã«ã‚ˆã‚‹ã¨ã€`--dynamo_dynamic`ã«ã¯å•é¡ŒãŒå¤šã„ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

__åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š__

```
--dynamo_backend {NO, INDUCTOR, NVFUSER, CUDAGRAPHS, CUDAGRAPHS_FALLBACK, ãªã©}
    ä½¿ç”¨ã™ã‚‹Dynamoãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯NOã§ã€Dynamoã‚’ç„¡åŠ¹ã«ã—ã¾ã™ï¼‰

--dynamo_mode {default, reduce-overhead, max-autotune}
    æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'default'ï¼‰
    - 'default'ï¼šæ¨™æº–çš„ãªæœ€é©åŒ–
    - 'reduce-overhead'ï¼šã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹
    - 'max-autotune'ï¼šã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã«åºƒç¯„ãªè‡ªå‹•èª¿æ•´ã‚’å®Ÿè¡Œ

--dynamo_fullgraph
    ãƒ•ãƒ«ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°ã€‚ãƒ¢ãƒ‡ãƒ«ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦æœ€é©åŒ–ã—ã‚ˆã†ã¨ã—ã¾ã™

--dynamo_dynamic
    å¯å¤‰å…¥åŠ›å½¢çŠ¶ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®å‹•çš„å½¢çŠ¶å‡¦ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°
```

__ä½¿ç”¨ä¾‹ï¼š__

```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode default
```

ã‚ˆã‚Šç©æ¥µçš„ãªæœ€é©åŒ–ã®å ´åˆï¼š
```bash
python src/musubi_tuner/hv_train_network.py --dynamo_backend INDUCTOR --dynamo_mode max-autotune --dynamo_fullgraph
```

æ³¨æ„ï¼šæœ€é©ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®çµ„ã¿åˆã‚ã›ã¯ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ä¾å­˜ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚æœ€é©ãªæ§‹æˆã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«å®Ÿé¨“ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
</details>

## LoRA Post-Hoc EMA merging / LoRAã®Post-Hoc EMAãƒãƒ¼ã‚¸

The LoRA Post-Hoc EMA (Exponential Moving Average) merging is a technique to combine multiple LoRA checkpoint files into a single, potentially more stable model. This method applies exponential moving average across multiple checkpoints sorted by modification time, with configurable decay rates.

The Post-Hoc EMA method works by:

1. Sorting checkpoint files by modification time (oldest to newest)
2. Using the oldest checkpoint as the base
3. Iteratively merging subsequent checkpoints with a decay rate (beta)
4. Optionally using linear interpolation between two beta values across the merge process

Pseudo-code for merging multiple checkpoints with beta=0.95 would look like this:

```
beta = 0.95
checkpoints = [checkpoint1, checkpoint2, checkpoint3]  # List of checkpoints
merged_weights = checkpoints[0]  # Use the first checkpoint as the base
for checkpoint in checkpoints[1:]:
    merged_weights = beta * merged_weights + (1 - beta) * checkpoint
```

### Key features:

- **Temporal ordering**: Automatically sorts files by modification time
- **Configurable decay rates**: Supports single beta value or linear interpolation between two beta values
- **Metadata preservation**: Maintains and updates metadata from the last checkpoint
- **Hash updating**: Recalculates model hashes for the merged weights
- **Dtype preservation**: Maintains original data types of tensors

### Usage

The LoRA Post-Hoc EMA merging is available as a standalone script:

```bash
python src/musubi_tuner/lora_post_hoc_ema.py checkpoint1.safetensors checkpoint2.safetensors checkpoint3.safetensors --output_file merged_lora.safetensors --beta 0.95
```

### Command line options:

```
path [path ...]
    List of paths to the LoRA weight files to merge

--beta BETA
    Decay rate for merging weights (default: 0.95)
    Higher values (closer to 1.0) give more weight to the accumulated average
    Lower values give more weight to the current checkpoint

--beta2 BETA2
    Second decay rate for linear interpolation (optional)
    If specified, the decay rate will linearly interpolate from beta to beta2
    across the merging process

--sigma_rel SIGMA_REL
    Relative sigma for Power Function EMA (optional, mutually exclusive with beta/beta2)
    This resolves the issue where the first checkpoint has a disproportionately large influence when beta is specified.
    If specified, beta is calculated using the Power Function EMA method from the paper:
    https://arxiv.org/pdf/2312.02696. This overrides beta and beta2.

--output_file OUTPUT_FILE
    Output file path for the merged weights (required)

--no_sort
    Disable sorting of checkpoint files (merge in specified order)
```

### Examples:

Basic usage with constant decay rate:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_merged.safetensors \
    --beta 0.95
```

Using linear interpolation between two decay rates:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_interpolated.safetensors \
    --beta 0.90 \
    --beta2 0.95
```

Using Power Function EMA with `sigma_rel`:
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_power_ema_merged.safetensors \
    --sigma_rel 0.2
```


#### betas for different Ïƒ-rel values:

![beta-sigma_rel-graph](./betas_for_sigma_rel.png)

### Recommended settings example (after training for 30 epochs, using  `--beta`)

If you're unsure which settings to try, start with the following "General Recommended Settings".

#### 1. General Recommended Settings (start with these combinations)

- **Target Epochs:** `15-30` (the latter half of training)
- **beta:** `0.9` (a balanced value)

#### 2. If training converged early

- **Situation:** Loss dropped early and stabilized afterwards.
- **Target Epochs:** `10-30` (from the epoch where loss stabilized to the end)
- **beta:** `0.95` (wider range, smoother)

#### 3. If you want to avoid overfitting

- **Situation:** In the latter part of training, generated results are too similar to training data.
- **Target Epochs:** `15-25` (focus on the peak performance range)
- **beta:** `0.8` (more emphasis on the latter part of the range while maintaining diversity)

**Note:** The optimal values may vary depending on the model and dataset. It's recommended to experiment with multiple `beta` values (e.g., 0.8, 0.9, 0.95) and compare the generated results.

### Recommended Settings Example (30 epochs training, using `--sigma_rel`)

When using `--sigma_rel`, the beta decay schedule is determined by the Power Function EMA method. Here are some starting points:

#### 1. General Recommended Settings
- **Target Epochs:** All epochs (from the first to the last).
- **sigma_rel:** `0.2` (a general starting point).

#### 2. If training converged early
- **Situation:** Loss dropped early and stabilized afterwards.
- **Target Epochs:** All epochs.
- **sigma_rel:** `0.25` (gives more weight to earlier checkpoints, suitable for early convergence).

#### 3. If you want to avoid overfitting
- **Situation:** In the latter part of training, generated results are too similar to training data.
- **Target Epochs:** From the first epoch, omitting the last few potentially overfitted epochs.
- **sigma_rel:** `0.15` (gives more weight to later (but not the very last) checkpoints, helping to mitigate overfitting from the final stages).

**Note:** The optimal `sigma_rel` value can depend on the dataset, model, and training duration. Experimentation is encouraged. Values typically range from 0.1 to 0.5. A graph showing the relationship between `sigma_rel` and the calculated `beta` values over epochs will be provided later to help understand its behavior.

### Notes:

- Files are automatically sorted by modification time, so the order in the command line doesn't matter
- The `--sigma_rel` option is mutually exclusive with `--beta` and `--beta2`. If `--sigma_rel` is provided, it will determine the beta values, and any provided `--beta` or `--beta2` will be ignored.
- All checkpoint files to be merged should be from the same training run, saved per epoch or step
    - Merging is possible if shapes match, but may not work correctly as Post Hoc EMA
- All checkpoint files must have the same alpha value
- The merged model will have updated hash values in its metadata 
- The metadata of the merged model will be taken from the last checkpoint, with only the hash value recalculated
- Non-float tensors (long, int, bool, etc.) are not merged and will use the first checkpoint's values
- Processing is done in float32 precision to maintain numerical stability during merging. The original data types are preserved when saving

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA Post-Hoc EMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ãƒãƒ¼ã‚¸ã¯ã€è¤‡æ•°ã®LoRAãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å˜ä¸€ã®ã€ã‚ˆã‚Šå®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã«çµåˆã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€ä¿®æ­£æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰ã•ã‚ŒãŸè¤‡æ•°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦æŒ‡å®šã•ã‚ŒãŸæ¸›è¡°ç‡ã§æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’é©ç”¨ã—ã¾ã™ã€‚æ¸›è¡°ç‡ã¯æŒ‡å®šå¯èƒ½ã§ã™ã€‚

Post-Hoc EMAæ–¹æ³•ã®å‹•ä½œï¼š

1. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£æ™‚åˆ»é †ï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã¸ï¼‰ã«ã‚½ãƒ¼ãƒˆ
2. æœ€å¤ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
3. æ¸›è¡°ç‡ï¼ˆbetaï¼‰ã‚’ä½¿ã£ã¦å¾Œç¶šã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åå¾©çš„ã«ãƒãƒ¼ã‚¸
4. ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§2ã¤ã®ãƒ™ãƒ¼ã‚¿å€¤é–“ã®ç·šå½¢è£œé–“ã‚’ä½¿ç”¨

ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼šè¤‡æ•°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’beta=0.95ã§ãƒãƒ¼ã‚¸ã™ã‚‹å ´åˆã€æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚

```
beta = 0.95
checkpoints = [checkpoint1, checkpoint2, checkpoint3]  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
merged_weights = checkpoints[0]  # æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
for checkpoint in checkpoints[1:]:
    merged_weights = beta * merged_weights + (1 - beta) * checkpoint
```

### ä¸»ãªç‰¹å¾´ï¼š

- **æ™‚ç³»åˆ—é †åºä»˜ã‘**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£æ™‚åˆ»ã§è‡ªå‹•çš„ã«ã‚½ãƒ¼ãƒˆ
- **è¨­å®šå¯èƒ½ãªæ¸›è¡°ç‡**: å˜ä¸€ã®ãƒ™ãƒ¼ã‚¿å€¤ã¾ãŸã¯2ã¤ã®ãƒ™ãƒ¼ã‚¿å€¤é–“ã®ç·šå½¢è£œé–“ã‚’ã‚µãƒãƒ¼ãƒˆ
- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒ**: æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¶­æŒãƒ»æ›´æ–°
- **ãƒãƒƒã‚·ãƒ¥æ›´æ–°**: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸé‡ã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’å†è¨ˆç®—
- **ãƒ‡ãƒ¼ã‚¿å‹ä¿æŒ**: ãƒ†ãƒ³ã‚½ãƒ«ã®å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¶­æŒ

### ä½¿ç”¨æ³•

LoRA Post-Hoc EMAãƒãƒ¼ã‚¸ã¯ç‹¬ç«‹ã—ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š

```bash
python src/musubi_tuner/lora_post_hoc_ema.py checkpoint1.safetensors checkpoint2.safetensors checkpoint3.safetensors --output_file merged_lora.safetensors --beta 0.95
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼š

```
path [path ...]
    ãƒãƒ¼ã‚¸ã™ã‚‹LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

--beta BETA
    é‡ã¿ãƒãƒ¼ã‚¸ã®ãŸã‚ã®æ¸›è¡°ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š0.95ï¼‰
    é«˜ã„å€¤ï¼ˆ1.0ã«è¿‘ã„ï¼‰ã¯ç´¯ç©å¹³å‡ã«ã‚ˆã‚Šå¤§ããªé‡ã¿ã‚’ä¸ãˆã‚‹ï¼ˆå¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’é‡è¦–ï¼‰
    ä½ã„å€¤ã¯ç¾åœ¨ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚Šå¤§ããªé‡ã¿ã‚’ä¸ãˆã‚‹

--beta2 BETA2
    ç·šå½¢è£œé–“ã®ãŸã‚ã®ç¬¬2æ¸›è¡°ç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    æŒ‡å®šã•ã‚ŒãŸå ´åˆã€æ¸›è¡°ç‡ã¯ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§betaã‹ã‚‰beta2ã¸ç·šå½¢è£œé–“ã•ã‚Œã‚‹

--sigma_rel SIGMA_REL
    Power Function EMAã®ãŸã‚ã®ç›¸å¯¾ã‚·ã‚°ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€beta/beta2ã¨åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“ï¼‰
    betaã‚’æŒ‡å®šã—ãŸå ´åˆã®ã€æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒç›¸å¯¾çš„ã«å¤§ããªå½±éŸ¿ã‚’æŒã¤æ¬ ç‚¹ã‚’è§£æ±ºã—ã¾ã™
    æŒ‡å®šã•ã‚ŒãŸå ´åˆã€betaã¯æ¬¡ã®è«–æ–‡ã«åŸºã¥ã„ã¦Power Function EMAæ³•ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼š
    https://arxiv.org/pdf/2312.02696. ã“ã‚Œã«ã‚ˆã‚Šbetaã¨beta2ãŒä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚

--output_file OUTPUT_FILE
    ãƒãƒ¼ã‚¸ã•ã‚ŒãŸé‡ã¿ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¿…é ˆï¼‰

--no_sort
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚½ãƒ¼ãƒˆã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆæŒ‡å®šã—ãŸé †åºã§ãƒãƒ¼ã‚¸ï¼‰
```

### ä¾‹ï¼š

å®šæ•°æ¸›è¡°ç‡ã§ã®åŸºæœ¬çš„ãªä½¿ç”¨æ³•ï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_merged.safetensors \
    --beta 0.95
```

2ã¤ã®æ¸›è¡°ç‡é–“ã®ç·šå½¢è£œé–“ã‚’ä½¿ç”¨ï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_ema_interpolated.safetensors \
    --beta 0.90 \
    --beta2 0.95
```

`ã‚·ã‚°ãƒ_rel`ã‚’ä½¿ç”¨ã—ãŸPower Function EMAï¼š
```bash
python src/musubi_tuner/lora_post_hoc_ema.py \
    lora_epoch_001.safetensors \
    lora_epoch_002.safetensors \
    lora_epoch_003.safetensors \
    --output_file lora_power_ema_merged.safetensors \
    --sigma_rel 0.2
```

### æ¨å¥¨è¨­å®šã®ä¾‹ (30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã€ `--beta`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ)

ã©ã®è¨­å®šã‹ã‚‰è©¦ã›ã°è‰¯ã„ã‹åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€ã¾ãšä»¥ä¸‹ã®ã€Œ**ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š**ã€ã‹ã‚‰å§‹ã‚ã¦ã¿ã¦ãã ã•ã„ã€‚

#### 1. ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š (ã¾ãšè©¦ã™ã¹ãçµ„ã¿åˆã‚ã›)

- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `15-30` (å­¦ç¿’ã®å¾ŒåŠåŠåˆ†)
- **beta:** `0.9` (ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå€¤)

#### 2. æ—©æœŸã«å­¦ç¿’ãŒåæŸã—ãŸå ´åˆ

- **çŠ¶æ³:** lossãŒæ—©ã„æ®µéšã§ä¸‹ãŒã‚Šã€ãã®å¾Œã¯å®‰å®šã—ã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `10-30` (lossãŒå®‰å®šã—å§‹ã‚ãŸã‚¨ãƒãƒƒã‚¯ã‹ã‚‰æœ€å¾Œã¾ã§)
- **beta:** `0.95` (å¯¾è±¡ç¯„å›²ãŒåºƒã„ã®ã§ã€ã‚ˆã‚Šæ»‘ã‚‰ã‹ã«ã™ã‚‹)

#### 3. éå­¦ç¿’ã‚’é¿ã‘ãŸã„å ´åˆ

- **çŠ¶æ³:** å­¦ç¿’ã®æœ€å¾Œã®æ–¹ã§ã€ç”ŸæˆçµæœãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã™ãã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** `15-25` (æ€§èƒ½ã®ãƒ”ãƒ¼ã‚¯ã¨æ€ã‚ã‚Œã‚‹ç¯„å›²ã«çµã‚‹)
- **beta:** `0.8` (ç¯„å›²ã®çµ‚ç›¤ã‚’é‡è¦–ã—ã¤ã¤ã€å¤šæ§˜æ€§ã‚’æ®‹ã™)

**ãƒ’ãƒ³ãƒˆ:** æœ€é©ãªå€¤ã¯ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚è¤‡æ•°ã®`beta`ï¼ˆä¾‹: 0.8, 0.9, 0.95ï¼‰ã‚’è©¦ã—ã¦ã€ç”Ÿæˆçµæœã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

### æ¨å¥¨è¨­å®šã®ä¾‹ (30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã€ `--sigma_rel`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ)

`--sigma_rel` ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€betaã®æ¸›è¡°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Power Function EMAæ³•ã«ã‚ˆã£ã¦æ±ºå®šã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã¯ã„ãã¤ã‹ã®é–‹å§‹ç‚¹ã§ã™ã€‚

#### 1. ä¸€èˆ¬çš„ãªæ¨å¥¨è¨­å®š
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** å…¨ã¦ã®ã‚¨ãƒãƒƒã‚¯ï¼ˆæœ€åˆã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- **sigma_rel:** `0.2` ï¼ˆä¸€èˆ¬çš„ãªé–‹å§‹ç‚¹ï¼‰

#### 2. æ—©æœŸã«å­¦ç¿’ãŒåæŸã—ãŸå ´åˆ
- **çŠ¶æ³:** lossãŒæ—©ã„æ®µéšã§ä¸‹ãŒã‚Šã€ãã®å¾Œã¯å®‰å®šã—ã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** å…¨ã¦ã®ã‚¨ãƒãƒƒã‚¯
- **sigma_rel:** `0.25` ï¼ˆåˆæœŸã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é‡ãã‚’ç½®ããŸã‚ã€æ—©æœŸåæŸã«é©ã—ã¦ã„ã¾ã™ï¼‰

#### 3. éå­¦ç¿’ã‚’é¿ã‘ãŸã„å ´åˆ
- **çŠ¶æ³:** å­¦ç¿’ã®æœ€å¾Œã®æ–¹ã§ã€ç”ŸæˆçµæœãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã™ãã¦ã„ã‚‹ã€‚
- **å¯¾è±¡ã‚¨ãƒãƒƒã‚¯:** æœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰ã€éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚‹æœ€å¾Œã®æ•°ã‚¨ãƒãƒƒã‚¯ã‚’é™¤å¤–
- **sigma_rel:** `0.15` ï¼ˆçµ‚ç›¤ï¼ˆãŸã ã—æœ€å¾Œã®æœ€å¾Œã§ã¯ãªã„ï¼‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é‡ãã‚’ç½®ãã€æœ€çµ‚æ®µéšã§ã®éå­¦ç¿’ã‚’è»½æ¸›ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼‰

**ãƒ’ãƒ³ãƒˆ:** æœ€é©ãª `sigma_rel` ã®å€¤ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã€å­¦ç¿’æœŸé–“ã«ã‚ˆã£ã¦ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚å®Ÿé¨“ã‚’æ¨å¥¨ã—ã¾ã™ã€‚å€¤ã¯é€šå¸¸0.1ã‹ã‚‰0.5ã®ç¯„å›²ã§ã™ã€‚`sigma_rel` ã¨ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®è¨ˆç®—ã•ã‚ŒãŸ `beta` å€¤ã®é–¢ä¿‚ã‚’ç¤ºã™ã‚°ãƒ©ãƒ•ã¯ã€ãã®æŒ™å‹•ã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã‚ˆã†å¾Œã»ã©æä¾›ã™ã‚‹äºˆå®šã§ã™ã€‚

### æ³¨æ„ç‚¹ï¼š

- ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿®æ­£æ™‚åˆ»ã§è‡ªå‹•çš„ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã‚‹ãŸã‚ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ã®é †åºã¯é–¢ä¿‚ã‚ã‚Šã¾ã›ã‚“
- `--sigma_rel`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯`--beta`ãŠã‚ˆã³`--beta2`ã¨ç›¸äº’ã«æ’ä»–çš„ã§ã™ã€‚`--sigma_rel`ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€ãã‚ŒãŒãƒ™ãƒ¼ã‚¿å€¤ã‚’æ±ºå®šã—ã€æŒ‡å®šã•ã‚ŒãŸ`--beta`ã¾ãŸã¯`--beta2`ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
- ãƒãƒ¼ã‚¸ã™ã‚‹å…¨ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã²ã¨ã¤ã®å­¦ç¿’ã§ã€ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    - å½¢çŠ¶ãŒä¸€è‡´ã—ã¦ã„ã‚Œã°ãƒãƒ¼ã‚¸ã¯ã§ãã¾ã™ãŒã€Post Hoc EMAã¨ã—ã¦ã¯æ­£ã—ãå‹•ä½œã—ã¾ã›ã‚“
- alphaå€¤ã¯ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§åŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã€æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚‚ã®ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒãƒƒã‚·ãƒ¥å€¤ã®ã¿ãŒå†è¨ˆç®—ã•ã‚Œã¾ã™
- æµ®å‹•å°æ•°ç‚¹ä»¥å¤–ã®ã€longã€intã€boolãªã©ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ãƒãƒ¼ã‚¸ã•ã‚Œã¾ã›ã‚“ï¼ˆæœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚‚ã®ãŒä½¿ç”¨ã•ã‚Œã¾ã™ï¼‰
- ãƒãƒ¼ã‚¸ä¸­ã®æ•°å€¤å®‰å®šæ€§ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã«float32ç²¾åº¦ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚ä¿å­˜æ™‚ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ãŒç¶­æŒã•ã‚Œã¾ã™

</details>

## MagCache

The following is quoted from the [MagCache github repository](https://github.com/Zehong-Ma/MagCache) "Magnitude-aware Cache (MagCache) for Video Diffusion Models":

> We introduce Magnitude-aware Cache (MagCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps based on the robust magnitude observations, thereby accelerating the inference. MagCache works well for Video Diffusion Models, Image Diffusion models. 

We have implemented the MagCache feature in Musubi Tuner. Some of the code is based on the MagCache repository. It is available for `fpack_generate_video.py` for now.

### Usage

1. Calibrate the mag ratios
   - Run the inference script as normal, but with the `--magcache_calibration` option to calibrate the mag ratios. You will get a following output:

   ```
   INFO:musubi_tuner.fpack_generate_video:Copy and paste following values to --magcache_mag_ratios argument to use them:
   1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```
   - It is recommended to run the calibration with your custom prompt and model.
   - If you inference the multi-section video, you will get the mag ratios for each section. You can use the one of the sections or average them.

2. Use the mag ratios
   - Run the inference script with the `--magcache_mag_ratios` option to use the mag ratios. For example:

   ```bash
   python fpack_generate_video.py --magcache_mag_ratios 1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```

   - Specify `--magcache_mag_ratios 0` to use the default mag ratios from the MagCache repository.
   - It is recommended to use the same steps as the calibration. If the steps are different, the mag ratios is interpolated to the specified steps. 
   - You can also specify the `--magcache_retention_ratio`, `--magcache_threshold`, and `--magcache_k` options to control the MagCache behavior. The default values are 0.2, 0.24, and 6, respectively (same as the MagCache repository).

    ```bash
    python fpack_generate_video.py --magcache_retention_ratio 0.2 --magcache_threshold 0.24 --magcache_k 6
    ```

    - The `--magcache_retention_ratio` option controls the ratio of the steps not to cache. For example, if you set it to 0.2, the first 20% of the steps will not be cached. The default value is 0.2.
    - The `--magcache_threshold` option controls the threshold whether to use the cached output or not. If the accumulated error is less than the threshold, the cached output will be used. The default value is 0.24.
        - The error is calculated by the accumulated error multiplied by the mag ratio.
    - The `--magcache_k` option controls the number of steps to use for the cache. The default value is 6, which means the consecutive 6 steps will be used for the cache. The default value 6 is recommended for 50 steps, so you may want to lower it for smaller number of steps.

### Generated video example

Using F1-model, without MagCache, approximately 90 seconds are required to generate single section video with 25 steps (without VAE decoding) in my environment.

https://github.com/user-attachments/assets/30b8d05e-9bd6-42bf-997f-5ba5b3dde876

With MagCache, default settings, approximately 30 seconds are required to generate with the same settings.

https://github.com/user-attachments/assets/080076ea-4088-443c-8138-4eeb00694ec5

With MagCache, `--magcache_retention_ratio 0.2 --magcache_threshold 0.12 --magcache_k 3`, approximately 35 seconds are required to generate with the same settings.

https://github.com/user-attachments/assets/27d6c7ff-e3db-4c52-8668-9a887441acef

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã¯ã€[MagCache githubãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Zehong-Ma/MagCache) "Magnitude-aware Cache (MagCache) for Video Diffusion Models"ã‹ã‚‰ã®å¼•ç”¨ã®æ‹™è¨³ã§ã™ï¼š

> Magnitude-aware Cache (MagCache)ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€å …ç‰¢ãªãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰è¦³æ¸¬ã«åŸºã¥ã„ã¦ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—é–“ã®ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®å¤‰å‹•å·®ã‚’æ¨å®šãŠã‚ˆã³æ´»ç”¨ã—ã€æ¨è«–ã‚’åŠ é€Ÿã—ã¾ã™ã€‚MagCacheã¯ã€ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€ç”»åƒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«é©ã—ã¦ã„ã¾ã™ã€‚

Musubi Tunerã«MagCacheæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã¯MagCacheãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’åŸºã«ã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã¯`fpack_generate_video.py`ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

### ä½¿ç”¨æ–¹æ³•

1. mag_ratiosã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - `--magcache_calibration`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ã€ãã‚Œä»¥å¤–ã¯é€šå¸¸é€šã‚Šæ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã€mag ratiosã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

   ```
   INFO:musubi_tuner.fpack_generate_video:Copy and paste following values to --magcache_mag_ratios argument to use them:
   1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
   ```
   - ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
   - è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ“ãƒ‡ã‚ªã‚’æ¨è«–ã™ã‚‹å ´åˆã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®mag ratiosãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚ã©ã‚Œã‹ä¸€ã¤ã€ã¾ãŸã¯ãã‚Œã‚‰ã‚’å¹³å‡ã—ãŸå€¤ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚

2. mag ratiosã®ä½¿ç”¨
   - `--magcache_mag_ratios`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§mag ratiosã‚’æŒ‡å®šã—ã¦æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ä¾‹ï¼š

   ```bash
    python fpack_generate_video.py --magcache_mag_ratios 1.00000,1.26562,1.08594,1.02344,1.00781,1.01562,1.01562,1.03125,1.04688,1.00781,1.03125,1.00000,1.01562,1.01562,1.02344,1.01562,0.98438,1.05469,0.98438,0.97266,1.03125,0.96875,0.93359,0.95703,0.77734
    ```

    - `--magcache_mag_ratios 0`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€MagCacheãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®mag ratiosãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
    - mag ratiosã®æ•°ã¯ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸæ™‚ã¨åŒã˜ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒç•°ãªã‚‹å ´åˆã€mag ratiosã¯æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—æ•°ã«åˆã†ã‚ˆã†ã«è£œé–“ã•ã‚Œã¾ã™ã€‚
    - `--magcache_retention_ratio`, `--magcache_threshold`, `--magcache_k`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦MagCacheã®å‹•ä½œã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.2ã€0.24ã€6ã§ã™ï¼ˆMagCacheãƒªãƒã‚¸ãƒˆãƒªã¨åŒã˜ã§ã™ï¼‰ã€‚
    
     ```bash
    python fpack_generate_video.py --magcache_retention_ratio 0.2 --magcache_threshold 0.24 --magcache_k 6
    ```

    - `--magcache_retention_ratio`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€0.2ã«è¨­å®šã™ã‚‹ã¨ã€æœ€åˆã®20%ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.2ã§ã™ã€‚
    - `--magcache_threshold`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‡ºåŠ›ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®é–¾å€¤ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ç´¯ç©èª¤å·®ãŒã“ã®é–¾å€¤æœªæº€ã®å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‡ºåŠ›ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0.24ã§ã™ã€‚
        - èª¤å·®ã¯ã€ç´¯ç©èª¤å·®ã«mag ratioã‚’æ›ã‘ãŸã‚‚ã®ã¨ã—ã¦è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
    - `--magcache_k`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯6ã§ã€ã“ã‚Œã¯é€£ç¶šã™ã‚‹6ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤6ã¯æã‚‰ã50ã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆã®æ¨å¥¨å€¤ã®ãŸã‚ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå°‘ãªã„å ´åˆã¯æ¸›ã‚‰ã™ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ã¯è‹±èªã§ã®èª¬æ˜ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>
