> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Advanced configuration / é«˜åº¦ãªè¨­å®š

## Table of contents / ç›®æ¬¡

- [How to specify `network_args`](#how-to-specify-network_args--network_argsã®æŒ‡å®šæ–¹æ³•)
- [LoRA+](#lora)
- [Select the target modules of LoRA](#select-the-target-modules-of-lora--loraã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹)
- [Save and view logs in TensorBoard format](#save-and-view-logs-in-tensorboard-format--tensorboardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [Save and view logs in wandb](#save-and-view-logs-in-wandb--wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§)
- [FP8 weight optimization for models](#fp8-weight-optimization-for-models--ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®fp8ã¸ã®æœ€é©åŒ–)

## How to specify `network_args` / `network_args`ã®æŒ‡å®šæ–¹æ³•

The `--network_args` option is an option for specifying detailed arguments to LoRA. Specify the arguments in the form of `key=value` in `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>
`--network_args`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€LoRAã¸ã®è©³ç´°ãªå¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚`--network_args`ã«ã¯ã€`key=value`ã®å½¢å¼ã§å¼•æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

If you specify it on the command line, write as follows. / ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 
    --network_args "key1=value1" "key2=value2" ...
```

If you specify it in the configuration file, write as follows. / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```toml
network_args = ["key1=value1", "key2=value2", ...]
```

If you specify `"verbose=True"`, detailed information of LoRA will be displayed. / `"verbose=True"`ã‚’æŒ‡å®šã™ã‚‹ã¨LoRAã®è©³ç´°ãªæƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```bash
--network_args "verbose=True" "key1=value1" "key2=value2" ...
```

## LoRA+

LoRA+ is a method to improve the training speed by increasing the learning rate of the UP side (LoRA-B) of LoRA. Specify the multiplier for the learning rate. The original paper recommends 16, but adjust as needed. It seems to be good to start from around 4. For details, please refer to the [related PR of sd-scripts](https://github.com/kohya-ss/sd-scripts/pull/1233).

Specify `loraplus_lr_ratio` with `--network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA+ã¯ã€LoRAã®UPå´ï¼ˆLoRA-Bï¼‰ã®å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã§å­¦ç¿’é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚å­¦ç¿’ç‡ã«å¯¾ã™ã‚‹å€ç‡ã‚’æŒ‡å®šã—ã¾ã™ã€‚å…ƒè«–æ–‡ã§ã¯16ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚4ç¨‹åº¦ã‹ã‚‰å§‹ã‚ã‚‹ã¨ã‚ˆã„ã‚ˆã†ã§ã™ã€‚è©³ç´°ã¯[sd-scriptsã®é–¢é€£PR]https://github.com/kohya-ss/sd-scripts/pull/1233)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`--network_args`ã§`loraplus_lr_ratio`ã‚’æŒ‡å®šã—ã¾ã™ã€‚
</details>

### Example / è¨˜è¿°ä¾‹

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 hv_train_network.py --dit ... 
    --network_module networks.lora --network_dim 32 --network_args "loraplus_lr_ratio=4" ...
```

## Select the target modules of LoRA / LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹

*This feature is highly experimental and the specification may change. / ã“ã®æ©Ÿèƒ½ã¯ç‰¹ã«å®Ÿé¨“çš„ãªã‚‚ã®ã§ã€ä»•æ§˜ã¯å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚*

By specifying `exclude_patterns` and `include_patterns` with `--network_args`, you can select the target modules of LoRA.

`exclude_patterns` excludes modules that match the specified pattern. `include_patterns` targets only modules that match the specified pattern.

Specify the values as a list. For example, `"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`.

The pattern is a regular expression for the module name. The module name is in the form of `double_blocks.0.img_mod.linear` or `single_blocks.39.modulation.linear`. The regular expression is not a partial match but a complete match.

The patterns are applied in the order of `exclude_patterns`â†’`include_patterns`. By default, the Linear layers of `img_mod`, `txt_mod`, and `modulation` of double blocks and single blocks are excluded.

(`.*(img_mod|txt_mod|modulation).*` is specified.)

<details>
<summary>æ—¥æœ¬èª</summary>

`--network_args`ã§`exclude_patterns`ã¨`include_patterns`ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€LoRAã®å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

`exclude_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é™¤å¤–ã—ã¾ã™ã€‚`include_patterns`ã¯ã€æŒ‡å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

å€¤ã¯ã€ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¾ã™ã€‚`"exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']"`ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã«å¯¾ã™ã‚‹æ­£è¦è¡¨ç¾ã§ã™ã€‚ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã¯ã€ãŸã¨ãˆã°`double_blocks.0.img_mod.linear`ã‚„`single_blocks.39.modulation.linear`ã®ã‚ˆã†ãªå½¢å¼ã§ã™ã€‚æ­£è¦è¡¨ç¾ã¯éƒ¨åˆ†ä¸€è‡´ã§ã¯ãªãå®Œå…¨ä¸€è‡´ã§ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€`exclude_patterns`â†’`include_patterns`ã®é †ã§é©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€double blocksã¨single blocksã®Linearå±¤ã®ã†ã¡ã€`img_mod`ã€`txt_mod`ã€`modulation`ãŒé™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚

ï¼ˆ`.*(img_mod|txt_mod|modulation).*`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ï¼‰
</details>

### Example / è¨˜è¿°ä¾‹

Only the modules of double blocks / double blocksã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*single_blocks.*']"
```

Only the modules of single blocks from the 10th / single blocksã®10ç•ªç›®ä»¥é™ã®Linearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆ:

```bash
--network_args "exclude_patterns=[r'.*']" "include_patterns=[r'.*single_blocks\.\d{2}\.linear.*']"
```

## Save and view logs in TensorBoard format / TensorBoardå½¢å¼ã®ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

Specify the folder to save the logs with the `--logging_dir` option. Logs in TensorBoard format will be saved.

For example, if you specify `--logging_dir=logs`, a `logs` folder will be created in the working folder, and logs will be saved in the date folder inside it.

Also, if you specify the `--log_prefix` option, the specified string will be added before the date. For example, use `--logging_dir=logs --log_prefix=lora_setting1_` for identification.

To view logs in TensorBoard, open another command prompt and activate the virtual environment. Then enter the following in the working folder.

```powershell
tensorboard --logdir=logs
```

(tensorboard installation is required.)

Then open a browser and access http://localhost:6006/ to display it.

<details>
<summary>æ—¥æœ¬èª</summary>
`--logging_dir`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ãƒ­ã‚°ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚TensorBoardå½¢å¼ã®ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ãŸã¨ãˆã°`--logging_dir=logs`ã¨æŒ‡å®šã™ã‚‹ã¨ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã«logsãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã€ãã®ä¸­ã®æ—¥æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

ã¾ãŸ`--log_prefix`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨ã€æ—¥æ™‚ã®å‰ã«æŒ‡å®šã—ãŸæ–‡å­—åˆ—ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚`--logging_dir=logs --log_prefix=lora_setting1_`ãªã©ã¨ã—ã¦è­˜åˆ¥ç”¨ã«ãŠä½¿ã„ãã ã•ã„ã€‚

TensorBoardã§ãƒ­ã‚°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€åˆ¥ã®ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ãã€ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹ã«ã—ã¦ã‹ã‚‰ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å…¥åŠ›ã—ã¾ã™ã€‚

```powershell
tensorboard --logdir=logs
```

ï¼ˆtensorboardã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ï¼‰

ãã®å¾Œãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€http://localhost:6006/ ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
</details>

## Save and view logs in wandb / wandbã§ãƒ­ã‚°ã®ä¿å­˜ã¨å‚ç…§

`--log_with wandb` option is available to save logs in wandb format. `tensorboard` or `all` is also available. The default is `tensorboard`.

Specify the project name with `--log_tracker_name` when using wandb.

<details>
<summary>æ—¥æœ¬èª</summary>
`--log_with wandb`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹ã¨wandbå½¢å¼ã§ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚`tensorboard`ã‚„`all`ã‚‚æŒ‡å®šå¯èƒ½ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`tensorboard`ã§ã™ã€‚

wandbã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`--log_tracker_name`ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
</details>

## FP8 weight optimization for models / ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®FP8ã¸ã®æœ€é©åŒ–

The `--fp8_scaled` option is available to quantize the weights of the model to FP8 (E4M3) format with appropriate scaling. This reduces the VRAM usage while maintaining precision. Important weights are kept in FP16/BF16/FP32 format.

The model weights must be in fp16 or bf16. Weights that have been pre-converted to float8_e4m3 cannot be used.

Wan2.1 inference and training are supported.

Specify the `--fp8_scaled` option in addition to the `--fp8` option during inference.

Specify the `--fp8_scaled` option in addition to the `--fp8_base` option during training.

Acknowledgments: This feature is based on the [implementation](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py) of [HunyuanVideo](https://github.com/Tencent/HunyuanVideo). The selection of high-precision modules is based on the [implementation](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py) of [diffusion-pipe](https://github.com/tdrussell/diffusion-pipe). I would like to thank these repositories.

<details>
<summary>æ—¥æœ¬èª</summary>
é‡ã¿ã‚’å˜ç´”ã«FP8ã¸castã™ã‚‹ã®ã§ã¯ãªãã€é©åˆ‡ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§FP8å½¢å¼ã«é‡å­åŒ–ã™ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚ã¾ãŸã€é‡è¦ãªé‡ã¿ã¯FP16/BF16/FP32å½¢å¼ã§ä¿æŒã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ã€fp16ã¾ãŸã¯bf16ãŒå¿…è¦ã§ã™ã€‚ã‚ã‚‰ã‹ã˜ã‚float8_e4m3ã«å¤‰æ›ã•ã‚ŒãŸé‡ã¿ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚

Wan2.1ã®æ¨è«–ã€å­¦ç¿’ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

æ¨è«–æ™‚ã¯`--fp8`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŠ ãˆã¦ `--fp8_scaled`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

å­¦ç¿’æ™‚ã¯`--fp8_base`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŠ ãˆã¦ `--fp8_scaled`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

è¬è¾ï¼šã“ã®æ©Ÿèƒ½ã¯ã€[HunyuanVideo](https://github.com/Tencent/HunyuanVideo)ã®[å®Ÿè£…](https://github.com/Tencent/HunyuanVideo/blob/7df4a45c7e424a3f6cd7d653a7ff1f60cddc1eb1/hyvideo/modules/fp8_optimization.py)ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã¾ãŸã€é«˜ç²¾åº¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é¸æŠã«ãŠã„ã¦ã¯[diffusion-pipe](https://github.com/tdrussell/diffusion-pipe)ã®[å®Ÿè£…](https://github.com/tdrussell/diffusion-pipe/blob/407c04fdae1c9ab5e67b54d33bef62c3e0a8dbc7/models/wan.py)ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒªã«æ„Ÿè¬ã—ã¾ã™ã€‚

</details>

### Key features and implementation details / ä¸»ãªç‰¹å¾´ã¨å®Ÿè£…ã®è©³ç´°

- Implements FP8 (E4M3) weight quantization for Linear layers
- Reduces VRAM requirements by using 8-bit weights for storage (slightly increased compared to existing `--fp8` `--fp8_base` options)
- Quantizes weights to FP8 format with appropriate scaling instead of simple cast to FP8
- Maintains computational precision by dequantizing to original precision (FP16/BF16/FP32) during forward pass
- Preserves important weights in FP16/BF16/FP32 format

The implementation:

1. Quantizes weights to FP8 format with appropriate scaling
2. Replaces weights by FP8 quantized weights and stores scale factors in model state dict
3. Applies monkey patching to Linear layers for transparent dequantization during computation

<details>
<summary>æ—¥æœ¬èª</summary>

- Linearå±¤ã®FP8ï¼ˆE4M3ï¼‰é‡ã¿é‡å­åŒ–ã‚’å®Ÿè£…
- 8ãƒ“ãƒƒãƒˆã®é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ï¼ˆæ—¢å­˜ã®`--fp8` `--fp8_base` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«æ¯”ã¹ã¦å¾®å¢—ï¼‰
- å˜ç´”ãªFP8ã¸ã®castã§ã¯ãªãã€é©åˆ‡ãªå€¤ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦é‡ã¿ã‚’FP8å½¢å¼ã«é‡å­åŒ–
- forwardæ™‚ã«å…ƒã®ç²¾åº¦ï¼ˆFP16/BF16/FP32ï¼‰ã«é€†é‡å­åŒ–ã—ã¦è¨ˆç®—ç²¾åº¦ã‚’ç¶­æŒ
- ç²¾åº¦ãŒé‡è¦ãªé‡ã¿ã¯FP16/BF16/FP32ã®ã¾ã¾ä¿æŒ

å®Ÿè£…:

1. ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹é©åˆ‡ãªå€ç‡ã§é‡ã¿ã‚’FP8å½¢å¼ã«é‡å­åŒ–
2. é‡ã¿ã‚’FP8é‡å­åŒ–é‡ã¿ã«ç½®ãæ›ãˆã€å€ç‡ã‚’ãƒ¢ãƒ‡ãƒ«ã®state dictã«ä¿å­˜
3. Linearå±¤ã«monkey patchingã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã›ãšã«é€†é‡å­åŒ–
 